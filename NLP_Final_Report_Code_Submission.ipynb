{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNgpmB7aeepoqnkSntqBjnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hozanBaydu/Aspect-Based-Sentiment-Analysis-for-Turkish-E-Commerce-Reviews-Using-BERTurk/blob/master/NLP_Final_Report_Code_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect-Based Sentiment Analysis for Turkish E-Commerce Reviews Using BERTurk"
      ],
      "metadata": {
        "id": "wiJtgO-YLIz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y jax jaxlib opencv-python opencv-contrib-python torchvision datasets pytensor thinc > /dev/null\n",
        "!pip install --upgrade --quiet pandas==2.2.2 pyarrow==15.0.2 scikit-learn==1.5.0 numpy==1.26.4 sentence-transformers==2.7.0 matplotlib==3.9.2 wordcloud==1.9.3\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "HUx1_7rSXjpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading the Kaggle dataset we prepared.**"
      ],
      "metadata": {
        "id": "G19noiH76qpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic sentence-transformers transformers nltk kaggle\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = 'serkaneren68'\n",
        "os.environ['KAGGLE_KEY'] = '56535c63a5e74766c651a0d38c9658be'\n",
        "\n",
        "import kaggle\n",
        "print(\"Veri indiriliyor...\")\n",
        "kaggle.api.dataset_download_files('serkaneren68/hepsiburada-reviews', path='./', unzip=True)"
      ],
      "metadata": {
        "id": "pm7jm49gfyfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Embedding - Clustering**"
      ],
      "metadata": {
        "id": "1FeTC3CSLaWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "FILE_PATH = '/content/nlp_data.csv'\n",
        "SAMPLE_SIZE = 20000\n",
        "NUM_CLUSTERS = 15\n",
        "\n",
        "print(\"1. Veri YÃ¼kleniyor...\")\n",
        "try:\n",
        "    df = pd.read_csv(FILE_PATH, usecols=['review_text'])\n",
        "    df = df.dropna().sample(n=min(SAMPLE_SIZE, len(df)), random_state=42)\n",
        "    print(f\"   -> {len(df)} adet yorum yÃ¼klendi.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"HATA: Dosya bulunamadÄ±! LÃ¼tfen dosya yolunu kontrol et.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "print(\"2. Yorumlar cÃ¼mlelere ayrÄ±ÅŸtÄ±rÄ±lÄ±yor...\")\n",
        "sentences = []\n",
        "source_indices = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    text = str(row['review_text'])\n",
        "    raw_sentences = nltk.sent_tokenize(text)\n",
        "    for sent in raw_sentences:\n",
        "        if len(sent.split()) > 2:\n",
        "            sentences.append(sent)\n",
        "\n",
        "df_sentences = pd.DataFrame({'sentence': sentences})\n",
        "print(f\"   -> Toplam CÃ¼mle SayÄ±sÄ±: {len(df_sentences)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"â¬‡ï¸ CÃ¼mle gÃ¶mÃ¼lÃ¼ler oluÅŸturuluyor...\")\n",
        "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embeddings = model.encode(sentences, show_progress_bar=True)\n",
        "\n",
        "embeddings = normalize(embeddings)\n",
        "print(\"âœ… GÃ¶mÃ¼lÃ¼ler oluÅŸturuldu ve normalize edildi.\")\n",
        "\n",
        "print(\"3. Embedding (VektÃ¶r) oluÅŸturuluyor...\")\n",
        "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embeddings = model.encode(sentences, show_progress_bar=True)\n",
        "\n",
        "print(\"4. PCA ile boyut indirgeme yapÄ±lÄ±yor (Performans artÄ±ÅŸÄ±)...\")\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "\n",
        "print(f\"5. K-Means ile {NUM_CLUSTERS} farklÄ± Aspect kÃ¼meleniyor...\")\n",
        "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
        "df_sentences['cluster'] = kmeans.fit_predict(reduced_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "custom_stopwords = [\n",
        "    \"bir\", \"ve\", \"Ã§ok\", \"ama\", \"ile\", \"gibi\", \"daha\", \"iÃ§in\", \"ben\", \"sen\", \"o\",\n",
        "    \"biz\", \"siz\", \"onlar\", \"bu\", \"ÅŸu\", \"da\", \"de\", \"ki\", \"mi\", \"mu\", \"mÄ±\", \"mÃ¼\",\n",
        "    \"diye\", \"yani\", \"ise\", \"veya\", \"ya\", \"hem\", \"her\", \"ÅŸey\", \"ne\", \"bi\", \"bile\",\n",
        "    \"kadar\", \"bence\", \"sanki\", \"zaten\", \"ancak\", \"fakat\", \"lakin\", \"pek\", \"hiÃ§\",\n",
        "    \"bÃ¶yle\", \"Ã¶yle\", \"ÅŸÃ¶yle\", \"en\", \"biraz\", \"fazla\", \"asla\", \"tam\",\n",
        "\n",
        "    \"Ã¼rÃ¼n\", \"Ã¼rÃ¼nÃ¼\", \"Ã¼rÃ¼nÃ¼n\", \"urun\", \"tavsiye\", \"ederim\", \"etmem\", \"aldÄ±m\",\n",
        "    \"almayÄ±n\", \"alÄ±n\", \"geldi\", \"verdim\", \"sipariÅŸ\", \"kullanÄ±yorum\", \"kullandÄ±m\",\n",
        "    \"elime\", \"ulaÅŸtÄ±\", \"teslim\", \"gÃ¼n\", \"sonra\", \"Ã¶nce\", \"ay\", \"yÄ±l\", \"saat\",\n",
        "    \"var\", \"yok\", \"oldu\", \"olmadÄ±\", \"yapÄ±yor\", \"yapmÄ±yor\", \"ediyorum\",\n",
        "\n",
        "    \"gÃ¼zel\", \"kÃ¶tÃ¼\", \"iyi\", \"berbat\", \"harika\", \"mÃ¼kemmel\", \"sÃ¼per\", \"fena\",\n",
        "    \"beÄŸendim\", \"beÄŸenmedim\", \"memnun\", \"kaldÄ±m\", \"kalmadÄ±m\", \"teÅŸekkÃ¼rler\",\n",
        "    \"teÅŸekkÃ¼r\", \"baÅŸarÄ±lÄ±\", \"kaliteli\", \"kalitesiz\", \"saÄŸlam\", \"ÅŸÄ±k\", \"rahat\",\n",
        "    \"gayet\", \"inanÄ±lmaz\", \"rezalet\",\n",
        "\n",
        "    \"deÄŸil\", \"yok\", \"var\", \"etmiyorum\", \"iÅŸe\", \"yaramadÄ±\", \"kesinlikle\",\n",
        "    \"cok\", \"Ã§ok\", \"bana\", \"sana\", \"olarak\", \"gÃ¶re\", \"kadar\", \"tane\", \"adet\"\n",
        "\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"SONUÃ‡LAR: BULUNAN ASPECTLER\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=custom_stopwords, max_features=2000, ngram_range=(1, 2))\n",
        "cluster_docs = df_sentences.groupby(['cluster'])['sentence'].apply(lambda x: \" \".join(x))\n",
        "tfidf_matrix = vectorizer.fit_transform(cluster_docs)\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "aspect_results = {}\n",
        "\n",
        "for i in range(NUM_CLUSTERS):\n",
        "    subset = df_sentences[df_sentences['cluster'] == i]\n",
        "    count = len(subset)\n",
        "\n",
        "    if count == 0: continue\n",
        "\n",
        "    row = tfidf_matrix[i].toarray().flatten()\n",
        "    top_indices = row.argsort()[-8:][::-1]\n",
        "    keywords = feature_names[top_indices]\n",
        "\n",
        "    aspect_results[i] = \", \".join(keywords)\n",
        "\n",
        "    print(f\"\\nðŸ”¹ KÃœME {i} (CÃ¼mle SayÄ±sÄ±: {count})\")\n",
        "    print(f\"   ðŸ·ï¸ Anahtar Kelimeler: {aspect_results[i]}\")\n",
        "\n",
        "    n_samples = min(3, count)\n",
        "    examples = subset['sentence'].sample(n_samples, random_state=42).values\n",
        "    print(\"   ðŸ’¬ Ã–rnekler:\")\n",
        "    for ex in examples:\n",
        "        print(f\"      - {ex}\")\n",
        "\n",
        "\n",
        "print(\"\\nðŸ–¼ï¸ WordCloud oluÅŸturuluyor...\")\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "top_clusters = df_sentences['cluster'].value_counts().index[:4]\n",
        "\n",
        "for idx, cluster_id in enumerate(top_clusters):\n",
        "    text = \" \".join(df_sentences[df_sentences['cluster'] == cluster_id]['sentence'])\n",
        "\n",
        "    wc = WordCloud(width=400, height=300, background_color='white',\n",
        "                   stopwords=set(custom_stopwords)).generate(text)\n",
        "\n",
        "    plt.subplot(2, 2, idx+1)\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.title(f\"KÃ¼me {cluster_id}\\n({aspect_results.get(cluster_id, '')[:30]}...)\", fontsize=12)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KgVctCZqwQmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing clusters**"
      ],
      "metadata": {
        "id": "a2-tk50GStXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "aspect_map = {\n",
        "    0: \"Seller Errors\",                 # Wrong item sent, seller-related issues, authenticity concerns.\n",
        "    1: \"General Satisfaction\",          # \"Liked it\", \"smooth\", \"quality\" (General positive expressions).\n",
        "    2: \"Size & Fit\",                    # Running large/small, non-standard fits.\n",
        "    3: \"Damaged/Broken Item\",           # Physical damage from shipping/production (broken plastic, leaked).\n",
        "    4: \"Installation & Usage\",          # Assembly, charging time, ease of use.\n",
        "    5: \"Return Process\",                # Return actions due to fit issues or defects.\n",
        "    6: \"Seller Courtesy\",               # Fast shipping, gifts, thank you notes (Praise for seller).\n",
        "    7: \"Product Features (Scent/Liquid)\", # Smell, waterproofness, taste, specific physical traits.\n",
        "    8: \"General Quality\",               # \"Useful\", \"super\", \"does the job\" type comments.\n",
        "    9: \"Color & Appearance\",            # Color, visual accuracy, aesthetic look.\n",
        "    10: \"Durability Issues\",            # \"Broke immediately\", \"wobbling\", \"torn\".\n",
        "    11: \"Missing/Defective Item\",       # Missing parts or product never arrived.\n",
        "    12: \"Price/Performance\",            # Worth the money, cheap, expensive.\n",
        "    13: \"Shipping Disasters\"            # Leaked liquids, crushed boxes, late delivery.\n",
        "}\n",
        "df_sentences['aspect'] = df_sentences['cluster'].map(aspect_map)\n",
        "\n",
        "print(\"\\nâœ… 15 KÃ¼me -> 7 Aspect EÅŸleÅŸtirmesi TamamlandÄ±!\")\n",
        "print(df_sentences[['sentence', 'cluster', 'aspect']].sample(15))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "df_sentences['aspect'].value_counts().plot(kind='barh', color='teal')\n",
        "plt.title(\"YorumlarÄ±n 7 Ana BaÅŸlÄ±ÄŸa GÃ¶re DaÄŸÄ±lÄ±mÄ±\")\n",
        "plt.xlabel(\"CÃ¼mle SayÄ±sÄ±\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df_sentences['aspect'] = df_sentences['cluster'].map(aspect_map)\n",
        "\n",
        "print(\"\\nâœ… Aspect AtamasÄ± YapÄ±ldÄ±!\")\n",
        "print(df_sentences[['sentence', 'aspect']].sample(15))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "df_sentences['aspect'].value_counts().plot(kind='barh', color='teal')\n",
        "plt.title(\"YorumlarÄ±n Aspect DaÄŸÄ±lÄ±mÄ±\")\n",
        "plt.xlabel(\"CÃ¼mle SayÄ±sÄ±\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E6z_HFhdzHWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bT3ICnRaR5dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "ZqfVSeqwUOqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "df[['review_text']].to_csv('ham_5000_yorum.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"âœ… {len(df)} adet ham yorum indiriliyor.\")\n",
        "files.download('ham_5000_yorum.csv')"
      ],
      "metadata": {
        "id": "4MYpePag2L7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p78tJeiS3nFS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', None)\n",
        "df_ham = pd.read_csv('ham_5000_yorum.csv')\n",
        "display(df_ham.iloc[1800:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Labeling**"
      ],
      "metadata": {
        "id": "VOozrWbMUacw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = \"\"\"\n",
        "0.yorum: 1,0,0,0,0,1,0 1.yorum: 1,0,0,0,1,0,0 2.yorum: 0,0,0,-1,0,0,0 3.yorum: 1,0,0,0,0,-1,0 4.yorum: 0,0,0,0,0,1,0 5.yorum: 0,1,0,0,-1,0,0 6.yorum: 0,0,0,0,0,1,0 7.yorum: 0,0,0,0,-1,-1,0 8.yorum: 0,0,0,0,0,1,0 9.yorum: 0,0,-1,0,0,-1,-1 10.yorum: 0,0,0,0,1,1,-1 11.yorum: 0,0,0,0,-1,0,0 12.yorum: 0,0,0,0,0,-1,0 13.yorum: 0,0,-1,0,0,0,-1 14.yorum: 0,-1,0,0,0,-1,-1 15.yorum: 0,0,0,0,-1,-1,0 16.yorum: 1,0,-1,0,0,0,0 17.yorum: 0,0,-1,0,0,-1,0 18.yorum: 0,0,0,0,0,-1,0 19.yorum: 0,0,-1,0,-1,-1,-1 20.yorum: 0,1,0,0,0,0,0 21.yorum: 0,0,-1,0,-1,-1,0 22.yorum: 0,-1,-1,0,0,-1,0 23.yorum: 1,0,0,0,0,0,0 24.yorum: 0,0,0,0,0,0,1 25.yorum: 0,-1,0,0,1,1,0 26.yorum: 0,0,0,0,0,1,0 27.yorum: 0,0,-1,-1,0,0,-1 28.yorum: 0,0,0,0,0,0,-1 29.yorum: 0,0,-1,0,0,0,0 30.yorum: 0,-1,0,0,0,-1,0 31.yorum: 0,0,0,0,0,0,0 32.yorum: 0,1,0,0,0,0,0 33.yorum: 0,0,0,0,1,1,0 34.yorum: 0,0,0,0,-1,-1,0 35.yorum: 1,-1,0,0,0,-1,-1 36.yorum: 0,0,0,0,1,1,0 37.yorum: 0,1,0,0,1,0,1 38.yorum: 0,0,0,-1,0,0,-1 39.yorum: 1,0,0,0,0,0,0 40.yorum: 0,0,0,0,1,-1,0 41.yorum: 0,1,0,0,0,0,1 42.yorum: 0,0,0,0,0,0,-1 43.yorum: 0,0,0,0,-1,0,0 44.yorum: 0,-1,0,0,0,0,-1 45.yorum: 0,0,0,0,0,0,0 46.yorum: 0,0,0,0,0,0,1 47.yorum: 0,1,0,0,1,0,1 48.yorum: 0,0,0,0,1,0,0 49.yorum: 0,1,0,0,0,0,0 50.yorum: 0,0,0,0,1,1,0 51.yorum: 0,0,0,-1,0,0,0 52.yorum: 0,0,-1,1,0,0,0 53.yorum: 0,0,0,0,-1,0,0 54.yorum: 0,-1,0,0,0,0,0 55.yorum: 0,-1,0,0,0,1,0 56.yorum: 0,-1,0,0,1,-1,0 57.yorum: 0,0,0,0,0,0,-1 58.yorum: 0,0,0,0,0,-1,0 59.yorum: 0,0,1,0,0,0,0 60.yorum: 0,-1,0,0,0,-1,0 61.yorum: 0,0,0,0,-1,0,0 62.yorum: 0,0,0,0,-1,0,0 63.yorum: 0,1,1,0,0,0,0 64.yorum: 0,0,0,0,-1,-1,0 65.yorum: 0,0,0,0,0,1,-1 66.yorum: 0,0,0,0,-1,-1,0 67.yorum: 0,0,0,0,-1,-1,0 68.yorum: 0,0,0,0,0,-1,0 69.yorum: 0,1,0,0,0,0,0 70.yorum: 0,-1,0,0,0,0,-1 71.yorum: 0,0,-1,0,0,0,0 72.yorum: 0,0,0,0,-1,0,0 73.yorum: 0,0,-1,0,0,0,0 74.yorum: 0,0,0,0,0,0,0 75.yorum: 0,0,0,0,1,0,1 76.yorum: 0,0,0,0,-1,-1,0 77.yorum: 0,0,0,0,0,1,0 78.yorum: 0,0,0,0,0,1,0 79.yorum: 0,0,0,0,0,-1,0 80.yorum: 0,0,-1,0,0,-1,0 81.yorum: 0,0,0,0,0,0,-1 82.yorum: 0,1,1,0,0,0,1 83.yorum: 0,0,0,1,0,0,0 84.yorum: 0,0,0,0,1,1,0 85.yorum: 0,1,0,0,0,1,0 86.yorum: 1,0,0,0,0,1,0 87.yorum: 0,0,-1,0,0,0,0 88.yorum: 0,0,0,0,0,1,0 89.yorum: 0,0,1,-1,0,0,0 90.yorum: 0,0,0,0,1,0,0 91.yorum: 0,0,1,0,0,1,0 92.yorum: 0,0,1,0,-1,0,0 93.yorum: 0,0,0,0,0,1,0 94.yorum: 0,0,-1,0,-1,1,0 95.yorum: 1,0,-1,0,0,-1,0 96.yorum: 0,0,0,0,-1,0,0 97.yorum: 0,0,0,0,0,0,0 98.yorum: 0,-1,0,0,0,-1,0 99.yorum: 0,0,1,0,1,1,0 100.yorum: 0,0,1,0,1,-1,0 101.yorum: 0,0,0,0,0,-1,0 102.yorum: 1,0,0,0,0,0,0 103.yorum: 0,0,0,0,0,0,-1 104.yorum: 0,1,1,0,0,0,0 105.yorum: 0,-1,0,-1,0,0,-1 106.yorum: -1,0,0,0,0,-1,0 107.yorum: 0,0,0,0,0,0,-1 108.yorum: 0,0,0,0,0,0,0 109.yorum: 0,-1,0,0,-1,0,-1 110.yorum: 0,0,0,0,0,0,0 111.yorum: 0,1,0,0,0,0,0 112.yorum: 1,0,1,0,0,0,-1 113.yorum: 0,0,0,-1,0,0,0 114.yorum: 0,0,-1,0,0,0,-1 115.yorum: 0,0,0,1,0,1,0 116.yorum: 0,0,0,0,-1,0,0 117.yorum: 0,0,0,0,0,0,0 118.yorum: 1,0,0,0,0,0,0 119.yorum: 0,0,0,0,0,1,0 120.yorum: -1,0,0,0,0,0,0 121.yorum: 0,0,1,0,0,1,0 122.yorum: 0,0,0,0,0,0,0 123.yorum: 0,0,0,0,0,-1,0 124.yorum: 0,0,0,0,0,1,0 125.yorum: 0,0,0,0,0,-1,-1 126.yorum: 0,0,0,0,0,0,-1 127.yorum: 0,-1,0,0,0,0,0 128.yorum: 0,0,0,0,0,0,0 129.yorum: 0,0,0,1,1,0,0 130.yorum: 0,0,0,0,1,0,0 131.yorum: 0,0,-1,0,0,0,0 132.yorum: 0,0,-1,0,0,0,-1 133.yorum: 0,0,0,0,1,0,0 134.yorum: 0,0,0,0,0,-1,0 135.yorum: 0,0,0,0,0,0,0 136.yorum: 0,0,0,0,1,0,0 137.yorum: 0,0,-1,0,-1,0,0 138.yorum: 0,0,0,0,1,0,0 139.yorum: 0,0,0,0,0,0,0 140.yorum: 0,0,0,0,-1,-1,0 141.yorum: 0,0,0,0,0,0,-1 142.yorum: 0,0,1,0,-1,0,0 143.yorum: 1,0,0,0,0,1,0 144.yorum: 0,0,0,0,0,0,0 145.yorum: 0,0,0,0,0,1,0 146.yorum: 0,-1,0,0,0,0,-1 147.yorum: 0,0,0,0,-1,0,0 148.yorum: 1,0,0,0,0,0,0 149.yorum: 0,0,0,0,0,0,0 150.yorum: 0,0,-1,0,0,-1,0 151.yorum: 0,-1,0,0,0,0,-1 152.yorum: 0,0,-1,0,0,-1,0 153.yorum: 0,0,0,0,0,-1,-1 154.yorum: 0,0,0,0,0,-1,0 155.yorum: 1,0,0,0,0,1,0 156.yorum: 0,0,0,0,0,1,0 157.yorum: 0,0,0,0,1,0,0 158.yorum: 0,0,0,0,0,0,-1 159.yorum: 1,0,0,0,-1,0,0 160.yorum: 0,-1,0,0,0,-1,0 161.yorum: 0,0,0,0,0,0,-1 162.yorum: 0,0,0,0,1,0,0 163.yorum: 0,0,0,0,1,0,0 164.yorum: 0,0,0,0,0,0,0 165.yorum: 0,0,-1,0,0,-1,0 166.yorum: 0,0,0,0,0,0,0 167.yorum: 0,1,1,0,0,0,0 168.yorum: 0,0,0,0,1,0,0 169.yorum: 1,0,-1,0,1,0,0 170.yorum: 0,1,0,0,0,0,0 171.yorum: 0,0,0,0,0,0,0 172.yorum: 0,0,0,0,0,0,0 173.yorum: 0,-1,0,0,-1,-1,-1 174.yorum: 0,0,-1,0,0,-1,0 175.yorum: 0,0,0,0,-1,0,0 176.yorum: 0,0,0,0,0,-1,0 177.yorum: 0,0,0,0,1,0,0 178.yorum: 0,0,0,0,-1,1,0 179.yorum: 0,0,-1,0,0,-1,0 180.yorum: 0,0,1,0,0,0,0 181.yorum: 0,0,0,0,0,0,0 182.yorum: -1,0,0,0,0,0,0 183.yorum: 0,0,0,1,0,0,0 184.yorum: 0,0,0,0,0,0,0 185.yorum: 0,1,0,0,0,1,0 186.yorum: 0,1,0,0,0,0,0 187.yorum: 0,0,0,0,0,0,-1 188.yorum: 0,1,0,1,0,1,0 189.yorum: 0,1,0,0,0,-1,0 190.yorum: 1,0,0,0,0,0,0 191.yorum: 0,0,0,0,0,0,0 192.yorum: 0,-1,0,0,0,0,0 193.yorum: 0,0,0,-1,0,0,0 194.yorum: 0,0,1,0,0,1,0 195.yorum: 0,0,-1,-1,0,-1,0 196.yorum: 0,0,0,0,0,0,-1 197.yorum: 0,0,0,1,1,0,0 198.yorum: 0,0,-1,0,0,0,0 199.yorum: 0,0,0,0,1,0,0 200.yorum: 0,0,0,-1,0,0,0 201.yorum: 0,0,0,0,0,-1,0 202.yorum: 1,1,0,0,1,0,0 203.yorum: 0,0,0,0,1,0,0 204.yorum: 0,0,0,0,0,0,0 205.yorum: 0,0,0,0,-1,0,0 206.yorum: 0,0,-1,0,0,0,0 207.yorum: 0,0,-1,0,0,0,0 208.yorum: 0,0,0,0,0,0,-1 209.yorum: 0,0,-1,0,0,0,0 210.yorum: 0,0,-1,0,0,0,0 211.yorum: 0,0,-1,0,0,-1,-1 212.yorum: 0,0,-1,0,-1,-1,0 213.yorum: 0,0,0,0,-1,0,0 214.yorum: 0,1,0,0,0,0,0 215.yorum: 0,0,-1,0,0,-1,0 216.yorum: 0,0,-1,0,0,0,0 217.yorum: 0,-1,0,0,0,0,0 218.yorum: 0,-1,0,0,0,-1,0 219.yorum: 0,0,0,0,0,0,0 220.yorum: 0,0,0,0,1,0,0 221.yorum: 0,1,0,0,1,1,0 222.yorum: 0,-1,0,0,0,0,-1 223.yorum: 1,0,0,0,0,0,0 224.yorum: 0,0,0,0,1,0,0 225.yorum: 0,0,0,0,-1,0,0 226.yorum: 0,0,-1,-1,0,0,0 227.yorum: 0,0,1,0,0,0,0 228.yorum: 1,0,0,0,0,0,0 229.yorum: 0,0,-1,1,1,1,0 230.yorum: 0,0,0,0,0,-1,0 231.yorum: 0,0,0,0,-1,0,0 232.yorum: 0,0,0,0,0,0,-1 233.yorum: 0,0,0,0,0,0,-1 234.yorum: 1,0,0,0,0,-1,0 235.yorum: 0,0,0,0,0,0,0 236.yorum: 0,0,0,0,0,0,-1 237.yorum: 0,0,-1,0,1,0,0 238.yorum: 0,0,0,1,0,0,0 239.yorum: 0,-1,0,0,0,0,0 240.yorum: 0,0,0,0,0,0,-1 241.yorum: 1,0,-1,0,0,-1,0 242.yorum: 0,0,0,-1,0,0,0 243.yorum: 0,0,0,0,0,0,0 244.yorum: 0,0,1,0,0,0,0 245.yorum: 0,-1,0,0,0,0,0 246.yorum: 0,0,0,0,0,1,0 247.yorum: 1,0,0,0,0,0,0 248.yorum: 0,0,-1,0,0,0,0 249.yorum: 0,0,0,0,0,0,0 250.yorum: 0,0,1,0,0,0,0 251.yorum: 0,0,1,0,0,0,0 252.yorum: 0,1,0,1,0,0,0 253.yorum: 0,-1,0,0,0,0,0 254.yorum: 0,-1,0,0,0,0,0 255.yorum: 0,0,-1,0,0,0,0 256.yorum: 0,0,0,0,0,-1,-1 257.yorum: 0,-1,0,0,0,-1,-1 258.yorum: 0,0,0,-1,0,0,0 259.yorum: 0,0,0,0,0,0,0 260.yorum: 0,0,0,0,1,0,0 261.yorum: 0,-1,0,0,0,1,0 262.yorum: 0,0,-1,0,0,0,0 263.yorum: 0,0,-1,0,0,0,0 264.yorum: 0,0,-1,0,0,0,0 265.yorum: 0,0,-1,0,0,0,0 266.yorum: 0,-1,0,0,0,0,0 267.yorum: 0,0,0,0,1,0,0 268.yorum: 0,0,1,0,0,1,0 269.yorum: 1,0,0,0,1,1,0 270.yorum: 0,0,0,0,0,0,0 271.yorum: 0,0,0,0,0,0,0 272.yorum: 0,0,0,0,-1,0,0 273.yorum: 0,-1,-1,0,0,-1,0 274.yorum: 0,1,0,0,0,0,1 275.yorum: 0,0,0,0,0,0,-1 276.yorum: 0,0,0,0,0,0,0 277.yorum: 1,0,0,0,0,0,0 278.yorum: 0,0,0,0,0,-1,0 279.yorum: 0,0,0,0,0,1,0 280.yorum: 1,1,0,0,0,0,0 281.yorum: 0,0,0,0,-1,0,0 282.yorum: 0,0,0,0,0,0,-1 283.yorum: 0,0,0,0,0,1,1 284.yorum: 0,0,-1,0,0,0,0 285.yorum: 0,0,-1,0,0,0,0 286.yorum: 1,0,0,0,0,0,0 287.yorum: 1,1,0,0,0,1,0 288.yorum: 0,0,0,0,1,0,0 289.yorum: 1,1,0,0,0,0,0 290.yorum: 0,0,0,0,0,0,0 291.yorum: 0,0,0,0,0,-1,0 292.yorum: 0,0,0,0,-1,0,0 293.yorum: -1,0,0,0,-1,-1,0 294.yorum: 0,0,0,0,0,1,1 295.yorum: 0,0,0,0,-1,-1,0 296.yorum: 0,0,0,0,-1,-1,0 297.yorum: 0,1,0,0,1,0,0 298.yorum: 0,0,0,0,1,0,0 299.yorum: 0,0,0,0,0,1,0 300.yorum: 0,0,0,0,-1,0,0 301.yorum: 0,0,1,0,1,1,0 302.yorum: 0,0,0,0,0,0,0 303.yorum: 0,0,1,0,0,0,0 304.yorum: 0,0,-1,0,0,0,0 305.yorum: 0,0,0,0,0,0,0 306.yorum: 0,0,-1,0,0,0,0 307.yorum: 0,1,0,0,0,1,0 308.yorum: 0,0,0,0,0,0,-1 309.yorum: 0,0,0,-1,0,1,-1 310.yorum: 0,1,0,0,0,0,0 311.yorum: 0,0,0,0,0,0,0 312.yorum: 0,0,0,0,1,1,0 313.yorum: 0,0,0,0,0,-1,0 314.yorum: 0,0,0,-1,0,0,0 315.yorum: 0,0,0,1,0,0,0 316.yorum: 0,-1,-1,0,0,-1,-1 317.yorum: 0,-1,0,0,0,1,-1 318.yorum: 0,0,0,0,0,-1,0 319.yorum: 0,0,0,0,0,0,0 320.yorum: 0,0,-1,0,0,0,0 321.yorum: 0,0,-1,0,0,-1,0 322.yorum: 0,0,0,0,1,0,0 323.yorum: 0,0,-1,0,0,0,0 324.yorum: 0,0,0,0,0,0,0 325.yorum: 0,0,-1,0,0,0,0 326.yorum: 0,0,0,0,0,0,0 327.yorum: 0,0,0,1,1,0,0 328.yorum: 0,0,1,0,-1,0,0 329.yorum: 0,-1,0,0,0,0,0 330.yorum: 0,0,0,0,1,1,0 331.yorum: 0,0,0,0,0,0,0 332.yorum: 0,0,0,0,1,-1,0 333.yorum: 0,-1,0,0,0,0,0 334.yorum: 0,0,1,0,0,0,0 335.yorum: 1,0,-1,0,1,0,0 336.yorum: 0,0,0,0,-1,0,0 337.yorum: 0,-1,-1,0,0,0,0 338.yorum: 0,0,0,0,0,0,0 339.yorum: 0,-1,0,0,0,0,0 340.yorum: 0,0,0,0,1,0,0 341.yorum: 0,0,0,0,1,0,0 342.yorum: 1,0,0,0,0,-1,0 343.yorum: 0,0,0,-1,0,0,0 344.yorum: 0,0,-1,0,0,0,0 345.yorum: 0,1,0,0,0,1,0 346.yorum: 0,1,-1,0,0,0,0 347.yorum: 0,0,0,0,-1,-1,0 348.yorum: 0,0,0,0,0,0,0 349.yorum: 0,0,0,0,0,0,-1 350.yorum: 0,0,-1,0,0,-1,0 351.yorum: 0,0,-1,-1,-1,0,0 352.yorum: 0,0,0,0,0,-1,0 353.yorum: 1,0,0,0,0,0,0 354.yorum: 0,0,0,0,0,0,-1 355.yorum: 1,0,0,0,0,-1,0 356.yorum: 0,1,0,0,0,0,0 357.yorum: 0,0,0,0,-1,0,0 358.yorum: 0,0,1,0,0,0,0 359.yorum: 0,0,0,1,0,0,0 360.yorum: 0,0,-1,-1,0,-1,0 361.yorum: 0,0,0,0,0,1,0 362.yorum: 0,0,0,0,1,0,0 363.yorum: 0,0,0,0,-1,0,0 364.yorum: 0,1,0,0,0,0,0 365.yorum: 0,0,-1,0,0,0,0 366.yorum: 0,0,0,0,0,0,0 367.yorum: 0,0,0,0,1,0,0 368.yorum: 0,0,0,0,0,0,-1 369.yorum: 0,0,0,0,1,0,0 370.yorum: 0,0,0,0,-1,0,-1 371.yorum: 0,0,0,0,0,-1,0 372.yorum: 0,0,0,0,0,1,0 373.yorum: 0,-1,0,0,0,0,0 374.yorum: 0,0,-1,0,0,-1,0 375.yorum: 1,0,0,0,0,0,0 376.yorum: 0,0,0,0,1,0,0 377.yorum: 0,0,0,0,0,0,0 378.yorum: 0,0,0,0,-1,0,0 379.yorum: 0,0,0,0,-1,0,0 380.yorum: 0,-1,1,0,0,0,0 381.yorum: 0,0,0,0,-1,0,0 382.yorum: 0,0,0,0,-1,0,0 383.yorum: 0,1,0,0,0,0,0 384.yorum: 0,0,0,0,-1,-1,-1 385.yorum: 0,0,0,0,-1,0,0 386.yorum: 0,0,0,0,-1,0,0 387.yorum: 0,0,0,0,-1,0,0 388.yorum: 0,0,0,0,0,-1,-1 389.yorum: 0,0,-1,0,0,-1,0 390.yorum: 0,0,-1,0,0,0,0 391.yorum: 0,0,-1,0,0,-1,0 392.yorum: 0,0,0,0,0,0,0 393.yorum: 0,0,0,0,0,1,0 394.yorum: 0,0,-1,0,0,0,0 395.yorum: 0,0,-1,1,0,0,0 396.yorum: -1,0,0,0,0,0,0 397.yorum: 1,0,-1,0,0,0,0 398.yorum: 0,1,1,0,0,0,0 399.yorum: 0,0,1,0,0,0,0 400.yorum: 0,0,0,0,0,0,0 401.yorum: 0,0,0,0,0,1,-1 402.yorum: 1,1,1,1,0,0,0 403.yorum: 0,-1,0,0,0,-1,0 404.yorum: 0,1,0,0,1,0,0 405.yorum: 0,0,-1,0,0,0,0 406.yorum: 0,0,0,0,0,0,0 407.yorum: 0,0,0,0,0,-1,0 408.yorum: 0,0,0,0,0,0,0 409.yorum: 0,0,0,0,0,0,0 410.yorum: -1,0,0,0,0,0,0 411.yorum: 0,-1,-1,0,1,1,0 412.yorum: 1,0,0,0,0,0,0 413.yorum: 0,0,0,0,0,0,0 414.yorum: 0,0,0,0,0,0,-1 415.yorum: 0,-1,0,0,0,0,-1 416.yorum: 0,0,0,0,-1,0,0 417.yorum: 0,0,1,0,0,0,0 418.yorum: 0,0,0,1,0,0,-1 419.yorum: 0,0,0,0,1,0,0 420.yorum: 1,0,0,-1,1,0,0 421.yorum: 0,0,0,0,0,0,-1 422.yorum: 0,0,0,0,-1,0,0 423.yorum: 0,0,0,0,1,0,0 424.yorum: 0,0,0,0,1,1,0 425.yorum: 0,-1,0,0,0,0,1 426.yorum: 0,0,0,0,0,0,0 427.yorum: 0,0,-1,0,0,0,0 428.yorum: 0,0,0,-1,0,0,0 429.yorum: 0,-1,0,0,0,-1,-1 430.yorum: 0,1,0,0,0,0,0 431.yorum: 0,0,1,0,1,0,0 432.yorum: 0,0,0,0,0,0,-1 433.yorum: 0,0,0,0,-1,0,0 434.yorum: 0,0,0,0,1,0,0 435.yorum: 0,0,-1,0,0,0,0 436.yorum: 0,0,1,0,0,0,1 437.yorum: 0,0,0,0,-1,0,0 438.yorum: 0,0,-1,1,0,0,0 439.yorum: 0,0,0,1,0,0,0 440.yorum: -1,0,0,0,0,0,0 441.yorum: 0,0,0,-1,0,0,-1 442.yorum: 0,0,0,0,0,0,0 443.yorum: 0,-1,0,0,0,-1,0 444.yorum: 0,1,0,0,1,0,0 445.yorum: 0,0,0,0,0,-1,-1 446.yorum: 0,0,0,0,1,0,0 447.yorum: 1,1,0,0,1,0,0 448.yorum: 0,0,0,0,-1,-1,0 449.yorum: 1,0,1,0,0,1,0 450.yorum: 0,0,-1,0,0,0,0 451.yorum: 0,0,0,0,0,0,0 452.yorum: 0,1,0,0,0,1,0 453.yorum: 0,-1,0,0,0,0,-1 454.yorum: 0,0,0,0,1,0,0 455.yorum: 0,0,0,0,0,0,0 456.yorum: 1,0,0,0,-1,0,0 457.yorum: 0,-1,0,0,0,0,0 458.yorum: 0,0,0,0,0,-1,0 459.yorum: 0,0,0,0,0,0,0 460.yorum: 0,0,0,0,0,-1,-1 461.yorum: 0,0,0,1,0,1,0 462.yorum: 0,0,-1,0,0,-1,0 463.yorum: 0,0,1,0,0,1,0 464.yorum: 0,0,0,0,0,-1,0 465.yorum: 1,1,0,0,0,0,0 466.yorum: 0,0,0,0,0,0,0 467.yorum: 0,0,0,0,0,0,0 468.yorum: 0,0,0,0,0,1,0 469.yorum: 0,0,0,0,-1,-1,0 470.yorum: 0,0,-1,0,0,1,0 471.yorum: 0,0,0,0,0,0,0 472.yorum: 0,0,0,0,0,0,-1 473.yorum: 0,0,0,0,1,0,0 474.yorum: 0,0,-1,0,0,-1,0 475.yorum: 0,0,0,0,0,0,0 476.yorum: 0,0,0,0,0,-1,0 477.yorum: 0,0,1,0,1,0,0 478.yorum: 0,0,0,0,0,0,0 479.yorum: 0,0,0,0,0,0,0 480.yorum: 0,0,0,0,0,1,0 481.yorum: 0,1,0,0,1,0,0 482.yorum: 0,0,0,0,-1,-1,-1 483.yorum: -1,0,-1,0,0,-1,0 484.yorum: 0,-1,0,0,0,-1,-1 485.yorum: 0,0,0,0,0,0,0 486.yorum: 0,1,0,0,1,1,0 487.yorum: 0,0,0,0,0,1,0 488.yorum: 0,1,0,0,0,0,0 489.yorum: 0,0,1,0,1,1,0 490.yorum: 0,0,-1,0,0,0,0 491.yorum: 0,0,0,0,0,-1,-1 492.yorum: 0,0,0,0,0,0,0 493.yorum: 0,0,0,0,0,-1,0 494.yorum: 0,0,-1,0,0,0,0 495.yorum: 0,0,0,0,1,0,0 496.yorum: 0,0,1,1,0,0,0 497.yorum: 0,0,0,1,0,0,0 498.yorum: 0,0,0,0,1,0,0 499.yorum: 1,0,0,0,0,0,0 500.yorum: 1,0,0,0,0,0,0 501.yorum: 1,1,0,0,0,1,0 502.yorum: 0,0,0,0,0,0,0 503.yorum: 0,0,-1,0,0,0,0 504.yorum: 0,-1,0,0,0,0,0 505.yorum: 0,0,0,0,0,0,1 506.yorum: 0,0,0,0,0,1,0 507.yorum: 0,0,0,0,0,-1,0 508.yorum: 0,0,1,0,-1,0,0 509.yorum: 0,0,0,0,-1,0,0 510.yorum: 0,0,1,0,0,0,0 511.yorum: 0,0,0,0,0,0,0 512.yorum: 0,0,0,0,-1,-1,0 513.yorum: 1,0,0,0,0,0,0 514.yorum: 0,0,0,0,0,-1,0 515.yorum: 0,1,0,0,0,1,1 516.yorum: 0,0,0,0,1,1,0 517.yorum: 0,0,0,0,0,0,-1 518.yorum: 0,0,0,0,0,0,-1 519.yorum: 0,0,0,0,0,0,0 520.yorum: 0,0,0,0,0,0,0 521.yorum: 0,0,0,0,0,-1,0 522.yorum: 0,0,0,0,0,0,0 523.yorum: 0,0,0,0,1,0,0 524.yorum: 1,-1,0,0,0,-1,-1 525.yorum: 0,0,0,0,0,0,0 526.yorum: 0,0,-1,-1,0,0,0 527.yorum: 1,0,1,0,0,0,0 528.yorum: 0,1,0,0,0,0,0 529.yorum: 0,1,0,0,-1,0,0 530.yorum: 0,0,0,0,0,0,0 531.yorum: 0,0,0,0,0,-1,-1 532.yorum: 0,0,0,0,0,0,-1 533.yorum: 1,0,0,0,0,1,0 534.yorum: 1,0,0,0,0,-1,-1 535.yorum: 0,0,0,0,0,0,0 536.yorum: 0,0,0,0,0,-1,0 537.yorum: 0,0,-1,0,0,-1,0 538.yorum: 1,0,-1,0,0,0,0 539.yorum: 0,0,0,0,-1,0,-1 540.yorum: 0,0,-1,0,-1,0,0 541.yorum: 0,0,0,0,0,0,0 542.yorum: 0,0,0,0,-1,-1,-1 543.yorum: 0,0,0,0,1,1,0 544.yorum: 0,0,0,0,0,0,0 545.yorum: -1,0,0,0,-1,0,0 546.yorum: 0,0,1,1,0,-1,0 547.yorum: 0,0,-1,-1,0,0,0 548.yorum: 0,1,0,0,0,1,0 549.yorum: 0,0,0,0,0,0,0 550.yorum: 0,-1,0,0,0,-1,0 551.yorum: 0,0,0,0,-1,0,0 552.yorum: 0,0,1,0,-1,0,0 553.yorum: 0,0,0,0,0,0,0 554.yorum: 1,1,0,0,0,0,0 555.yorum: 0,1,0,0,0,0,1 556.yorum: 0,1,0,0,0,0,0 557.yorum: 0,0,0,0,0,0,1 558.yorum: 0,0,0,0,-1,0,0 559.yorum: 0,0,-1,0,0,0,0 560.yorum: 0,-1,0,0,-1,0,0 561.yorum: 0,0,0,0,0,0,0 562.yorum: 0,0,0,0,-1,0,0 563.yorum: 0,0,0,0,0,0,0 564.yorum: 0,0,0,0,1,0,0 565.yorum: 1,0,0,0,0,0,0 566.yorum: 0,1,0,0,1,1,0 567.yorum: 0,0,0,0,1,0,0 568.yorum: 0,1,0,0,0,0,0 569.yorum: 1,0,0,1,0,0,0 570.yorum: 0,1,0,0,0,0,0 571.yorum: 0,0,0,0,-1,0,0 572.yorum: 0,0,0,0,0,0,-1 573.yorum: 1,0,-1,0,0,0,0 574.yorum: 0,0,0,0,0,0,-1 575.yorum: 0,0,0,0,0,0,0 576.yorum: 0,0,0,0,0,0,0 577.yorum: 1,1,0,0,0,0,0 578.yorum: 0,0,-1,0,0,0,0 579.yorum: 0,0,1,0,0,0,0 580.yorum: 0,0,-1,0,0,-1,0 581.yorum: 0,0,0,0,0,-1,-1 582.yorum: 0,0,-1,0,0,0,0 583.yorum: 0,0,0,1,0,0,0 584.yorum: 0,0,0,0,0,0,-1 585.yorum: 0,0,0,0,0,0,0 586.yorum: 1,0,0,0,0,1,0 587.yorum: 0,0,-1,-1,0,0,0 588.yorum: 0,0,-1,0,0,-1,-1 589.yorum: 0,0,0,0,0,1,0 590.yorum: 0,0,0,0,-1,0,0 591.yorum: 0,0,0,0,1,0,0 592.yorum: 0,0,0,0,0,0,-1 593.yorum: 0,0,0,0,0,0,0 594.yorum: 0,1,0,0,0,0,0 595.yorum: 0,-1,0,0,0,-1,0 596.yorum: 1,0,0,0,0,0,0 597.yorum: 0,0,0,0,0,-1,0 598.yorum: 0,0,-1,0,0,0,0 599.yorum: 0,0,0,0,-1,0,0 600.yorum: 1,0,0,0,0,0,0 601.yorum: 0,0,1,0,1,0,0 602.yorum: 0,0,0,0,0,0,0 603.yorum: 0,-1,1,0,-1,0,0 604.yorum: 0,0,0,0,0,0,0 605.yorum: 0,1,0,0,0,0,0 606.yorum: 1,0,0,0,0,0,0 607.yorum: 0,0,0,-1,0,0,0 608.yorum: 0,0,0,-1,0,-1,0 609.yorum: 1,0,-1,0,0,0,0 610.yorum: 0,0,0,0,0,-1,0 611.yorum: 0,0,0,0,-1,0,0 612.yorum: 0,0,0,0,1,0,0 613.yorum: 0,0,1,0,1,0,0 614.yorum: 0,0,0,0,0,0,0 615.yorum: 0,0,1,0,0,0,0 616.yorum: 0,0,0,1,0,0,0 617.yorum: 1,0,0,0,0,1,0 618.yorum: 0,0,0,0,0,-1,0 619.yorum: 0,-1,0,0,0,0,0 620.yorum: 0,0,0,0,1,0,0 621.yorum: 0,0,0,0,0,-1,0 622.yorum: 0,0,0,0,-1,-1,0 623.yorum: 0,1,0,0,0,0,0 624.yorum: 0,0,0,0,0,0,0 625.yorum: 0,1,0,0,1,0,-1 626.yorum: 0,0,0,0,-1,-1,-1 627.yorum: 0,0,0,0,-1,0,0 628.yorum: 0,0,0,0,0,-1,0 629.yorum: 0,0,0,0,0,-1,0 630.yorum: 0,0,0,0,0,0,0 631.yorum: 0,0,0,0,0,0,0 632.yorum: 0,1,0,0,0,0,0 633.yorum: 0,0,-1,0,0,-1,0 634.yorum: 0,0,0,0,0,0,0 635.yorum: 0,0,0,0,1,-1,0 636.yorum: 1,0,0,0,1,-1,0 637.yorum: 0,1,0,1,0,0,0 638.yorum: 0,0,0,0,1,0,0 639.yorum: 0,0,-1,0,0,-1,0 640.yorum: 0,0,0,0,1,0,0 641.yorum: 1,0,0,0,0,-1,0 642.yorum: 0,-1,0,0,0,1,0 643.yorum: 0,-1,0,0,0,-1,0 644.yorum: 0,0,1,0,0,0,0 645.yorum: 0,1,0,0,0,0,0 646.yorum: 0,0,0,0,-1,0,0 647.yorum: 0,0,0,0,0,0,0 648.yorum: 0,0,0,0,-1,-1,0 649.yorum: 0,-1,0,0,0,0,0 650.yorum: 1,0,0,0,0,0,0 651.yorum: 0,0,0,0,0,0,0 652.yorum: 0,0,0,0,0,0,0 653.yorum: 0,0,-1,0,0,0,0 654.yorum: 0,1,0,0,1,0,1 655.yorum: 0,0,-1,0,0,0,0 656.yorum: 0,-1,0,0,0,-1,0 657.yorum: 0,0,0,0,-1,0,0 658.yorum: 0,0,0,0,-1,0,0 659.yorum: 0,0,0,0,0,0,0 660.yorum: 0,0,0,0,-1,0,0 661.yorum: 0,0,0,0,0,1,0 662.yorum: 0,0,0,0,0,0,0 663.yorum: 0,0,0,-1,0,0,0 664.yorum: 0,0,0,0,0,-1,0 665.yorum: 0,0,1,0,0,0,0 666.yorum: 0,-1,0,0,0,-1,-1 667.yorum: 0,-1,0,0,0,0,0 668.yorum: 0,0,0,0,1,0,0 669.yorum: 0,0,0,0,0,1,0 670.yorum: 0,0,-1,0,0,-1,0 671.yorum: 0,0,0,0,0,0,0 672.yorum: 0,0,0,-1,0,1,0 673.yorum: 0,0,0,0,1,0,0 674.yorum: 0,0,-1,0,0,-1,-1 675.yorum: 1,0,0,0,0,1,0 676.yorum: 0,-1,0,0,0,0,0 677.yorum: 1,0,-1,0,0,-1,0 678.yorum: 0,0,0,0,0,0,0 679.yorum: 0,0,0,0,0,0,0 680.yorum: 0,0,0,0,1,0,0 681.yorum: 0,0,1,0,0,0,0 682.yorum: 0,0,0,0,0,0,0 683.yorum: 1,1,0,0,0,0,0 684.yorum: 0,0,-1,0,0,0,0 685.yorum: 0,0,0,0,0,0,0 686.yorum: 0,-1,-1,0,0,0,0 687.yorum: 0,-1,0,0,0,0,0 688.yorum: 0,0,0,0,0,1,0 689.yorum: 0,-1,0,0,0,0,0 690.yorum: 0,0,0,0,0,-1,0 691.yorum: 1,0,0,0,1,0,0 692.yorum: 0,0,0,0,0,1,0 693.yorum: 0,0,-1,-1,0,0,0 694.yorum: 0,-1,0,0,0,0,0 695.yorum: 1,0,0,0,0,-1,0 696.yorum: 0,0,0,-1,0,0,0 697.yorum: 0,0,0,0,0,0,0 698.yorum: 0,0,1,0,0,0,0 699.yorum: 0,0,-1,0,0,0,0 700.yorum: 0,0,0,0,0,0,0 701.yorum: 0,1,0,0,0,0,1 702.yorum: 0,0,-1,0,0,1,0 703.yorum: 0,0,-1,0,0,-1,0 704.yorum: 0,1,0,1,0,0,0 705.yorum: 0,0,-1,0,0,-1,0 706.yorum: 0,-1,0,0,0,-1,0 707.yorum: 0,0,-1,0,0,-1,0 708.yorum: 1,0,0,0,0,1,0 709.yorum: 0,0,0,0,0,0,0 710.yorum: 0,0,0,0,0,0,0 711.yorum: 0,1,0,0,1,0,0 712.yorum: 0,0,0,0,0,0,1 713.yorum: 0,0,0,0,-1,0,0 714.yorum: 1,0,0,0,0,-1,0 715.yorum: 0,0,0,0,0,1,0 716.yorum: 0,0,0,0,0,1,0 717.yorum: 0,0,0,0,-1,0,0 718.yorum: 0,0,1,-1,0,1,0 719.yorum: 0,0,-1,0,0,-1,0 720.yorum: 0,0,0,0,0,0,0 721.yorum: 0,0,0,0,0,0,0 722.yorum: 0,0,-1,0,0,0,-1 723.yorum: 0,0,0,0,-1,0,0 724.yorum: 0,0,0,0,0,1,0 725.yorum: 0,0,0,0,0,0,0 726.yorum: 0,0,-1,0,0,-1,0 727.yorum: 0,0,0,0,0,0,0 728.yorum: 0,1,0,0,0,0,0 729.yorum: 0,0,0,-1,0,0,-1 730.yorum: 1,0,-1,0,0,-1,0 731.yorum: 0,0,0,0,0,0,0 732.yorum: 0,0,0,0,0,0,0 733.yorum: 0,0,0,0,1,0,0 734.yorum: 0,0,0,1,1,0,0 735.yorum: -1,0,-1,0,0,0,-1 736.yorum: 1,0,-1,0,0,-1,0 737.yorum: 0,-1,0,0,0,0,0 738.yorum: 0,0,0,0,-1,0,0 739.yorum: 0,-1,0,0,0,0,0 740.yorum: 0,0,0,0,0,0,0 741.yorum: 0,-1,0,0,-1,-1,0 742.yorum: 0,0,0,0,-1,0,0 743.yorum: 0,0,0,0,0,0,0 744.yorum: 0,0,-1,0,0,0,0 745.yorum: 0,0,-1,0,0,-1,-1 746.yorum: 1,0,0,0,0,0,0 747.yorum: 0,1,1,0,0,1,0 748.yorum: 0,0,-1,0,0,1,0 749.yorum: 0,0,0,1,0,0,0 750.yorum: 0,0,-1,0,0,0,-1 751.yorum: 0,0,0,0,0,0,0 752.yorum: 0,0,0,0,-1,0,0 753.yorum: 0,0,0,0,1,1,0 754.yorum: 0,0,0,0,0,-1,0 755.yorum: 0,-1,0,0,0,0,0 756.yorum: 0,0,-1,0,0,0,0 757.yorum: 0,0,0,0,1,1,0 758.yorum: 0,0,0,0,-1,0,0 759.yorum: 0,0,0,-1,0,0,0 760.yorum: 0,-1,0,0,0,0,-1 761.yorum: 1,0,0,0,1,-1,0 762.yorum: 0,0,0,-1,0,0,0 763.yorum: 0,0,0,0,0,0,0 764.yorum: 1,0,-1,0,0,-1,0 765.yorum: 0,0,0,0,0,0,0 766.yorum: 0,0,-1,0,0,-1,0 767.yorum: 0,1,0,0,1,0,0 768.yorum: 0,0,0,-1,0,0,0 769.yorum: 0,0,0,-1,0,0,0 770.yorum: 0,0,0,0,1,0,0 771.yorum: 0,0,0,0,-1,-1,0 772.yorum: 0,0,0,0,-1,0,0 773.yorum: 0,0,0,0,0,1,0 774.yorum: 0,0,1,0,0,1,0 775.yorum: 0,0,-1,0,0,0,0 776.yorum: 0,0,0,0,0,0,-1 777.yorum: 0,0,0,0,0,0,0 778.yorum: 0,0,0,0,1,0,0 779.yorum: 0,0,-1,0,0,-1,0 780.yorum: 0,0,0,0,0,0,0 781.yorum: 1,0,0,0,0,0,0 782.yorum: 0,1,0,0,0,0,0 783.yorum: 1,0,0,0,1,0,0 784.yorum: 0,0,-1,0,0,0,-1 785.yorum: 0,0,0,0,0,0,0 786.yorum: 0,1,0,0,1,0,0 787.yorum: 0,0,0,0,-1,0,0 788.yorum: 0,1,0,0,0,0,1 789.yorum: 0,0,0,0,1,-1,0 790.yorum: 0,0,-1,0,0,-1,0 791.yorum: 0,0,0,0,0,0,0 792.yorum: 0,0,-1,0,0,-1,0 793.yorum: 0,0,0,0,0,0,0 794.yorum: 0,0,0,0,0,1,1 795.yorum: 0,1,0,0,0,0,0 796.yorum: 0,0,0,0,0,1,0 797.yorum: 0,0,1,0,0,1,0 798.yorum: 0,0,-1,0,0,-1,0 799.yorum: 0,0,0,0,-1,0,0 800.yorum: 1,0,0,-1,0,0,0 801.yorum: 1,0,0,0,0,0,0 802.yorum: 0,0,0,0,0,0,0 803.yorum: 0,0,-1,0,0,-1,0 804.yorum: 0,0,0,0,0,1,0 805.yorum: 1,0,1,0,0,0,0 806.yorum: 0,0,0,0,1,0,0 807.yorum: 0,0,-1,0,-1,0,0 808.yorum: 0,0,-1,0,0,0,-1 809.yorum: 0,0,0,0,0,0,-1 810.yorum: 1,0,-1,0,1,0,0 811.yorum: 0,-1,0,0,0,0,0 812.yorum: 0,0,0,0,0,-1,0 813.yorum: 0,0,0,0,0,0,0 814.yorum: 0,0,-1,0,0,0,-1 815.yorum: 0,0,0,0,0,0,0 816.yorum: 0,0,0,0,1,0,0 817.yorum: 0,0,0,0,0,0,-1 818.yorum: 0,0,0,0,1,0,0 819.yorum: 0,1,-1,0,0,0,0 820.yorum: 0,0,0,0,0,0,-1 821.yorum: 0,0,0,0,-1,0,-1 822.yorum: 1,0,0,0,0,1,0 823.yorum: 0,-1,0,0,0,0,0 824.yorum: 0,0,0,-1,0,0,0 825.yorum: 1,0,0,0,0,1,0 826.yorum: 0,1,0,0,0,0,0 827.yorum: 1,0,0,0,0,0,0 828.yorum: 0,0,0,0,0,0,1 829.yorum: 0,0,0,0,0,0,0 830.yorum: 0,0,-1,0,0,0,0 831.yorum: 0,0,0,0,-1,0,0 832.yorum: 0,0,0,0,0,0,0 833.yorum: 0,0,0,0,1,0,0 834.yorum: 1,0,0,0,0,0,1 835.yorum: 0,0,-1,0,0,-1,0 836.yorum: 0,0,-1,0,0,0,-1 837.yorum: 0,0,-1,0,0,0,0 838.yorum: 0,0,1,0,0,0,0 839.yorum: 0,-1,0,0,-1,0,0 840.yorum: 0,1,0,0,0,1,0 841.yorum: 1,0,0,0,0,1,0 842.yorum: 0,0,0,0,-1,0,-1 843.yorum: 0,0,-1,0,0,-1,0 844.yorum: 0,0,-1,0,0,0,1 845.yorum: 0,0,-1,-1,0,0,0 846.yorum: 0,1,0,0,0,0,0 847.yorum: 1,0,-1,0,0,-1,0 848.yorum: 0,0,0,0,0,0,-1 849.yorum: 0,0,-1,0,-1,0,0 850.yorum: 1,0,0,0,0,0,0 851.yorum: 0,0,0,0,0,0,0 852.yorum: 0,0,0,0,1,0,0 853.yorum: 0,0,0,0,0,0,0 854.yorum: 0,0,-1,0,0,0,0 855.yorum: 1,1,0,0,0,0,1 856.yorum: 0,0,-1,-1,0,0,0 857.yorum: 0,0,0,0,0,1,0 858.yorum: 0,1,0,0,0,0,0 859.yorum: 0,-1,-1,0,0,0,0 860.yorum: 0,0,0,0,0,-1,-1 861.yorum: 0,0,0,0,0,0,0 862.yorum: 0,0,0,0,0,1,0 863.yorum: 0,0,0,0,1,1,0 864.yorum: 0,0,0,0,-1,-1,0 865.yorum: 0,0,0,-1,0,0,0 866.yorum: 0,1,1,0,0,0,1 867.yorum: 0,0,-1,0,0,0,-1 868.yorum: 0,0,0,-1,0,0,0 869.yorum: 0,-1,0,0,0,0,0 870.yorum: 0,0,0,0,0,0,0 871.yorum: 0,-1,0,0,0,0,0 872.yorum: 0,0,0,0,-1,0,0 873.yorum: 0,0,0,0,-1,-1,0 874.yorum: 0,0,0,0,0,0,-1 875.yorum: 0,1,0,0,0,0,1 876.yorum: 0,0,1,0,0,0,0 877.yorum: 0,0,-1,0,-1,0,0 878.yorum: 0,0,0,0,-1,-1,0 879.yorum: 0,0,-1,0,0,0,-1 880.yorum: 0,0,0,-1,0,0,0 881.yorum: 0,1,-1,0,0,0,0 882.yorum: 0,1,0,0,0,0,0 883.yorum: 1,0,0,0,-1,0,0 884.yorum: 0,0,0,0,0,0,0 885.yorum: 0,0,0,0,0,0,0 886.yorum: 0,0,1,1,0,0,0 887.yorum: 1,0,0,0,-1,0,0 888.yorum: 0,0,-1,0,-1,0,1 889.yorum: 1,0,0,0,0,1,0 890.yorum: 0,0,0,0,0,1,0 891.yorum: 0,0,0,0,-1,0,0 892.yorum: 0,0,1,0,0,0,0 893.yorum: 1,0,0,0,0,0,0 894.yorum: 0,0,0,0,0,0,0 895.yorum: 0,0,-1,0,0,-1,0 896.yorum: 0,0,0,0,1,0,0 897.yorum: 1,0,0,0,1,0,0 898.yorum: 0,0,0,-1,0,0,0 899.yorum: 0,0,0,0,0,0,-1 900.yorum: 0,0,1,0,0,0,0 901.yorum: 0,0,0,0,0,0,0 902.yorum: 0,1,0,1,-1,-1,0 903.yorum: 0,0,0,0,0,0,0 904.yorum: 0,0,0,0,0,1,0 905.yorum: 0,0,0,0,0,0,0 906.yorum: 0,0,-1,0,0,0,0 907.yorum: 0,0,-1,0,0,-1,0 908.yorum: 0,0,0,0,-1,0,0 909.yorum: 0,0,-1,0,0,-1,0 910.yorum: 0,0,0,0,0,0,0 911.yorum: 0,0,0,0,-1,0,0 912.yorum: 1,0,0,0,0,0,0 913.yorum: 0,0,0,0,-1,0,0 914.yorum: 0,0,0,0,0,0,0 915.yorum: 0,0,0,0,0,0,-1 916.yorum: 0,0,0,0,1,0,0 917.yorum: 0,0,0,0,0,0,0 918.yorum: 0,1,1,0,1,0,0 919.yorum: 0,0,0,0,0,0,0 920.yorum: 0,-1,0,0,0,0,-1 921.yorum: 1,1,0,0,0,0,0 922.yorum: 0,0,-1,-1,0,0,0 923.yorum: 0,0,0,0,1,0,0 924.yorum: 0,0,0,0,-1,-1,-1 925.yorum: 0,1,-1,0,0,-1,0 926.yorum: 0,0,0,0,1,0,0 927.yorum: 0,0,0,0,0,0,-1 928.yorum: 1,0,0,0,0,1,0 929.yorum: 0,0,0,0,0,0,0 930.yorum: 0,0,0,0,0,0,0 931.yorum: 0,1,0,0,0,0,0 932.yorum: 0,-1,0,0,0,0,0 933.yorum: 0,0,0,0,0,0,0 934.yorum: 0,0,0,0,0,-1,0 935.yorum: 0,-1,0,0,0,0,0 936.yorum: 0,1,0,0,0,0,0 937.yorum: 0,0,0,0,0,0,0 938.yorum: 1,0,0,0,0,0,0 939.yorum: 1,0,0,0,1,0,0 940.yorum: 0,1,-1,0,0,0,0 941.yorum: 0,0,0,0,1,0,0 942.yorum: 0,0,-1,0,0,0,0 943.yorum: 0,0,0,0,1,0,1 944.yorum: 0,0,0,0,0,0,-1 945.yorum: 0,0,0,0,1,0,0 946.yorum: 1,0,0,0,0,0,0 947.yorum: 0,0,0,0,0,1,0 948.yorum: 0,0,0,0,0,1,0 949.yorum: 0,0,0,0,0,0,-1 950.yorum: 0,0,0,0,1,0,0 951.yorum: -1,0,0,0,0,0,0 952.yorum: 0,-1,0,0,0,0,0 953.yorum: 0,0,0,0,0,1,0 954.yorum: 0,1,0,0,0,0,0 955.yorum: 0,0,-1,0,0,-1,0 956.yorum: 0,0,0,0,1,0,0 957.yorum: 0,0,0,0,0,1,0 958.yorum: 0,0,0,0,0,-1,0 959.yorum: 1,0,0,0,1,0,0 960.yorum: 0,0,-1,0,-1,0,0 961.yorum: 1,1,0,0,0,0,0 962.yorum: 0,1,0,0,1,0,0 963.yorum: 1,0,0,0,0,0,0 964.yorum: 0,0,0,0,0,0,0 965.yorum: 0,0,0,0,0,-1,0 966.yorum: 0,0,-1,0,0,0,0 967.yorum: 0,0,-1,0,1,0,0 968.yorum: 0,0,0,0,-1,0,0 969.yorum: 0,0,-1,0,-1,-1,0 970.yorum: 0,0,0,0,0,1,0 971.yorum: 1,0,1,0,0,1,0 972.yorum: 0,0,0,0,0,0,0 973.yorum: 0,-1,0,0,0,0,0 974.yorum: 0,0,0,-1,0,0,0 975.yorum: 0,0,0,0,1,1,0 976.yorum: 0,0,0,0,0,0,0 977.yorum: 0,0,0,0,0,0,0 978.yorum: -1,0,0,0,-1,0,0 979.yorum: 0,1,0,0,0,1,0 980.yorum: 0,0,-1,0,1,0,0 981.yorum: 0,0,-1,0,0,-1,0 982.yorum: 0,0,0,0,0,0,0 983.yorum: 0,0,0,0,-1,0,0 984.yorum: 0,0,0,1,0,0,0 985.yorum: 0,0,0,0,0,1,0 986.yorum: 0,0,-1,0,-1,0,0 987.yorum: 0,0,0,0,-1,0,0 988.yorum: 0,1,0,0,0,0,0 989.yorum: 0,0,0,0,-1,-1,0 990.yorum: 0,0,0,0,1,0,0 991.yorum: 0,1,0,0,1,0,0 992.yorum: 0,0,0,0,0,0,0 993.yorum: 1,0,0,0,0,-1,0 994.yorum: -1,0,0,0,0,0,0 995.yorum: 0,0,0,0,-1,0,0 996.yorum: 0,1,0,1,1,0,0 997.yorum: 0,0,-1,0,-1,0,0 998.yorum: 0,0,1,0,0,-1,0 999.yorum: 0,1,0,0,1,0,0 1000.yorum: 0,0,-1,0,-1,0,-1 1001.yorum: 0,0,0,-1,-1,0,0 1002.yorum: 0,0,0,0,0,-1,0 1003.yorum: 1,0,0,0,0,0,0 1004.yorum: 0,0,-1,0,0,0,0 1005.yorum: 0,0,0,0,0,-1,0 1006.yorum: 0,-1,0,0,0,0,0 1007.yorum: 0,0,0,0,1,0,0 1008.yorum: 0,-1,0,0,0,1,-1 1009.yorum: 0,-1,0,0,0,-1,0 1010.yorum: 0,-1,0,0,0,1,0 1011.yorum: -1,0,-1,0,0,0,0 1012.yorum: 0,1,0,0,1,0,0 1013.yorum: 0,0,-1,0,0,-1,-1 1014.yorum: 0,0,-1,0,0,-1,0 1015.yorum: 0,0,0,0,0,0,-1 1016.yorum: 0,0,0,0,0,0,0 1017.yorum: 0,0,0,0,1,1,0 1018.yorum: 0,0,0,0,0,0,0 1019.yorum: 0,0,0,-1,0,0,0 1020.yorum: 0,0,0,0,0,0,-1 1021.yorum: 0,0,0,0,0,0,0 1022.yorum: 1,0,0,0,1,1,0 1023.yorum: 0,0,0,0,0,-1,0 1024.yorum: 0,0,0,0,0,-1,0 1025.yorum: 1,0,-1,-1,0,0,0 1026.yorum: 0,0,0,0,0,0,0 1027.yorum: 0,1,0,0,0,0,0 1028.yorum: 0,-1,0,0,0,0,-1 1029.yorum: 0,-1,0,0,0,1,0 1030.yorum: 0,0,0,0,0,1,0 1031.yorum: 0,-1,0,0,0,-1,0 1032.yorum: 0,1,0,0,0,0,0 1033.yorum: 0,0,0,0,0,-1,0 1034.yorum: 1,0,0,0,0,0,0 1035.yorum: 0,0,0,0,0,-1,-1 1036.yorum: 0,0,0,-1,0,1,0 1037.yorum: 0,0,0,0,0,0,0 1038.yorum: 0,0,0,1,0,0,0 1039.yorum: 0,0,-1,0,0,0,0 1040.yorum: 0,0,0,0,1,0,0 1041.yorum: 0,0,0,1,0,1,0 1042.yorum: 0,0,0,0,0,-1,0 1043.yorum: 0,-1,0,0,1,0,0 1044.yorum: 0,0,0,0,0,0,-1 1045.yorum: 0,0,0,0,0,1,0 1046.yorum: 0,0,0,0,0,0,0 1047.yorum: 0,0,0,0,0,0,0 1048.yorum: 0,0,0,0,0,0,0 1049.yorum: 0,0,0,0,0,0,0 1050.yorum: 0,-1,0,0,0,0,0 1051.yorum: 1,0,0,0,0,0,0 1052.yorum: 1,0,0,0,0,0,0 1053.yorum: 0,0,0,0,-1,0,0 1054.yorum: 0,0,0,0,1,1,0 1055.yorum: 0,0,0,0,0,-1,0 1056.yorum: 0,0,0,0,0,0,-1 1057.yorum: 0,0,-1,0,0,0,0 1058.yorum: 1,0,0,0,0,0,0 1059.yorum: 0,0,0,0,-1,0,0 1060.yorum: 0,0,0,0,0,-1,0 1061.yorum: 0,0,0,0,0,0,0 1062.yorum: 0,0,0,0,0,0,0 1063.yorum: 0,0,-1,0,0,0,0 1064.yorum: 1,0,0,0,0,0,0 1065.yorum: 0,0,-1,0,0,0,0 1066.yorum: 0,0,0,0,0,1,0 1067.yorum: 0,0,0,0,0,0,0 1068.yorum: 0,0,0,0,0,0,0 1069.yorum: 0,0,1,0,0,0,0 1070.yorum: 1,0,-1,0,0,0,0 1071.yorum: 0,-1,0,0,-1,0,0 1072.yorum: 0,0,0,0,0,0,0 1073.yorum: 0,-1,0,0,0,0,-1 1074.yorum: 0,0,0,0,0,0,0 1075.yorum: 0,0,0,0,1,0,0 1076.yorum: 0,0,0,1,1,1,0 1077.yorum: 0,0,-1,-1,0,0,0 1078.yorum: 0,0,-1,0,0,-1,-1 1079.yorum: 0,0,-1,0,0,-1,0 1080.yorum: 0,0,0,0,0,-1,0 1081.yorum: 0,0,0,0,-1,0,0 1082.yorum: 0,0,0,0,-1,1,0 1083.yorum: 0,0,0,0,0,1,0 1084.yorum: 1,0,-1,0,1,0,0 1085.yorum: 1,0,0,0,1,0,0 1086.yorum: 0,-1,0,0,1,0,0 1087.yorum: 0,-1,0,0,0,0,0 1088.yorum: 0,0,0,0,0,0,0 1089.yorum: 0,0,0,0,0,1,0 1090.yorum: 0,1,0,0,0,0,0 1091.yorum: 1,1,0,0,1,0,0 1092.yorum: 0,0,1,0,0,0,0 1093.yorum: 0,0,-1,0,0,0,0 1094.yorum: 0,0,0,0,0,0,0 1095.yorum: 0,0,0,0,1,0,0 1096.yorum: 0,0,-1,1,0,0,0 1097.yorum: 0,0,0,0,1,1,0 1098.yorum: 0,0,0,0,0,0,0 1099.yorum: 0,0,0,0,0,0,-1 1100.yorum: 0,0,0,0,-1,0,0 1101.yorum: 0,0,0,0,0,1,0 1102.yorum: 0,0,0,0,0,0,0 1103.yorum: 1,0,0,0,0,1,0 1104.yorum: 0,0,0,0,0,1,0 1105.yorum: 0,0,0,0,0,0,0 1106.yorum: 0,0,0,0,0,0,0 1107.yorum: 0,0,0,0,-1,0,0 1108.yorum: 0,0,0,0,-1,0,0 1109.yorum: 0,0,0,0,0,0,0 1110.yorum: 0,0,0,0,0,0,-1 1111.yorum: 0,0,0,0,0,-1,-1 1112.yorum: 0,0,-1,0,0,0,0 1113.yorum: 1,0,0,0,1,0,0 1114.yorum: 0,0,0,0,0,0,0 1115.yorum: 0,0,-1,0,0,-1,0 1116.yorum: 0,-1,0,0,0,-1,0 1117.yorum: 0,0,-1,0,0,1,0 1118.yorum: 0,0,0,0,0,-1,0 1119.yorum: 0,0,0,0,0,0,0 1120.yorum: 0,0,0,0,0,1,0 1121.yorum: 0,0,0,0,0,0,0 1122.yorum: 0,0,0,0,0,-1,-1 1123.yorum: 0,0,0,0,0,0,-1 1124.yorum: 0,0,0,0,0,0,0 1125.yorum: 0,0,0,0,-1,0,0 1126.yorum: 0,0,0,0,-1,0,0 1127.yorum: 0,0,0,0,0,1,1 1128.yorum: 0,1,0,0,0,0,0 1129.yorum: 0,0,-1,0,1,0,0 1130.yorum: 0,0,-1,0,0,1,0 1131.yorum: 0,0,0,0,-1,0,0 1132.yorum: 0,0,0,0,0,-1,0 1133.yorum: 1,0,0,0,1,0,0 1134.yorum: 0,0,0,0,1,0,0 1135.yorum: 0,0,0,0,-1,-1,0 1136.yorum: 0,0,0,0,-1,0,0 1137.yorum: 0,0,0,-1,0,0,0 1138.yorum: 0,0,0,0,1,1,0 1139.yorum: 0,0,0,0,0,0,-1 1140.yorum: 0,0,0,-1,0,-1,0 1141.yorum: 0,0,1,1,0,0,0 1142.yorum: 0,0,0,0,0,-1,0 1143.yorum: 0,0,0,0,0,0,0 1144.yorum: 0,0,0,0,0,-1,0 1145.yorum: 0,1,0,0,0,0,0 1146.yorum: 0,0,0,0,1,0,0 1147.yorum: 0,0,0,0,-1,0,0 1148.yorum: 0,0,-1,0,-1,0,0 1149.yorum: 0,0,-1,0,0,0,0 1150.yorum: 1,1,0,0,1,0,0 1151.yorum: 0,0,-1,0,0,0,0 1152.yorum: 0,0,0,0,0,-1,0 1153.yorum: 0,0,0,0,0,0,0 1154.yorum: 0,0,-1,0,0,-1,0 1155.yorum: 0,0,0,0,0,0,-1 1156.yorum: 0,0,0,0,0,-1,0 1157.yorum: 0,0,-1,0,-1,0,0 1158.yorum: 1,0,0,0,0,0,0 1159.yorum: 0,0,0,0,0,-1,0 1160.yorum: 0,-1,0,0,0,0,0 1161.yorum: 0,0,0,-1,0,0,-1 1162.yorum: 1,0,-1,0,0,0,0 1163.yorum: 0,0,0,0,0,0,0 1164.yorum: 0,0,0,0,1,0,0 1165.yorum: 0,0,-1,0,0,-1,0 1166.yorum: 0,0,0,-1,0,0,0 1167.yorum: 0,0,0,0,0,-1,0 1168.yorum: 0,0,0,0,-1,0,0 1169.yorum: 0,0,0,0,0,0,0 1170.yorum: 0,0,0,1,0,0,0 1171.yorum: 0,0,0,0,0,-1,0 1172.yorum: 0,1,0,0,1,0,1 1173.yorum: 0,0,0,0,0,0,0 1174.yorum: 0,0,1,0,0,0,0 1175.yorum: 0,0,0,0,0,0,0 1176.yorum: 0,0,0,0,1,0,0 1177.yorum: 1,0,0,0,0,0,0 1178.yorum: 0,0,0,0,0,-1,0 1179.yorum: 1,0,0,0,0,0,0 1180.yorum: 0,0,0,0,0,0,0 1181.yorum: 0,0,0,0,1,0,0 1182.yorum: 0,0,-1,0,1,0,1 1183.yorum: 0,0,0,0,0,0,-1 1184.yorum: 0,0,0,0,0,-1,-1 1185.yorum: 0,0,0,0,1,0,-1 1186.yorum: 0,0,0,0,0,0,0 1187.yorum: 0,0,0,0,0,0,0 1188.yorum: 0,0,0,0,-1,-1,0 1189.yorum: 1,0,0,0,-1,-1,0 1190.yorum: 0,0,-1,-1,0,0,0 1191.yorum: 0,1,0,0,-1,1,0 1192.yorum: 0,-1,0,0,1,0,0 1193.yorum: 0,0,0,0,-1,0,0 1194.yorum: 0,0,-1,0,0,0,0 1195.yorum: 0,0,0,0,0,0,0 1196.yorum: 0,0,0,0,-1,0,0 1197.yorum: 1,0,1,1,0,1,0 1198.yorum: 0,0,0,0,0,0,-1 1199.yorum: 0,0,0,0,0,0,0 1200.yorum: 0,0,1,0,1,0,0 1201.yorum: 0,0,0,0,0,0,0 1202.yorum: 0,-1,-1,0,0,0,-1 1203.yorum: 0,0,1,0,0,1,0 1204.yorum: 0,0,0,0,0,0,1 1205.yorum: 0,0,1,0,0,1,1 1206.yorum: 0,1,0,0,0,0,0 1207.yorum: 0,0,0,0,1,0,0 1208.yorum: 0,-1,0,0,0,0,-1 1209.yorum: 0,0,0,0,0,0,0 1210.yorum: 0,0,0,0,-1,-1,0 1211.yorum: 0,0,-1,0,-1,1,0 1212.yorum: 1,0,0,0,1,1,0 1213.yorum: 1,0,0,0,-1,0,0 1214.yorum: 0,0,-1,0,0,0,0 1215.yorum: 0,-1,0,0,0,-1,0 1216.yorum: 0,-1,-1,0,0,0,-1 1217.yorum: 0,0,0,0,-1,-1,0 1218.yorum: 0,0,-1,0,0,-1,0 1219.yorum: 0,0,0,0,0,1,0 1220.yorum: 0,0,0,0,0,0,0 1221.yorum: 0,-1,0,0,0,0,0 1222.yorum: 0,0,0,0,0,0,0 1223.yorum: 0,0,-1,0,0,-1,0 1224.yorum: 0,0,-1,0,0,0,0 1225.yorum: 0,0,0,0,0,0,0 1226.yorum: 0,0,-1,0,0,0,0 1227.yorum: 0,1,0,0,0,0,0 1228.yorum: 0,0,0,0,0,1,0 1229.yorum: 0,0,1,0,0,0,0 1230.yorum: 0,0,-1,0,0,0,0 1231.yorum: 0,0,0,0,1,0,1 1232.yorum: 0,0,0,0,0,0,0 1233.yorum: 1,1,0,0,0,0,1 1234.yorum: 0,0,0,0,0,0,-1 1235.yorum: 0,-1,0,0,0,0,0 1236.yorum: 1,0,0,0,1,0,0 1237.yorum: 0,0,0,0,-1,0,0 1238.yorum: 0,0,0,0,0,1,0 1239.yorum: 0,0,-1,1,0,-1,0 1240.yorum: 0,0,0,0,0,0,0 1241.yorum: 1,0,0,0,-1,0,0 1242.yorum: 0,0,0,0,0,0,0 1243.yorum: 0,0,0,1,0,0,0 1244.yorum: 0,-1,-1,0,0,-1,-1 1245.yorum: 0,0,0,0,0,0,-1 1246.yorum: 0,-1,0,0,0,0,0 1247.yorum: 0,0,0,0,0,0,-1 1248.yorum: 0,0,0,0,0,0,0 1249.yorum: 0,0,0,-1,0,0,0 1250.yorum: 0,0,0,0,0,-1,-1 1251.yorum: 0,0,0,0,1,1,0 1252.yorum: 0,1,0,0,1,0,0 1253.yorum: 1,0,0,0,1,1,0 1254.yorum: 0,-1,0,0,0,-1,0 1255.yorum: 0,0,-1,0,0,0,0 1256.yorum: 0,-1,0,0,0,-1,0 1257.yorum: 0,1,0,0,1,0,0 1258.yorum: 0,0,0,0,0,0,0 1259.yorum: 0,0,0,0,0,0,0 1260.yorum: 0,0,0,0,0,1,0 1261.yorum: 0,0,0,0,1,0,0 1262.yorum: 0,0,0,0,-1,0,0 1263.yorum: -1,0,0,0,0,0,0 1264.yorum: 0,-1,0,0,0,0,0 1265.yorum: 0,1,0,0,0,0,1 1266.yorum: 0,0,0,0,0,0,0 1267.yorum: 1,1,0,0,0,0,0 1268.yorum: 0,0,1,0,0,1,0 1269.yorum: 1,0,0,0,0,0,0 1270.yorum: 1,0,0,0,0,0,0 1271.yorum: 0,0,0,0,0,0,-1 1272.yorum: 0,0,0,0,1,0,0 1273.yorum: 0,0,0,0,0,0,0 1274.yorum: 0,0,0,0,0,0,0 1275.yorum: 0,0,-1,0,0,0,0 1276.yorum: 0,0,0,0,0,0,0 1277.yorum: 0,0,0,0,0,0,0 1278.yorum: 0,0,0,0,0,0,0 1279.yorum: 0,0,0,0,0,0,0 1280.yorum: 0,-1,0,0,0,-1,0 1281.yorum: 1,0,0,0,0,0,0 1282.yorum: 1,0,0,0,0,0,0 1283.yorum: 0,0,0,0,-1,-1,0 1284.yorum: 1,0,0,1,0,0,0 1285.yorum: 0,0,0,0,0,0,0 1286.yorum: 0,0,0,0,1,0,0 1287.yorum: 0,0,-1,0,0,-1,0 1288.yorum: 0,0,0,0,1,0,0 1289.yorum: 0,0,0,-1,0,0,0 1290.yorum: 0,0,0,0,1,0,0 1291.yorum: 0,0,-1,0,0,0,0 1292.yorum: 0,0,0,0,0,-1,-1 1293.yorum: 0,1,1,0,0,0,1 1294.yorum: 0,0,-1,0,0,0,0 1295.yorum: 0,0,0,0,1,0,0 1296.yorum: 0,0,0,0,-1,-1,0 1297.yorum: 0,0,-1,0,0,0,0 1298.yorum: 0,0,0,1,0,0,0 1299.yorum: 0,0,0,0,0,1,0 1300.yorum: 0,0,0,0,1,0,0 1301.yorum: 0,-1,0,1,0,0,0 1302.yorum: 0,0,0,0,0,0,-1 1303.yorum: 0,0,-1,0,0,0,0 1304.yorum: 0,0,0,0,0,0,0 1305.yorum: 0,0,-1,-1,-1,0,0 1306.yorum: 0,0,0,0,0,-1,0 1307.yorum: 0,0,-1,0,0,-1,0 1308.yorum: 0,0,-1,0,0,-1,0 1309.yorum: 0,-1,0,0,0,0,0 1310.yorum: 0,0,0,0,-1,0,0 1311.yorum: 0,0,-1,0,0,0,0 1312.yorum: 0,0,1,0,0,0,0 1313.yorum: 0,0,0,0,0,-1,0 1314.yorum: 0,1,0,0,0,0,0 1315.yorum: 0,0,0,0,0,0,0 1316.yorum: 0,0,-1,0,0,0,0 1317.yorum: 1,0,0,0,0,0,0 1318.yorum: 0,1,0,0,0,0,0 1319.yorum: 0,-1,0,0,0,1,0 1320.yorum: 0,0,0,0,0,-1,-1 1321.yorum: 0,0,0,0,0,-1,-1 1322.yorum: 0,0,0,1,0,0,0 1323.yorum: 0,0,0,0,0,-1,0 1324.yorum: 0,0,-1,0,0,-1,0 1325.yorum: 0,1,0,0,1,0,0 1326.yorum: 0,0,0,0,0,1,0 1327.yorum: 0,0,0,0,0,0,-1 1328.yorum: 0,0,0,0,0,0,0 1329.yorum: 0,0,-1,0,0,0,0 1330.yorum: 0,0,0,0,0,0,-1 1331.yorum: 0,0,0,0,-1,0,0 1332.yorum: 0,0,0,-1,0,-1,0 1333.yorum: 0,0,0,0,0,0,0 1334.yorum: 1,0,0,0,0,1,0 1335.yorum: 0,0,0,0,0,1,0 1336.yorum: 0,0,0,0,-1,0,0 1337.yorum: 0,-1,0,0,0,-1,0 1338.yorum: 0,0,0,0,1,0,0 1339.yorum: 0,0,-1,0,0,-1,0 1340.yorum: 0,0,0,0,0,0,-1 1341.yorum: 0,1,0,0,0,0,0 1342.yorum: 0,0,-1,0,-1,0,0 1343.yorum: 0,0,0,0,0,1,0 1344.yorum: 0,0,0,0,0,1,0 1345.yorum: 0,1,0,0,0,0,0 1346.yorum: 0,1,0,0,1,0,0 1347.yorum: 0,0,0,0,-1,-1,0 1348.yorum: 0,0,0,0,0,0,0 1349.yorum: 0,0,1,0,-1,0,0 1350.yorum: 1,0,0,0,0,0,0 1351.yorum: 0,1,0,0,0,0,0 1352.yorum: 0,0,0,0,0,0,-1 1353.yorum: 0,0,0,0,1,0,0 1354.yorum: 0,0,-1,0,-1,0,0 1355.yorum: 0,0,0,0,0,-1,-1 1356.yorum: 0,0,0,1,0,0,0 1357.yorum: 0,0,0,0,0,0,-1 1358.yorum: 0,0,0,0,0,0,0 1359.yorum: -1,0,0,0,0,0,0 1360.yorum: 0,0,0,0,0,0,0 1361.yorum: 0,0,0,0,0,0,0 1362.yorum: 0,0,0,0,0,0,0 1363.yorum: 0,0,0,0,1,0,0 1364.yorum: 0,0,-1,0,0,0,0 1365.yorum: 1,0,0,0,1,0,0 1366.yorum: 0,-1,0,0,1,-1,0 1367.yorum: 0,0,0,1,0,1,0 1368.yorum: 0,-1,0,0,0,-1,0 1369.yorum: 0,0,0,-1,0,0,-1 1370.yorum: -1,0,-1,0,0,-1,-1 1371.yorum: 0,0,0,0,0,1,0 1372.yorum: 0,-1,0,0,0,1,0 1373.yorum: 0,-1,1,0,0,-1,0 1374.yorum: 0,0,-1,0,-1,0,0 1375.yorum: 0,0,-1,0,0,0,0 1376.yorum: 0,0,0,1,0,0,0 1377.yorum: 0,0,0,0,0,0,0 1378.yorum: 0,0,0,0,0,1,0 1379.yorum: 0,1,0,0,0,0,0 1380.yorum: 0,0,0,0,1,0,0 1381.yorum: 0,0,-1,0,0,0,-1 1382.yorum: 0,0,0,0,-1,0,0 1383.yorum: 0,0,0,0,0,1,0 1384.yorum: 0,0,0,0,0,1,0 1385.yorum: 0,0,0,0,-1,0,0 1386.yorum: 0,0,0,0,0,-1,-1 1387.yorum: 0,0,0,0,0,0,0 1388.yorum: 1,0,1,0,1,1,0 1389.yorum: 0,0,0,0,0,-1,0 1390.yorum: 0,0,0,0,0,1,0 1391.yorum: 0,0,-1,0,0,-1,0 1392.yorum: 0,0,0,0,0,-1,1 1393.yorum: 0,0,0,0,0,0,0 1394.yorum: 0,0,-1,-1,0,0,0 1395.yorum: 0,0,-1,0,-1,0,0 1396.yorum: 0,0,0,0,-1,0,0 1397.yorum: 0,1,0,0,1,0,1 1398.yorum: 0,0,0,0,0,0,0 1399.yorum: 0,0,0,-1,0,0,0 1400.yorum: 0,0,0,0,0,0,0 1401.yorum: 0,0,0,0,-1,-1,-1 1402.yorum: 0,0,-1,0,0,0,0 1403.yorum: 0,0,0,0,0,1,0 1404.yorum: 0,0,0,0,1,0,0 1405.yorum: 0,1,0,0,0,0,0 1406.yorum: 0,0,0,0,0,1,0 1407.yorum: 0,-1,-1,0,-1,-1,-1 1408.yorum: 0,0,-1,0,0,-1,0 1409.yorum: 0,0,0,0,0,0,0 1410.yorum: 1,0,-1,0,0,-1,0 1411.yorum: 0,0,0,0,1,0,0 1412.yorum: 0,0,0,0,0,-1,0 1413.yorum: 0,0,0,-1,0,0,-1 1414.yorum: 0,0,-1,0,0,-1,0 1415.yorum: 0,0,-1,0,0,0,0 1416.yorum: 1,0,0,0,0,0,0 1417.yorum: 0,0,0,0,0,0,0 1418.yorum: 1,-1,0,0,0,1,0 1419.yorum: 0,0,0,0,0,0,0 1420.yorum: 0,-1,0,0,0,0,0 1421.yorum: 0,0,0,0,0,-1,0 1422.yorum: 0,0,0,0,1,0,1 1423.yorum: 0,0,0,0,-1,0,0 1424.yorum: 0,1,0,0,0,1,0 1425.yorum: 0,0,-1,0,0,0,0 1426.yorum: 0,0,1,0,0,0,0 1427.yorum: 0,0,0,0,0,0,-1 1428.yorum: 0,1,0,0,0,0,0 1429.yorum: 0,0,0,1,0,0,0 1430.yorum: 0,0,0,0,0,-1,0 1431.yorum: 0,0,0,0,0,0,0 1432.yorum: 0,0,0,0,-1,0,0 1433.yorum: 0,1,0,0,0,1,0 1434.yorum: 0,0,0,0,0,0,0 1435.yorum: 1,0,0,0,0,1,0 1436.yorum: 0,1,0,0,0,0,0 1437.yorum: 0,-1,0,0,0,0,-1 1438.yorum: 0,0,0,0,0,0,-1 1439.yorum: 1,0,0,0,-1,-1,0 1440.yorum: 0,0,0,0,0,1,0 1441.yorum: 0,0,0,0,0,0,0 1442.yorum: 0,0,-1,0,-1,0,0 1443.yorum: 0,0,0,0,-1,-1,0 1444.yorum: 0,-1,-1,0,0,0,0 1445.yorum: 0,0,0,0,0,0,0 1446.yorum: 0,0,0,0,0,0,-1 1447.yorum: 1,0,-1,0,0,-1,0 1448.yorum: 0,0,-1,0,0,0,0 1449.yorum: 0,0,0,0,0,0,0 1450.yorum: 0,0,0,-1,0,-1,0 1451.yorum: 1,0,-1,0,0,-1,0 1452.yorum: 0,0,-1,0,0,0,0 1453.yorum: 0,0,0,0,0,0,0 1454.yorum: 0,0,-1,0,0,-1,0 1455.yorum: 0,-1,0,0,0,0,0 1456.yorum: 0,1,0,0,0,0,0 1457.yorum: 0,1,1,0,0,0,0 1458.yorum: 0,0,0,0,0,1,0 1459.yorum: 0,0,-1,0,0,0,0 1460.yorum: 1,0,0,0,0,1,0 1461.yorum: 0,0,0,0,0,-1,0 1462.yorum: 1,0,0,0,0,1,0 1463.yorum: 0,0,0,0,-1,0,0 1464.yorum: 0,0,0,0,-1,0,0 1465.yorum: 0,0,-1,-1,0,0,0 1466.yorum: 0,0,0,0,1,0,0 1467.yorum: 0,0,0,0,0,0,0 1468.yorum: 0,0,-1,0,-1,0,0 1469.yorum: 0,1,0,0,0,0,0 1470.yorum: 0,0,0,0,1,0,0 1471.yorum: 0,0,0,0,0,0,0 1472.yorum: 0,1,0,0,0,0,0 1473.yorum: 0,0,0,0,0,0,0 1474.yorum: 0,0,0,0,1,0,0 1475.yorum: 1,0,-1,0,0,-1,0 1476.yorum: 0,0,0,0,1,1,0 1477.yorum: 0,0,-1,0,0,0,0 1478.yorum: 1,0,0,0,1,0,0 1479.yorum: 0,-1,0,0,0,0,0 1480.yorum: 0,0,0,0,-1,0,0 1481.yorum: 0,-1,-1,0,0,-1,0 1482.yorum: 1,1,0,0,0,0,0 1483.yorum: 0,-1,0,0,0,0,0 1484.yorum: 0,0,0,0,-1,0,0 1485.yorum: 0,0,0,0,-1,-1,0 1486.yorum: 0,0,0,0,1,0,0 1487.yorum: 0,-1,0,0,0,0,0 1488.yorum: 0,0,0,0,1,0,0 1489.yorum: 0,1,0,0,0,0,0 1490.yorum: 0,0,0,0,1,0,0 1491.yorum: 0,1,0,0,0,1,0 1492.yorum: 0,0,0,0,-1,0,0 1493.yorum: 1,0,0,0,0,0,0 1494.yorum: 0,0,0,0,0,0,-1 1495.yorum: 0,-1,0,0,0,0,-1 1496.yorum: 0,0,0,0,0,0,0 1497.yorum: 0,0,0,0,0,0,0 1498.yorum: 0,-1,0,0,0,-1,0 1499.yorum: 0,0,0,0,-1,0,0 1500.yorum: 0,0,0,0,0,-1,-1 1501.yorum: 0,-1,0,0,0,0,0 1502.yorum: 0,-1,-1,0,0,0,-1 1503.yorum: 0,0,-1,0,0,0,0 1504.yorum: 0,0,0,0,-1,0,0 1505.yorum: 0,0,0,0,0,-1,0 1506.yorum: 0,0,1,-1,0,0,0 1507.yorum: 0,1,0,1,0,0,0 1508.yorum: 1,1,0,0,1,1,1 1509.yorum: 0,0,0,0,1,1,0 1510.yorum: 0,0,0,1,0,0,0 1511.yorum: 0,-1,0,0,0,0,0 1512.yorum: 0,0,0,0,0,0,-1 1513.yorum: 0,0,-1,0,0,-1,-1 1514.yorum: 0,-1,0,0,-1,-1,-1 1515.yorum: 0,0,0,0,0,-1,0 1516.yorum: 0,-1,0,0,0,-1,0 1517.yorum: 0,0,0,-1,0,0,0 1518.yorum: 0,0,-1,0,0,0,0 1519.yorum: 0,0,0,0,1,0,0 1520.yorum: 0,0,0,0,1,0,0 1521.yorum: 0,0,0,0,1,0,0 1522.yorum: 0,0,0,1,0,0,0 1523.yorum: 1,0,0,0,0,0,0 1524.yorum: 0,0,0,0,0,0,0 1525.yorum: 0,-1,0,0,0,0,-1 1526.yorum: 1,0,0,0,1,0,0 1527.yorum: 0,0,1,0,1,1,0 1528.yorum: 0,0,0,0,-1,-1,0 1529.yorum: 0,0,0,0,0,0,-1 1530.yorum: 0,0,0,0,0,0,0 1531.yorum: 0,-1,0,0,0,0,0 1532.yorum: 0,0,0,0,0,0,0 1533.yorum: 0,0,0,0,0,1,0 1534.yorum: 1,0,0,0,0,0,0 1535.yorum: 1,0,0,0,1,0,0 1536.yorum: 0,0,-1,0,0,-1,0 1537.yorum: 0,0,-1,0,-1,0,0 1538.yorum: 1,0,0,0,0,1,0 1539.yorum: 0,0,0,0,1,0,0 1540.yorum: 0,0,0,0,1,0,0 1541.yorum: 1,0,0,0,1,0,0 1542.yorum: 0,0,1,0,0,0,0 1543.yorum: 0,0,-1,0,0,0,0 1544.yorum: 1,0,0,0,1,0,0 1545.yorum: -1,0,0,0,-1,0,0 1546.yorum: 0,0,0,-1,0,0,0 1547.yorum: 0,0,-1,0,-1,0,0 1548.yorum: 0,0,0,0,0,0,-1 1549.yorum: 0,0,0,0,0,0,0 1550.yorum: 0,0,0,0,0,0,0 1551.yorum: 0,0,-1,0,0,0,0 1552.yorum: 0,0,0,0,0,0,0 1553.yorum: 0,1,0,0,0,0,0 1554.yorum: 1,0,0,1,0,0,0 1555.yorum: 0,0,-1,0,0,0,0 1556.yorum: 0,0,0,0,0,0,0 1557.yorum: 0,1,0,0,0,0,0 1558.yorum: 0,0,0,0,1,1,0 1559.yorum: 0,0,0,0,0,0,0 1560.yorum: 1,0,0,0,1,0,0 1561.yorum: 0,0,0,0,0,0,0 1562.yorum: 0,0,0,0,0,-1,0 1563.yorum: 1,0,1,0,0,0,0 1564.yorum: 0,1,0,0,0,-1,0 1565.yorum: 0,0,0,-1,0,0,0 1566.yorum: 0,0,0,0,1,0,0 1567.yorum: 0,0,0,0,-1,0,0 1568.yorum: 0,0,0,0,0,0,0 1569.yorum: 1,-1,-1,0,0,-1,0 1570.yorum: 0,0,0,0,0,-1,0 1571.yorum: 1,0,-1,0,0,-1,0 1572.yorum: 0,0,0,0,1,0,-1 1573.yorum: 0,0,-1,0,0,0,0 1574.yorum: 0,0,0,0,1,0,0 1575.yorum: 0,1,0,0,0,0,0 1576.yorum: 1,0,0,0,1,0,0 1577.yorum: 0,0,0,0,0,-1,0 1578.yorum: 0,0,0,0,0,1,0 1579.yorum: 0,0,0,0,0,-1,0 1580.yorum: 0,-1,0,0,0,1,0 1581.yorum: 0,-1,0,0,0,0,-1 1582.yorum: 0,0,0,0,0,-1,0 1583.yorum: 0,0,-1,0,0,-1,0 1584.yorum: 0,0,0,0,0,0,0 1585.yorum: 0,0,0,0,0,-1,0 1586.yorum: 0,-1,0,0,0,-1,0 1587.yorum: 0,0,-1,0,0,0,-1 1588.yorum: 1,0,1,0,1,1,0 1589.yorum: 0,0,-1,-1,0,0,0 1590.yorum: 0,0,0,0,0,0,-1 1591.yorum: 0,-1,0,0,0,-1,0 1592.yorum: 0,1,0,1,0,0,0 1593.yorum: 0,0,0,0,0,0,0 1594.yorum: 0,0,-1,0,0,0,0 1595.yorum: 0,0,0,0,-1,-1,0 1596.yorum: 0,0,0,0,0,-1,0 1597.yorum: 0,1,0,0,0,0,0 1598.yorum: 0,0,0,0,0,0,0 1599.yorum: 0,0,0,1,0,0,0 1600.yorum: 0,0,0,0,0,0,0 1601.yorum: 0,0,0,0,0,-1,-1 1602.yorum: 0,0,0,0,0,0,-1 1603.yorum: 0,0,-1,0,0,-1,0 1604.yorum: 0,0,0,0,1,0,0 1605.yorum: 0,0,0,0,-1,0,0 1606.yorum: 0,0,0,0,0,0,0 1607.yorum: 0,0,1,1,0,0,0 1608.yorum: 0,1,0,0,0,0,1 1609.yorum: 1,0,0,0,0,-1,0 1610.yorum: 0,0,-1,1,0,0,0 1611.yorum: 0,0,0,0,0,0,0 1612.yorum: 0,0,0,-1,0,0,0 1613.yorum: 0,0,-1,0,0,-1,0 1614.yorum: 1,0,0,0,0,0,1 1615.yorum: 0,-1,0,0,0,0,-1 1616.yorum: 0,-1,0,0,0,0,0 1617.yorum: 0,0,0,0,0,0,1 1618.yorum: 1,0,0,1,0,1,0 1619.yorum: 0,0,0,0,0,0,0 1620.yorum: 0,1,0,0,0,0,1 1621.yorum: 0,0,0,0,0,-1,-1 1622.yorum: 0,0,0,0,1,0,0 1623.yorum: 0,0,0,-1,0,0,0 1624.yorum: 0,0,0,0,0,0,0 1625.yorum: 0,0,1,0,1,1,0 1626.yorum: 0,0,1,0,0,0,0 1627.yorum: 1,0,-1,0,0,0,0 1628.yorum: 0,0,0,0,0,0,-1 1629.yorum: 0,0,-1,0,-1,0,0 1630.yorum: 1,0,0,0,0,0,0 1631.yorum: 0,0,0,0,1,1,0 1632.yorum: 0,0,-1,0,0,0,0 1633.yorum: 0,0,0,0,0,0,0 1634.yorum: 0,0,-1,0,0,-1,0 1635.yorum: 0,0,0,0,-1,0,0 1636.yorum: 0,0,0,0,0,1,0 1637.yorum: 0,0,0,0,1,0,0 1638.yorum: 0,-1,0,0,0,0,-1 1639.yorum: 0,0,-1,-1,0,0,0 1640.yorum: 0,0,0,0,1,0,1 1641.yorum: 0,0,0,0,1,0,0 1642.yorum: -1,0,0,0,-1,0,0 1643.yorum: 0,0,0,0,1,0,0 1644.yorum: 1,0,0,0,0,0,0 1645.yorum: 0,0,0,0,-1,0,-1 1646.yorum: 0,-1,0,0,0,0,0 1647.yorum: 1,0,0,0,0,0,0 1648.yorum: 0,1,0,0,0,0,0 1649.yorum: 0,0,1,0,0,0,0 1650.yorum: 0,1,0,0,0,1,0 1651.yorum: 0,0,0,-1,0,0,0 1652.yorum: 0,0,0,0,-1,0,0 1653.yorum: 0,0,0,0,0,0,0 1654.yorum: 0,0,1,0,0,0,0 1655.yorum: 0,0,0,0,-1,0,0 1656.yorum: 0,-1,0,0,0,-1,0 1657.yorum: 0,0,0,0,0,-1,0 1658.yorum: 0,0,1,0,0,1,0 1659.yorum: 0,-1,-1,0,0,0,0 1660.yorum: 0,0,0,0,-1,0,0 1661.yorum: 1,0,0,0,0,0,0 1662.yorum: 0,1,-1,0,0,0,0 1663.yorum: 0,-1,-1,0,0,-1,0 1664.yorum: 0,0,0,0,-1,0,0 1665.yorum: 0,0,-1,0,0,0,0 1666.yorum: 0,0,0,0,0,-1,-1 1667.yorum: 0,0,0,0,1,0,0 1668.yorum: 0,1,0,0,0,1,0 1669.yorum: 0,0,0,0,1,0,0 1670.yorum: 1,0,-1,0,0,0,0 1671.yorum: 0,0,0,0,1,0,0 1672.yorum: 0,0,0,0,0,0,0 1673.yorum: 0,0,0,0,0,1,0 1674.yorum: 0,0,0,1,0,1,0 1675.yorum: 0,0,0,0,-1,0,0 1676.yorum: 1,0,0,-1,0,0,0 1677.yorum: 0,0,-1,0,-1,0,-1 1678.yorum: 0,0,0,0,-1,0,0 1679.yorum: 0,0,0,0,0,0,-1 1680.yorum: 0,0,-1,0,0,0,-1 1681.yorum: 0,0,0,0,0,-1,-1 1682.yorum: 0,0,0,0,-1,0,0 1683.yorum: 0,0,0,0,0,1,0 1684.yorum: 0,1,0,0,0,0,0 1685.yorum: 0,1,0,0,-1,0,-1 1686.yorum: 0,0,-1,0,1,0,0 1687.yorum: 0,0,1,0,0,0,0 1688.yorum: 0,0,0,0,0,0,0 1689.yorum: 0,0,0,0,0,0,1 1690.yorum: 1,0,0,0,0,0,0 1691.yorum: 0,0,0,0,1,0,0 1692.yorum: 0,0,0,0,1,0,0 1693.yorum: 0,-1,0,0,0,0,0 1694.yorum: 0,-1,0,0,0,0,-1 1695.yorum: 0,0,0,0,0,0,0 1696.yorum: 0,0,0,0,0,0,0 1697.yorum: 0,0,0,0,0,0,-1 1698.yorum: 0,1,0,0,0,0,0 1699.yorum: 0,0,-1,0,-1,-1,0 1700.yorum: 0,0,0,0,0,0,0 1701.yorum: 0,0,-1,0,1,0,0 1702.yorum: 0,0,0,0,-1,0,0 1703.yorum: 0,0,0,0,0,-1,0 1704.yorum: 0,0,-1,0,0,-1,0 1705.yorum: 1,0,0,0,0,0,0 1706.yorum: 1,0,0,0,1,0,0 1707.yorum: 1,0,0,0,0,-1,0 1708.yorum: 0,0,-1,1,0,0,0 1709.yorum: 1,0,0,0,0,0,0 1710.yorum: 0,0,0,0,0,-1,0 1711.yorum: 0,0,0,0,0,0,-1 1712.yorum: 0,0,0,0,1,0,0 1713.yorum: 0,0,0,0,0,0,0 1714.yorum: 0,0,0,0,0,0,0 1715.yorum: 0,0,0,0,1,0,0 1716.yorum: 0,0,0,0,1,0,0 1717.yorum: 0,0,0,0,-1,-1,0 1718.yorum: 0,0,-1,1,0,0,0 1719.yorum: 0,0,0,0,-1,0,0 1720.yorum: 0,0,0,0,0,0,0 1721.yorum: 0,0,1,1,0,0,0 1722.yorum: 1,-1,0,0,1,0,0 1723.yorum: 0,0,1,0,-1,0,0 1724.yorum: 1,0,0,0,0,0,0 1725.yorum: 0,0,0,0,0,-1,0 1726.yorum: 0,0,0,0,-1,0,0 1727.yorum: 0,-1,0,0,0,0,0 1728.yorum: 0,0,0,0,-1,-1,0 1729.yorum: 0,0,0,0,-1,0,0 1730.yorum: 0,0,0,0,-1,0,0 1731.yorum: 0,0,0,0,0,1,0 1732.yorum: 0,0,0,0,0,-1,0 1733.yorum: 0,0,0,0,0,0,0 1734.yorum: 0,-1,0,0,0,0,0 1735.yorum: 0,0,0,0,0,-1,-1 1736.yorum: 0,0,0,0,0,0,0 1737.yorum: 0,0,0,0,0,0,-1 1738.yorum: 0,0,-1,0,0,0,0 1739.yorum: 0,0,0,0,0,1,0 1740.yorum: 1,0,0,0,0,0,0 1741.yorum: 0,-1,0,0,1,0,0 1742.yorum: 0,0,0,0,0,-1,0 1743.yorum: 0,1,0,0,1,-1,0 1744.yorum: 0,0,0,0,0,0,0 1745.yorum: 0,0,-1,0,0,-1,-1 1746.yorum: 0,0,0,0,0,-1,-1 1747.yorum: 0,0,0,0,-1,0,0 1748.yorum: 0,0,0,1,0,0,0 1749.yorum: 0,0,0,0,0,1,0 1750.yorum: 0,-1,0,0,0,-1,0 1751.yorum: 0,0,0,0,1,0,0 1752.yorum: 0,0,0,1,0,0,0 1753.yorum: 0,0,-1,0,0,0,0 1754.yorum: 1,0,-1,0,1,0,0 1755.yorum: 0,0,0,0,-1,0,0 1756.yorum: 0,-1,0,0,0,-1,-1 1757.yorum: 1,0,0,0,1,0,0 1758.yorum: 0,0,0,0,0,-1,0 1759.yorum: 0,0,-1,0,0,-1,0 1760.yorum: 0,0,0,0,0,-1,0 1761.yorum: 0,1,0,0,0,1,0 1762.yorum: 0,0,-1,0,0,-1,0 1763.yorum: 0,0,0,0,1,0,0 1764.yorum: 0,0,0,0,0,-1,0 1765.yorum: 1,0,0,0,0,0,0 1766.yorum: 0,1,0,0,0,0,0 1767.yorum: 0,0,-1,0,0,-1,0 1768.yorum: 0,0,1,0,0,0,0 1769.yorum: 0,0,0,0,0,-1,0 1770.yorum: 0,0,0,0,1,0,0 1771.yorum: 0,1,0,0,0,-1,0 1772.yorum: 0,0,0,0,-1,0,0 1773.yorum: 0,0,0,0,0,0,0 1774.yorum: 0,0,0,0,-1,0,0 1775.yorum: 0,0,-1,0,0,-1,0 1776.yorum: 0,0,1,0,1,0,0 1777.yorum: 0,1,0,0,0,0,0 1778.yorum: 0,0,0,-1,0,0,0 1779.yorum: 0,0,-1,0,0,0,-1 1780.yorum: 0,-1,0,0,0,0,-1 1781.yorum: 0,0,0,0,0,1,0 1782.yorum: 0,-1,0,0,0,0,0 1783.yorum: 0,0,0,0,0,0,0 1784.yorum: 1,0,0,0,0,0,0 1785.yorum: 0,0,0,-1,0,0,0 1786.yorum: 0,0,1,1,0,0,0 1787.yorum: 0,1,0,0,0,1,0 1788.yorum: 0,0,0,-1,0,0,0 1789.yorum: 0,0,0,0,0,0,0 1790.yorum: 0,1,0,0,0,0,0 1791.yorum: 1,0,0,0,0,-1,0 1792.yorum: 0,1,0,-1,0,0,0 1793.yorum: 0,0,0,0,0,-1,0 1794.yorum: 0,0,0,0,0,1,0 1795.yorum: 0,0,-1,0,-1,0,0 1796.yorum: 0,1,0,0,1,0,0 1797.yorum: 0,0,0,0,-1,0,0 1798.yorum: 0,-1,0,0,0,0,0 1799.yorum: 1,0,0,0,0,0,0 1800.yorum: 1,0,0,0,1,0,0 1801.yorum: 0,0,0,0,0,1,0 1802.yorum: 0,0,0,0,0,0,1 1803.yorum: 0,0,0,0,-1,-1,-1 1804.yorum: 0,-1,0,0,0,0,0 1805.yorum: 0,0,0,0,1,0,0 1806.yorum: 0,1,0,0,0,0,0 1807.yorum: 0,-1,0,0,0,0,0 1808.yorum: 0,0,0,0,0,1,0 1809.yorum: 0,0,0,0,1,0,0 1810.yorum: 0,0,0,0,0,0,0 1811.yorum: 0,0,0,0,1,0,0 1812.yorum: 0,0,0,0,-1,-1,0 1813.yorum: 0,0,0,0,0,0,0 1814.yorum: 1,0,-1,0,0,-1,0 1815.yorum: 0,0,0,0,-1,-1,0 1816.yorum: 0,0,0,0,1,1,0 1817.yorum: 0,0,-1,0,0,0,0 1818.yorum: 0,0,0,0,-1,-1,0 1819.yorum: 0,-1,0,0,0,0,0 1820.yorum: 0,0,1,0,0,0,0 1821.yorum: 0,1,0,-1,0,0,0 1822.yorum: 1,1,0,0,0,0,1 1823.yorum: 0,0,0,0,-1,0,0 1824.yorum: 0,0,-1,0,0,0,0 1825.yorum: 0,0,0,0,-1,0,0 1826.yorum: 0,1,0,0,0,0,0 1827.yorum: 0,-1,0,0,0,0,-1 1828.yorum: 0,0,0,0,-1,0,-1 1829.yorum: 0,0,0,0,-1,-1,-1 1830.yorum: -1,0,0,0,0,0,0 1831.yorum: 1,-1,0,0,0,0,0 1832.yorum: 1,0,0,0,0,0,0 1833.yorum: 0,0,0,0,1,0,0 1834.yorum: 0,0,0,0,0,0,0 1835.yorum: 1,0,0,0,0,0,0 1836.yorum: 0,1,0,0,0,0,0 1837.yorum: 0,0,0,0,1,0,0 1838.yorum: 0,0,0,-1,0,0,0 1839.yorum: 0,0,0,-1,0,0,0 1840.yorum: 0,0,0,0,0,0,0 1841.yorum: 0,0,0,0,-1,0,0 1842.yorum: 0,0,-1,0,0,0,0 1843.yorum: 0,0,0,0,1,0,0 1844.yorum: 0,0,-1,0,0,0,0 1845.yorum: 0,1,0,0,0,0,0 1846.yorum: 0,0,0,0,-1,-1,0 1847.yorum: 0,0,1,0,0,0,0 1848.yorum: 0,-1,0,0,1,0,0 1849.yorum: 0,0,0,-1,0,0,0 1850.yorum: 0,1,0,0,0,0,0 1851.yorum: 0,0,0,0,-1,-1,0 1852.yorum: 0,-1,0,0,0,0,0 1853.yorum: -1,0,0,0,0,0,0 1854.yorum: 0,0,0,0,0,-1,0 1855.yorum: 0,-1,0,0,0,0,-1 1856.yorum: 0,0,0,-1,0,0,-1 1857.yorum: 0,0,0,0,0,0,1 1858.yorum: 0,-1,0,0,0,0,0 1859.yorum: 0,0,0,0,1,0,0 1860.yorum: 0,0,0,0,0,0,0 1861.yorum: 0,-1,0,0,0,0,-1 1862.yorum: 0,0,0,0,0,1,0 1863.yorum: 0,0,0,0,-1,0,0 1864.yorum: 0,0,-1,0,0,0,0 1865.yorum: 0,0,0,0,1,1,0 1866.yorum: 0,0,0,0,1,0,0 1867.yorum: 0,0,-1,0,0,0,0 1868.yorum: 0,0,0,0,0,0,0 1869.yorum: 0,-1,0,0,0,0,0 1870.yorum: 0,0,0,0,-1,-1,0 1871.yorum: 0,0,1,0,0,1,0 1872.yorum: 0,0,0,0,1,0,0 1873.yorum: 0,0,0,0,1,-1,0 1874.yorum: 0,0,0,0,-1,0,0 1875.yorum: 0,0,0,0,1,0,0 1876.yorum: 0,0,0,0,-1,-1,-1 1877.yorum: 0,-1,0,0,0,0,-1 1878.yorum: 0,0,0,0,1,0,0 1879.yorum: 0,0,0,1,0,0,0 1880.yorum: 0,0,0,0,1,0,0 1881.yorum: 0,0,0,0,1,1,0 1882.yorum: 0,0,1,0,0,0,0 1883.yorum: 0,0,1,0,0,0,0 1884.yorum: 0,-1,0,0,0,0,-1 1885.yorum: 0,-1,0,0,-1,-1,0 1886.yorum: 0,0,0,0,0,-1,0 1887.yorum: 0,0,-1,0,0,-1,0 1888.yorum: 0,0,0,0,-1,0,0 1889.yorum: 0,0,0,0,0,1,1 1890.yorum: 0,0,0,0,1,0,0 1891.yorum: 0,0,0,-1,0,0,0 1892.yorum: 1,0,0,0,0,0,0 1893.yorum: 1,0,0,0,1,0,0 1894.yorum: 0,0,0,0,0,-1,0 1895.yorum: 0,-1,0,0,0,-1,0 1896.yorum: 0,-1,0,0,0,0,-1 1897.yorum: 1,0,0,0,0,0,0 1898.yorum: 0,0,-1,0,0,0,0 1899.yorum: 0,0,1,0,0,0,0 1900.yorum: 0,0,0,0,-1,0,0 1901.yorum: 0,0,0,0,-1,0,0 1902.yorum: -1,0,0,-1,0,-1,0 1903.yorum: 0,0,0,0,0,1,0 1904.yorum: 0,0,0,0,0,1,0 1905.yorum: 0,0,0,0,0,0,0 1906.yorum: -1,0,0,0,0,-1,0 1907.yorum: 0,-1,0,0,0,0,0 1908.yorum: 0,0,0,0,-1,0,0 1909.yorum: 0,0,0,0,0,0,0 1910.yorum: 0,1,0,0,1,1,1 1911.yorum: 1,0,0,0,0,1,0 1912.yorum: 0,-1,0,0,0,0,-1 1913.yorum: 0,0,-1,0,0,0,0 1914.yorum: 1,0,0,0,0,0,0 1915.yorum: 0,0,0,0,0,0,0 1916.yorum: 1,0,1,0,0,0,0 1917.yorum: 0,0,1,0,0,0,0 1918.yorum: 0,0,0,0,0,0,0 1919.yorum: 0,1,0,0,0,0,0 1920.yorum: 0,-1,0,-1,0,0,-1 1921.yorum: 1,0,1,0,0,0,0 1922.yorum: 0,0,0,0,-1,-1,0 1923.yorum: 0,-1,0,0,0,0,0 1924.yorum: 0,0,0,0,0,0,0 1925.yorum: 0,-1,0,0,0,-1,0 1926.yorum: 0,0,0,0,0,0,0 1927.yorum: 0,-1,0,0,0,0,0 1928.yorum: 0,0,0,0,0,-1,0 1929.yorum: 0,1,0,0,0,0,0 1930.yorum: 0,1,0,0,0,0,0 1931.yorum: 0,-1,0,0,0,0,-1 1932.yorum: 0,0,0,0,-1,0,-1 1933.yorum: 0,0,0,0,0,1,0 1934.yorum: 0,0,0,0,-1,0,0 1935.yorum: 0,-1,0,0,0,-1,0 1936.yorum: 0,0,-1,0,0,0,0 1937.yorum: 0,-1,0,0,0,-1,0 1938.yorum: 0,0,-1,0,0,0,0 1939.yorum: 0,0,0,0,1,0,0 1940.yorum: 0,0,0,0,0,-1,0 1941.yorum: 0,-1,0,0,0,0,0 1942.yorum: 0,0,0,0,0,0,0 1943.yorum: 0,0,-1,0,0,0,0 1944.yorum: 0,0,0,0,-1,-1,0 1945.yorum: 0,0,0,0,-1,-1,0 1946.yorum: 0,0,-1,1,0,-1,0 1947.yorum: 0,-1,0,0,0,0,0 1948.yorum: 0,0,0,0,1,0,0 1949.yorum: 0,0,0,0,1,0,0 1950.yorum: -1,0,0,0,0,0,0 1951.yorum: 0,0,0,0,1,0,1 1952.yorum: 0,1,0,0,0,0,0 1953.yorum: 0,0,0,0,-1,-1,0 1954.yorum: 0,0,-1,0,0,-1,0 1955.yorum: 0,0,0,0,-1,0,0 1956.yorum: 0,1,0,0,1,0,0 1957.yorum: 0,0,0,0,0,0,0 1958.yorum: 0,1,0,0,0,0,1 1959.yorum: 0,0,0,0,0,0,0 1960.yorum: 0,0,1,0,0,0,0 1961.yorum: 0,0,0,0,1,0,0 1962.yorum: 0,0,0,0,0,-1,0 1963.yorum: 0,0,0,0,0,-1,0 1964.yorum: 0,-1,0,0,0,0,-1 1965.yorum: 0,0,-1,0,0,0,0 1966.yorum: 1,0,0,-1,0,0,0 1967.yorum: 1,0,0,0,1,1,0 1968.yorum: 0,-1,0,0,0,0,-1 1969.yorum: 0,0,0,1,0,0,0 1970.yorum: 0,0,0,0,0,1,0 1971.yorum: -1,0,0,0,0,0,0 1972.yorum: 0,0,0,0,-1,-1,0 1973.yorum: 0,0,0,0,-1,-1,0 1974.yorum: 0,0,0,0,0,0,0 1975.yorum: 0,-1,0,0,0,0,-1 1976.yorum: 0,0,0,0,1,0,0 1977.yorum: 0,0,0,0,-1,0,0 1978.yorum: 0,0,0,0,-1,-1,0 1979.yorum: 0,0,0,0,-1,-1,0 1980.yorum: 0,0,0,-1,0,0,0 1981.yorum: 0,-1,1,0,0,0,0 1982.yorum: 0,-1,-1,0,0,-1,-1 1983.yorum: 0,-1,0,0,0,0,-1 1984.yorum: 0,0,0,0,1,0,0 1985.yorum: 0,0,0,0,0,-1,0 1986.yorum: 0,-1,0,0,0,0,-1 1987.yorum: 0,0,0,0,0,0,0 1988.yorum: 0,0,0,0,-1,0,0 1989.yorum: 0,-1,0,0,0,-1,0 1990.yorum: 0,0,0,-1,-1,0,0 1991.yorum: 0,0,0,0,0,0,-1 1992.yorum: 0,0,0,0,0,0,1 1993.yorum: 0,-1,0,0,0,0,-1 1994.yorum: 1,0,0,0,0,0,0 1995.yorum: 0,0,-1,0,0,0,0 1996.yorum: 0,0,0,0,0,1,0 1997.yorum: 0,-1,0,0,0,0,0 1998.yorum: 0,-1,0,0,0,0,-1 1999.yorum: 0,0,-1,0,0,0,0\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p1IZLu7Z6j7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "\n",
        "\n",
        "try:\n",
        "    df_ham = pd.read_csv('ham_5000_yorum.csv')\n",
        "    print(f\"âœ… Orijinal dosya okundu. Toplam satÄ±r: {len(df_ham)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"HATA: 'ham_5000_yorum.csv' dosyasÄ± bulunamadÄ±. LÃ¼tfen dosya yolunu kontrol et.\")\n",
        "    df_ham = pd.DataFrame({'review_text': [f\"Yorum {i}\" for i in range(5000)]})\n",
        "\n",
        "\n",
        "print(\"ðŸ”„ Veriler taranÄ±yor ve ayrÄ±ÅŸtÄ±rÄ±lÄ±yor...\")\n",
        "\n",
        "\n",
        "pattern = r\"(\\d+)\\.yorum:\\s*([-\\d,]+)\"\n",
        "matches = re.findall(pattern, raw_data)\n",
        "\n",
        "parsed_data = []\n",
        "valid_indices = []\n",
        "\n",
        "for match in matches:\n",
        "    try:\n",
        "\n",
        "        idx = int(match[0])\n",
        "        values = [int(x) for x in match[1].split(',')]\n",
        "\n",
        "\n",
        "        if len(values) == 7:\n",
        "            parsed_data.append(values)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            print(f\"âš ï¸ UyarÄ±: {idx}. yorumda eksik/fazla veri var. AtlanÄ±yor.\")\n",
        "\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "print(f\"âœ… Toplam {len(parsed_data)} adet yorum baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±.\")\n",
        "\n",
        "\n",
        "\n",
        "if len(parsed_data) > 0:\n",
        "    aspect_cols = [\n",
        "        \"Fiyat/Performans\", \"Teslimat ve Paketleme\", \"TasarÄ±m ve Malzeme\",\n",
        "        \"Beden ve Uyum\", \"Ä°ÅŸlevsellik ve KullanÄ±m\", \"Kalite ve SaÄŸlamlÄ±k\",\n",
        "        \"SatÄ±cÄ± ve Ä°ade SÃ¼reci\"\n",
        "    ]\n",
        "    df_sentiments = pd.DataFrame(parsed_data, columns=aspect_cols)\n",
        "\n",
        "\n",
        "    df_reviews = df_ham.iloc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "    df_final = pd.concat([df_reviews[['review_text']], df_sentiments], axis=1)\n",
        "\n",
        "    filename = 'egitim_seti_2000_final.csv'\n",
        "    df_final.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(f\"ðŸ’¾ Dosya kaydedildi: {filename}\")\n",
        "    print(\"\\nðŸ” Ä°lk 3 SatÄ±r:\")\n",
        "    print(df_final.head(3))\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(filename)\n",
        "    except ImportError:\n",
        "        pass\n",
        "else:\n",
        "    print(\"âŒ HATA: HiÃ§bir veri ayrÄ±ÅŸtÄ±rÄ±lamadÄ±. raw_data kÄ±smÄ±nÄ± kontrol et.\")"
      ],
      "metadata": {
        "id": "N2hPnOg7DmpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xaxRXBWuHzAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training with early stopping**"
      ],
      "metadata": {
        "id": "dyou7L_RUtkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.44.2 accelerate datasets evaluate\n",
        "!pip install -q scikit-learn --upgrade"
      ],
      "metadata": {
        "id": "Fozy6UyhHy7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Cihaz:\", device)\n",
        "\n",
        "\n",
        "FILE_PATH = \"/content/egitim_seti_2000_final.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FILE_PATH)\n",
        "    print(f\"âœ… Veri yÃ¼klendi. Toplam satÄ±r: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"HATA: Dosya bulunamadÄ±. LÃ¼tfen dosya yolunu kontrol edin.\")\n",
        "    exit()\n",
        "\n",
        "TEXT_COL = \"review_text\"\n",
        "ASPECTS = [\n",
        "    \"Fiyat/Performans\",\n",
        "    \"Teslimat ve Paketleme\",\n",
        "    \"TasarÄ±m ve Malzeme\",\n",
        "    \"Beden ve Uyum\",\n",
        "    \"Ä°ÅŸlevsellik ve KullanÄ±m\",\n",
        "    \"Kalite ve SaÄŸlamlÄ±k\",\n",
        "    \"SatÄ±cÄ± ve Ä°ade SÃ¼reci\"\n",
        "]\n",
        "\n",
        "df.dropna(subset=[TEXT_COL], inplace=True)\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
        "val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=SEED)\n",
        "\n",
        "print(f\"EÄŸitim: {len(train_df)} | DoÄŸrulama: {len(val_df)} | Test: {len(test_df)}\")\n",
        "\n",
        "\n",
        "map_to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "idx_to_sent = {0: \"Negatif ðŸ˜¡\", 1: \"NÃ¶tr/Yok ðŸ˜\", 2: \"Pozitif ðŸ˜ƒ\"}\n",
        "\n",
        "def encode_labels(row):\n",
        "    return [map_to_idx[int(row[a])] for a in ASPECTS]\n",
        "\n",
        "train_labels = np.stack(train_df.apply(encode_labels, axis=1))\n",
        "val_labels   = np.stack(val_df.apply(encode_labels, axis=1))\n",
        "test_labels  = np.stack(test_df.apply(encode_labels, axis=1))\n",
        "\n",
        "\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, df, labels):\n",
        "        self.texts = df[TEXT_COL].astype(str).tolist()\n",
        "        self.labels = labels.astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"text\": self.texts[idx],\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = ABSADataset(train_df, train_labels)\n",
        "val_dataset   = ABSADataset(val_df, val_labels)\n",
        "test_dataset  = ABSADataset(test_df, test_labels)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [b[\"text\"] for b in batch]\n",
        "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
        "\n",
        "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    enc[\"labels\"] = labels\n",
        "    return enc\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(out.last_hidden_state[:, 0, :])\n",
        "\n",
        "        logits_list = [head(pooled) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            ce = nn.CrossEntropyLoss()\n",
        "            losses = [ce(logits[:, i, :], labels[:, i]) for i in range(logits.shape[1])]\n",
        "            loss = torch.mean(torch.stack(losses))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=len(ASPECTS)).to(device)\n",
        "\n",
        "\n",
        "EPOCHS = 100\n",
        "LR = 2e-5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            y_true.append(batch[\"labels\"].cpu().numpy())\n",
        "            y_pred.append(preds)\n",
        "\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    scores = {}\n",
        "    for i, aspect_name in enumerate(ASPECTS):\n",
        "        f1 = f1_score(y_true[:, i], y_pred[:, i], average=\"macro\")\n",
        "        scores[aspect_name] = f1\n",
        "\n",
        "    scores[\"GENEL_F1\"] = np.mean(list(scores.values()))\n",
        "    return scores\n",
        "\n",
        "\n",
        "best_f1 = 0\n",
        "patience = 10\n",
        "no_improve = 0\n",
        "\n",
        "print(\"\\nðŸš€ EÄŸitim BaÅŸlÄ±yor...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    val_scores = evaluate(model, val_loader)\n",
        "    curr_f1 = val_scores[\"GENEL_F1\"]\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {total_loss/len(train_loader):.4f} | Val F1: {curr_f1:.4f}\")\n",
        "\n",
        "    if curr_f1 > best_f1:\n",
        "        best_f1 = curr_f1\n",
        "        torch.save(model.state_dict(), \"absa_model_7_aspects.pt\")\n",
        "        print(f\"âœ… En iyi model kaydedildi! ({best_f1:.4f})\")\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"â¹ Early stopping: Model daha fazla geliÅŸmiyor.\")\n",
        "            break\n",
        "\n",
        "\n",
        "print(\"\\nðŸ“Š Test Seti DeÄŸerlendirmesi...\")\n",
        "model.load_state_dict(torch.load(\"absa_model_7_aspects.pt\", map_location=device))\n",
        "test_results = evaluate(model, test_loader)\n",
        "\n",
        "for aspect, score in test_results.items():\n",
        "    print(f\"{aspect}: {score:.4f}\")\n",
        "\n",
        "def predict_comment(text):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "        preds = torch.argmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    print(f\"\\nðŸ“ Yorum: {text}\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, aspect in enumerate(ASPECTS):\n",
        "        sentiment = idx_to_sent[preds[i]]\n",
        "        if preds[i] != 1:\n",
        "            print(f\"ðŸ”¹ {aspect}: {sentiment}\")\n",
        "\n",
        "predict_comment(\"Kargo Ã§ok hÄ±zlÄ±ydÄ± ama Ã¼rÃ¼nÃ¼n kumaÅŸÄ± berbat Ã§Ä±ktÄ±.\")\n",
        "predict_comment(\"FiyatÄ±na gÃ¶re harika bir performans, satÄ±cÄ± da Ã§ok ilgiliydi.\")\n",
        "predict_comment(\"ÃœrÃ¼n kÄ±rÄ±k geldi iade ettim.\")"
      ],
      "metadata": {
        "id": "I-pjmRhjHy4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of Results**"
      ],
      "metadata": {
        "id": "57fUedvcU4Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "results = {\n",
        "    \"Quality & Durability\": 0.8066,\n",
        "    \"Delivery & Packaging\": 0.8053,\n",
        "    \"Functionality & Usability\": 0.7976,\n",
        "    \"Price/Performance\": 0.7964,\n",
        "    \"Seller & Return Process\": 0.7379,\n",
        "    \"Size & Fit\": 0.7227,\n",
        "    \"Design & Material\": 0.6154\n",
        "}\n",
        "\n",
        "sorted_results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "colors = ['#2ecc71' if v > 0.75 else '#f1c40f' if v > 0.70 else '#e74c3c' for v in sorted_results.values()]\n",
        "\n",
        "bars = plt.barh(list(sorted_results.keys()), list(sorted_results.values()), color=colors)\n",
        "\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "             f'{bar.get_width():.4f}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.xlabel(\"F1 Score\")\n",
        "plt.title(\"Model Performance (Aspect-Based)\", fontsize=14)\n",
        "plt.xlim(0, 1.0)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KMLLCfktMdD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYo4W4cJs8IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jFVJsVAWs8DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real-time Prediction**"
      ],
      "metadata": {
        "id": "7EBPMSGGVHGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "MAX_LEN = 128\n",
        "\n",
        "ASPECTS = [\n",
        "    \"Fiyat/Performans\",\n",
        "    \"Teslimat ve Paketleme\",\n",
        "    \"TasarÄ±m ve Malzeme\",\n",
        "    \"Beden ve Uyum\",\n",
        "    \"Ä°ÅŸlevsellik ve KullanÄ±m\",\n",
        "    \"Kalite ve SaÄŸlamlÄ±k\",\n",
        "    \"SatÄ±cÄ± ve Ä°ade SÃ¼reci\"\n",
        "]\n",
        "\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(out.last_hidden_state[:, 0, :])\n",
        "        logits_list = [head(pooled) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "print(\"â³ Model yÃ¼kleniyor, lÃ¼tfen bekleyin...\")\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=len(ASPECTS)).to(device)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"absa_model_7_aspects.pt\", map_location=device))\n",
        "    model.eval()\n",
        "    print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ HATA: 'absa_model_7_aspects.pt' dosyasÄ± bulunamadÄ±. EÄŸitimi Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±ndan emin ol.\")\n",
        "    exit()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "idx_to_sent = {0: \"ðŸ”´ Negatif\", 1: \"âšª NÃ¶tr/Yok\", 2: \"ðŸŸ¢ Pozitif\"}\n",
        "\n",
        "\n",
        "def predict_new_comment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "        preds = torch.argmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    print(f\"\\nðŸ“ Yorum: {text}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    aspect_found = False\n",
        "    for i, aspect in enumerate(ASPECTS):\n",
        "        prediction_idx = preds[i]\n",
        "        sentiment = idx_to_sent[prediction_idx]\n",
        "\n",
        "        if prediction_idx != 1:\n",
        "            print(f\"   ðŸ”¹ {aspect:<25}: {sentiment}\")\n",
        "            aspect_found = True\n",
        "\n",
        "    if not aspect_found:\n",
        "        print(\"   (Model bu yorumda belirgin bir aspect bulamadÄ± veya hepsi NÃ¶tr)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "#\n",
        "while True:\n",
        "    user_input = input(\"\\nðŸ’¬ comment: \")\n",
        "    if user_input.lower() == 'q':\n",
        "        print(\"Ã‡Ä±kÄ±ÅŸ yapÄ±ldÄ±.\")\n",
        "        break\n",
        "\n",
        "    predict_new_comment(user_input)"
      ],
      "metadata": {
        "id": "xhHELqm8LTZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINAL TRAÄ°NÄ°NG WÄ°TH 14 CLUSTERS**"
      ],
      "metadata": {
        "id": "TWnT6f4liKEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install transformers==4.44.2 accelerate datasets evaluate scikit-learn torch --upgrade\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "DyYs39n3C3bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¥ Cihaz: {device}\")\n",
        "\n",
        "\n",
        "LABELS_PATH = \"/content/labeled_reviews_parallel3.csv\"\n",
        "REVIEWS_PATH = \"/content/reviews.csv\"\n",
        "\n",
        "try:\n",
        "    df_labels = pd.read_csv(LABELS_PATH)\n",
        "    df_reviews = pd.read_csv(REVIEWS_PATH)\n",
        "    df_merged = pd.merge(df_labels, df_reviews[['id', 'review_text']], on='id', how='inner')\n",
        "    print(f\"âœ… BirleÅŸtirme tamamlandÄ±. EÅŸleÅŸen toplam veri: {len(df_merged)}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ HATA: Dosya bulunamadÄ±! Yolunu kontrol et.\\n{e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "df_merged.dropna(subset=['review_text', 'scores'], inplace=True)\n",
        "\n",
        "def parse_scores(score_str):\n",
        "    try:\n",
        "        if isinstance(score_str, list):\n",
        "            return score_str\n",
        "        return ast.literal_eval(score_str)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df_merged['parsed_scores'] = df_merged['scores'].apply(parse_scores)\n",
        "df_merged.dropna(subset=['parsed_scores'], inplace=True)\n",
        "\n",
        "\n",
        "ASPECTS_DICT = {\n",
        "    0: \"Seller Errors\", 1: \"General Satisfaction\", 2: \"Size & Fit\", 3: \"Damaged/Broken Item\",\n",
        "    4: \"Installation & Usage\", 5: \"Return Process\", 6: \"Seller Courtesy\", 7: \"Product Features\",\n",
        "    8: \"General Quality\", 9: \"Color & Appearance\", 10: \"Durability Issues\", 11: \"Missing/Defective Item\",\n",
        "    12: \"Price/Performance\", 13: \"Shipping Disasters\"\n",
        "}\n",
        "ASPECT_NAMES = [ASPECTS_DICT[i] for i in range(len(ASPECTS_DICT))]\n",
        "NUM_ASPECTS = len(ASPECT_NAMES)\n",
        "\n",
        "\n",
        "map_to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "idx_to_sent = {0: \"Negatif ðŸ˜¡\", 1: \"NÃ¶tr/Yok ðŸ˜\", 2: \"Pozitif ðŸ˜ƒ\"}\n",
        "\n",
        "def encode_labels_list(scores_list):\n",
        "    encoded = []\n",
        "    for s in scores_list:\n",
        "        val = int(s)\n",
        "        if val not in map_to_idx:\n",
        "            encoded.append(map_to_idx[0])\n",
        "        else:\n",
        "            encoded.append(map_to_idx[val])\n",
        "    return encoded\n",
        "\n",
        "all_labels = np.stack(df_merged['parsed_scores'].apply(encode_labels_list))\n",
        "\n",
        "train_texts, temp_texts, train_y, temp_y = train_test_split(\n",
        "    df_merged['review_text'].tolist(), all_labels, test_size=0.2, random_state=SEED\n",
        ")\n",
        "val_texts, test_texts, val_y, test_y = train_test_split(\n",
        "    temp_texts, temp_y, test_size=0.5, random_state=SEED\n",
        ")\n",
        "\n",
        "\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"text\": str(self.texts[idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [b[\"text\"] for b in batch]\n",
        "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
        "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    enc[\"labels\"] = labels\n",
        "    return enc\n",
        "\n",
        "train_dataset = ABSADataset(train_texts, train_y)\n",
        "val_dataset   = ABSADataset(val_texts, val_y)\n",
        "test_dataset  = ABSADataset(test_texts, test_y)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        hidden = self.bert.config.hidden_size\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(out.last_hidden_state[:, 0, :])\n",
        "        logits_list = [head(pooled) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "\n",
        "            class_weights = torch.tensor([4.0, 1.0, 4.0]).to(logits.device)\n",
        "\n",
        "            ce = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "            losses = [ce(logits[:, i, :], labels[:, i]) for i in range(logits.shape[1])]\n",
        "            loss = torch.mean(torch.stack(losses))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=NUM_ASPECTS).to(device)\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            y_true.append(batch[\"labels\"].cpu().numpy())\n",
        "            y_pred.append(preds)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "    scores = {}\n",
        "    for i, aspect_name in enumerate(ASPECT_NAMES):\n",
        "        f1 = f1_score(y_true[:, i], y_pred[:, i], average=\"macro\")\n",
        "        scores[aspect_name] = f1\n",
        "    scores[\"GENEL_F1\"] = np.mean(list(scores.values()))\n",
        "    return scores\n",
        "\n",
        "train_losses = []\n",
        "val_f1_scores = []\n",
        "\n",
        "best_f1 = 0\n",
        "print(\"\\nðŸš€ EÄŸitim BaÅŸlÄ±yor (14 Aspect - Weighted Loss)...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    val_scores = evaluate(model, val_loader)\n",
        "    curr_f1 = val_scores['GENEL_F1']\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_f1_scores.append(curr_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Val F1: {curr_f1:.4f}\")\n",
        "\n",
        "    if curr_f1 > best_f1:\n",
        "        best_f1 = curr_f1\n",
        "        torch.save(model.state_dict(), \"absa_model_14_aspects.pt\")\n",
        "        print(f\"âœ… Model Kaydedildi! En iyi F1: {best_f1:.4f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss', color='red', linewidth=2, marker='o')\n",
        "plt.title('EÄŸitim KaybÄ± (Weighted Loss)', fontsize=14)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_f1_scores, label='Validation F1', color='blue', linewidth=2, marker='s')\n",
        "plt.title('DoÄŸrulama BaÅŸarÄ±sÄ± (Validation F1)', fontsize=14)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qukmVg6Cr0TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Delete data where all sentiment is 0.**"
      ],
      "metadata": {
        "id": "NcM4sqe94m7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def check_all_zeros(scores):\n",
        "    return all(x == 0 for x in scores)\n",
        "\n",
        "def check_single_aspect(scores):\n",
        "    non_zero_count = sum(1 for x in scores if x != 0)\n",
        "    return non_zero_count == 1\n",
        "\n",
        "empty_rows = df_merged[df_merged['parsed_scores'].apply(check_all_zeros)]\n",
        "single_rows = df_merged[df_merged['parsed_scores'].apply(check_single_aspect)]\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"ðŸ“‰ ANALÄ°Z SONUÃ‡LARI\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"ðŸ”´ TÃ¼mÃ¼ 0 olan (HiÃ§bir aspect yok) satÄ±r sayÄ±sÄ± : {len(empty_rows)}\")\n",
        "print(f\"ðŸŸ¢ Sadece 1 Aspect iÃ§eren (Tekil) satÄ±r sayÄ±sÄ±    : {len(single_rows)}\")\n",
        "print(f\"ðŸ”µ Toplam Veri SayÄ±sÄ±                             : {len(df_merged)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if len(empty_rows) > 0:\n",
        "    print(\"\\nÃ–rnek: TÃ¼mÃ¼ 0 Olan SatÄ±r:\")\n",
        "    print(f\"ID: {empty_rows.iloc[0]['id']} | Skorlar: {empty_rows.iloc[0]['parsed_scores']}\")\n",
        "\n",
        "if len(single_rows) > 0:\n",
        "    print(\"\\nÃ–rnek: Sadece 1 Tanesi Dolu Olan SatÄ±r:\")\n",
        "    print(f\"ID: {single_rows.iloc[0]['id']} | Skorlar: {single_rows.iloc[0]['parsed_scores']}\")"
      ],
      "metadata": {
        "id": "xnuDUV4xLiIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¥ Cihaz: {device}\")\n",
        "\n",
        "LABELS_PATH = \"/content/labeled_reviews_parallel3.csv\"\n",
        "REVIEWS_PATH = \"/content/reviews.csv\"\n",
        "\n",
        "try:\n",
        "    df_labels = pd.read_csv(LABELS_PATH)\n",
        "    df_reviews = pd.read_csv(REVIEWS_PATH)\n",
        "    df_merged = pd.merge(df_labels, df_reviews[['id', 'review_text']], on='id', how='inner')\n",
        "    print(f\"âœ… BirleÅŸtirme tamamlandÄ±. Ham veri sayÄ±sÄ±: {len(df_merged)}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ HATA: Dosya bulunamadÄ±! Yolunu kontrol et.\\n{e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "df_merged.dropna(subset=['review_text', 'scores'], inplace=True)\n",
        "\n",
        "def parse_scores(score_str):\n",
        "    try:\n",
        "        if isinstance(score_str, list): return score_str\n",
        "        return ast.literal_eval(score_str)\n",
        "    except: return None\n",
        "\n",
        "df_merged['parsed_scores'] = df_merged['scores'].apply(parse_scores)\n",
        "df_merged.dropna(subset=['parsed_scores'], inplace=True)\n",
        "\n",
        "initial_len = len(df_merged)\n",
        "df_merged = df_merged[~df_merged['parsed_scores'].apply(lambda x: all(s == 0 for s in x))]\n",
        "final_len = len(df_merged)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"ðŸ—‘ï¸ TEMÄ°ZLÄ°K RAPORU:\")\n",
        "print(f\"   BaÅŸlangÄ±Ã§ Verisi : {initial_len}\")\n",
        "print(f\"   Silinen (TÃ¼mÃ¼ 0) : {initial_len - final_len}\")\n",
        "print(f\"   Kalan Net Veri   : {final_len}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "ASPECTS_DICT = {\n",
        "    0: \"Seller Errors\", 1: \"General Satisfaction\", 2: \"Size & Fit\", 3: \"Damaged/Broken Item\",\n",
        "    4: \"Installation & Usage\", 5: \"Return Process\", 6: \"Seller Courtesy\", 7: \"Product Features\",\n",
        "    8: \"General Quality\", 9: \"Color & Appearance\", 10: \"Durability Issues\", 11: \"Missing/Defective Item\",\n",
        "    12: \"Price/Performance\", 13: \"Shipping Disasters\"\n",
        "}\n",
        "ASPECT_NAMES = [ASPECTS_DICT[i] for i in range(len(ASPECTS_DICT))]\n",
        "NUM_ASPECTS = len(ASPECT_NAMES)\n",
        "\n",
        "\n",
        "map_to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "idx_to_sent = {0: \"Negatif ðŸ˜¡\", 1: \"NÃ¶tr/Yok ðŸ˜\", 2: \"Pozitif ðŸ˜ƒ\"}\n",
        "\n",
        "def encode_labels_list(scores_list):\n",
        "    encoded = []\n",
        "    for s in scores_list:\n",
        "        val = int(s)\n",
        "        if val not in map_to_idx: encoded.append(map_to_idx[0])\n",
        "        else: encoded.append(map_to_idx[val])\n",
        "    return encoded\n",
        "\n",
        "all_labels = np.stack(df_merged['parsed_scores'].apply(encode_labels_list))\n",
        "\n",
        "flat_labels = all_labels.flatten()\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.array([0, 1, 2]),\n",
        "    y=flat_labels\n",
        ")\n",
        "\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "print(\"\\nðŸ“Š YENÄ° AÄžIRLIKLAR (BoÅŸ veriler silindikten sonra):\")\n",
        "print(f\"   Negatif (0): {class_weights[0]:.4f}\")\n",
        "print(f\"   NÃ¶tr    (1): {class_weights[1]:.4f}\")\n",
        "print(f\"   Pozitif (2): {class_weights[2]:.4f}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "train_texts, temp_texts, train_y, temp_y = train_test_split(\n",
        "    df_merged['review_text'].tolist(), all_labels, test_size=0.2, random_state=SEED\n",
        ")\n",
        "val_texts, test_texts, val_y, test_y = train_test_split(\n",
        "    temp_texts, temp_y, test_size=0.5, random_state=SEED\n",
        ")\n",
        "\n",
        "\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"text\": str(self.texts[idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [b[\"text\"] for b in batch]\n",
        "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
        "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    enc[\"labels\"] = labels\n",
        "    return enc\n",
        "\n",
        "train_dataset = ABSADataset(train_texts, train_y)\n",
        "val_dataset   = ABSADataset(val_texts, val_y)\n",
        "test_dataset  = ABSADataset(test_texts, test_y)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, num_classes=3, loss_weights=None):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        for param in self.bert.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for i in range(8):\n",
        "            for param in self.bert.encoder.layer[i].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(out.last_hidden_state[:, 0, :])\n",
        "        logits_list = [head(pooled) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            ce = nn.CrossEntropyLoss(weight=self.loss_weights)\n",
        "            losses = [ce(logits[:, i, :], labels[:, i]) for i in range(logits.shape[1])]\n",
        "            loss = torch.mean(torch.stack(losses))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=NUM_ASPECTS, loss_weights=weights_tensor).to(device)\n",
        "\n",
        "\n",
        "EPOCHS = 15\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            y_true.append(batch[\"labels\"].cpu().numpy())\n",
        "            y_pred.append(preds)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    scores = {}\n",
        "    for i, aspect_name in enumerate(ASPECT_NAMES):\n",
        "        f1 = f1_score(y_true[:, i], y_pred[:, i], average=\"macro\")\n",
        "        scores[aspect_name] = f1\n",
        "    scores[\"GENEL_F1\"] = np.mean(list(scores.values()))\n",
        "    acc = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
        "    scores[\"ACCURACY\"] = acc\n",
        "    return scores\n",
        "\n",
        "train_losses = []\n",
        "val_f1_scores = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_f1 = 0\n",
        "print(\"\\nðŸš€ EÄŸitim BaÅŸlÄ±yor (TemizlenmiÅŸ Veri + Auto Weights)...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    val_scores = evaluate(model, val_loader)\n",
        "\n",
        "    curr_f1 = val_scores['GENEL_F1']\n",
        "    curr_acc = val_scores['ACCURACY']\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_f1_scores.append(curr_f1)\n",
        "    val_accuracies.append(curr_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Val F1: {curr_f1:.4f} | Val Acc: {curr_acc:.4f}\")\n",
        "\n",
        "    if curr_f1 > best_f1:\n",
        "        best_f1 = curr_f1\n",
        "        torch.save(model.state_dict(), \"absa_model_cleaned.pt\")\n",
        "        print(f\"âœ… Model Kaydedildi! En iyi F1: {best_f1:.4f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses, label='Loss', color='red', marker='o'); plt.title('Loss'); plt.legend()\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(val_f1_scores, label='F1', color='blue', marker='s'); plt.title('F1 Score'); plt.legend()\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(val_accuracies, label='Accuracy', color='green', marker='^'); plt.title('Accuracy'); plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jSce_7Y1MCW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RcJvUeWP_pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synthetic data enhancement**"
      ],
      "metadata": {
        "id": "ZQPZqeui49Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_csv = \"\"\"comment_id,comment_text,sentiments\n",
        "1,\"ÃœrÃ¼n genel olarak Ã§ok hoÅŸuma gitti, kullanÄ±mÄ± kolaydÄ± ve kaliteli hissettiriyor ama kutu ezilmiÅŸti ve teslimat geÃ§ geldi, fiyatÄ±na gÃ¶re yine de iyi diyebilirim.\",\"[0,1,0,0,1,0,0,0,1,0,0,0,1,-1]\"\n",
        "2,\"ÃœrÃ¼n yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci Ã§ok zahmetliydi ama satÄ±cÄ± nazikti ve Ã¼rÃ¼nÃ¼n Ã¶zellikleri beklendiÄŸi gibiydi, fiyatÄ± da uygundu.\",\"[-1,0,0,0,0,-1,1,1,0,-1,0,0,1,0]\"\n",
        "3,\"Boyutu bana kÃ¼Ã§Ã¼k geldi, ilk kullanÄ±mda parÃ§asÄ± kÄ±rÄ±ldÄ± ve dayanÄ±klÄ±lÄ±ÄŸÄ± Ã§ok kÃ¶tÃ¼ ama satÄ±cÄ± hÄ±zlÄ± kargolamÄ±ÅŸ ve kurulumu kolaydÄ±.\",\"[0,0,-1,1,1,0,1,0,0,0,-1,0,0,0]\"\n",
        "4,\"ÃœrÃ¼n hasarlÄ± geldi, iÃ§inden bir parÃ§a eksikti ve kutu daÄŸÄ±lmÄ±ÅŸtÄ± ama iade sÃ¼reci hÄ±zlÄ±ydÄ± ve param sorunsuz geri yattÄ±.\",\"[0,0,0,-1,0,1,0,0,0,0,0,-1,0,-1]\"\n",
        "5,\"Genel olarak memnun kaldÄ±m, rengi Ã§ok gÃ¼zel, kaliteli duruyor ve fiyatÄ±na gÃ¶re performansÄ± yÃ¼ksek fakat uzun vadede dayanÄ±klÄ± olacaÄŸÄ±nÄ± sanmÄ±yorum.\",\"[0,1,0,0,0,0,0,0,1,1,-1,0,1,0]\"\n",
        "6,\"ÃœrÃ¼n Ã¶zellikleri anlatÄ±ldÄ±ÄŸÄ± gibiydi, kullanÄ±mÄ± pratikti ve satÄ±cÄ± kÃ¼Ã§Ã¼k bir hediye koymuÅŸ ama teslimat sÄ±rasÄ±nda kutu ciddi ÅŸekilde zarar gÃ¶rmÃ¼ÅŸ.\",\"[0,0,0,0,1,0,1,1,0,0,0,0,0,-1]\"\n",
        "7,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo firmasÄ± rezaletti ve satÄ±cÄ± hatayÄ± kabul etmedi, iade sÃ¼reci de Ã§ok uzadÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "8,\"ÃœrÃ¼nÃ¼ Ã§ok beÄŸendim, hem kaliteli hem de fiyatÄ±na gÃ¶re Ã§ok baÅŸarÄ±lÄ±, rengi fotoÄŸraftakiyle aynÄ± ve kullanÄ±mÄ± da oldukÃ§a rahat.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,0]\"\n",
        "9,\"Ä°lk baÅŸta her ÅŸey iyiydi ama kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, ayrÄ±ca kargo geÃ§ geldi fakat Ã¼rÃ¼nÃ¼n Ã¶zellikleri hoÅŸtu.\",\"[0,0,0,0,0,0,0,1,0,0,-1,0,0,-1]\"\n",
        "10,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n hÄ±zlÄ± geldi, kurulumu kolaydÄ± ve genel kalitesi yÃ¼ksek ama fiyatÄ± biraz pahalÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,-1,0]\"\n",
        "11,\"ÃœrÃ¼n beklediÄŸimden kaliteli Ã§Ä±ktÄ±, kullanÄ±mÄ± rahat ve rengi Ã§ok hoÅŸ ama kargo gecikti ve kutu ezilmiÅŸti, fiyatÄ±na gÃ¶re yine iyi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,-1]\"\n",
        "12,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderilmiÅŸti, iade sÃ¼reci yorucuydu ama satÄ±cÄ± Ã¶zÃ¼r dileyip yardÄ±mcÄ± oldu, Ã¼rÃ¼n Ã¶zellik olarak fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,1,0,0,0,0,0,0]\"\n",
        "13,\"ÃœrÃ¼n Ã§ok Ã§abuk kÄ±rÄ±ldÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve parÃ§a eksikti ama kurulumu kolaydÄ± ve fiyatÄ± uygundu.\",\"[0,0,0,1,1,0,0,0,0,0,-1,-1,1,0]\"\n",
        "14,\"Genel olarak memnun kaldÄ±m, Ã¼rÃ¼n kaliteli ve kullanÄ±mÄ± pratik, satÄ±cÄ± da nazikti fakat rengi gÃ¶rseldekinden farklÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,0,1,-1,0,0,0,0]\"\n",
        "15,\"ÃœrÃ¼n hasarlÄ± geldi, kargo rezaletti ve iade sÃ¼reci uzadÄ± ama satÄ±cÄ± sonunda yardÄ±mcÄ± oldu.\",\"[0,0,0,-1,0,-1,1,0,0,0,0,0,0,-1]\"\n",
        "16,\"FiyatÄ±na gÃ¶re performansÄ± Ã§ok iyi, Ã¶zellikleri baÅŸarÄ±lÄ± ve genel kalite hissi yÃ¼ksek ama uzun vadede dayanÄ±klÄ± deÄŸil.\",\"[0,1,0,0,0,0,0,1,1,0,-1,0,1,0]\"\n",
        "17,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu, satÄ±cÄ± ilgisizdi ve iade almak tam bir iÅŸkenceydi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "18,\"ÃœrÃ¼nÃ¼ Ã§ok sevdim, rengi ve gÃ¶rÃ¼nÃ¼mÃ¼ harika, kullanÄ±mÄ± kolay ve fiyatÄ±na gÃ¶re fazlasÄ±yla kaliteli.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,0]\"\n",
        "19,\"Ä°lk kullanÄ±mda sorun yoktu ama kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve fiyatÄ±na deÄŸmez.\",\"[0,0,0,0,0,0,0,0,0,0,-1,0,-1,0]\"\n",
        "20,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n hÄ±zlÄ± geldi, kurulumu kolay ve genel kalite iyi ama kutu hasarlÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,-1]\"\n",
        "21,\"ÃœrÃ¼n yanlÄ±ÅŸ beden gÃ¶nderildi, iade sÃ¼reci uzun sÃ¼rdÃ¼ ama Ã¼rÃ¼nÃ¼n kalitesi ve Ã¶zellikleri iyiydi.\",\"[-1,0,-1,0,0,-1,0,1,1,0,0,0,0,0]\"\n",
        "22,\"Kargo geÃ§ geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸ ama satÄ±cÄ± hÄ±zlÄ±ca telafi etti.\",\"[0,0,0,-1,0,0,1,1,0,0,0,0,0,-1]\"\n",
        "23,\"ÃœrÃ¼n beklediÄŸim gibi Ã§Ä±ktÄ±, kullanÄ±mÄ± rahat ve kaliteli ama fiyatÄ± biraz yÃ¼ksek.\",\"[0,1,0,0,1,0,0,0,1,0,0,0,-1,0]\"\n",
        "24,\"Eksik parÃ§a vardÄ±, Ã¼rÃ¼n tam Ã§alÄ±ÅŸmadÄ± ve iade sÃ¼reci sinir bozucuydu.\",\"[0,0,0,0,0,-1,0,0,0,0,0,-1,0,0]\"\n",
        "25,\"Genel memnuniyetim yÃ¼ksek, Ã¼rÃ¼n kaliteli, gÃ¶rÃ¼nÃ¼mÃ¼ gÃ¼zel ve kullanÄ±mÄ± Ã§ok kolay.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,0,0]\"\n",
        "26,\"ÃœrÃ¼n Ã§ok Ã§abuk yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ama Ã¶zellikleri ve kullanÄ±mÄ± fena deÄŸildi.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,0,0]\"\n",
        "27,\"SatÄ±cÄ± yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderdi, kargo geÃ§ geldi ama iade sÃ¼reci sorunsuzdu.\",\"[-1,0,0,0,0,1,0,0,0,0,0,0,0,-1]\"\n",
        "28,\"ÃœrÃ¼n fiyatÄ±na gÃ¶re Ã§ok iyi, kaliteli hissettiriyor ve rengi Ã§ok ÅŸÄ±k.\",\"[0,1,0,0,0,0,0,0,1,1,0,0,1,0]\"\n",
        "29,\"ÃœrÃ¼n hasarlÄ±ydÄ±, kutu ezilmiÅŸti ve dayanÄ±klÄ±lÄ±ÄŸÄ± da zayÄ±f Ã§Ä±ktÄ±.\",\"[0,0,0,-1,0,0,0,0,0,0,-1,0,0,-1]\"\n",
        "30,\"SatÄ±cÄ± hÄ±zlÄ± kargoladÄ±, Ã¼rÃ¼n sorunsuz geldi, kurulumu kolay ve genel kalite baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "31,\"ÃœrÃ¼n genel olarak iyi ama yanlÄ±ÅŸ renk gÃ¶nderilmiÅŸti, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ± yine de kullanÄ±mÄ± kolay ve kalite fena deÄŸil.\",\"[-1,0,0,0,1,0,0,0,1,-1,0,0,0,-1]\"\n",
        "32,\"ÃœrÃ¼n kaliteli duruyor, Ã¶zellikleri gÃ¼zel ve fiyat performans aÃ§Ä±sÄ±ndan baÅŸarÄ±lÄ± ama ilk haftada parÃ§asÄ± kÄ±rÄ±ldÄ±.\",\"[0,1,0,1,0,0,0,1,1,0,-1,0,1,0]\"\n",
        "33,\"Kargo Ã§ok kÃ¶tÃ¼ydÃ¼, kutu parÃ§alanmÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸ, ayrÄ±ca dayanÄ±klÄ±lÄ±ÄŸÄ± da zayÄ±f Ã§Ä±ktÄ±.\",\"[0,0,0,-1,0,0,0,1,0,0,-1,0,0,-1]\"\n",
        "34,\"ÃœrÃ¼n eksik parÃ§a ile geldi, iade sÃ¼reci Ã§ok uzundu ama satÄ±cÄ± ilgiliydi ve sonunda sorun Ã§Ã¶zÃ¼ldÃ¼.\",\"[0,0,0,0,0,-1,1,0,0,0,0,-1,0,0]\"\n",
        "35,\"Genel olarak memnunum, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel, kullanÄ±mÄ± rahat ve fiyatÄ±na gÃ¶re oldukÃ§a iyi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,0]\"\n",
        "36,\"ÃœrÃ¼n yanlÄ±ÅŸ beden geldi, iade etmek zorunda kaldÄ±m, kargo da geÃ§ teslim edildi ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,0,0,-1,1,0,0,0,0,0,0,-1]\"\n",
        "37,\"BaÅŸta her ÅŸey iyiydi fakat kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve fiyatÄ±na kesinlikle deÄŸmez.\",\"[0,0,0,0,0,0,0,0,0,0,-1,0,-1,0]\"\n",
        "38,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n hÄ±zlÄ± geldi, kurulumu kolay ve genel kalite beklentimin Ã¼zerindeydi.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "39,\"ÃœrÃ¼n hasarlÄ±ydÄ±, bir parÃ§asÄ± eksikti ve kutu daÄŸÄ±lmÄ±ÅŸtÄ±, iade sÃ¼reci de sinir bozucuydu.\",\"[0,0,0,-1,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "40,\"ÃœrÃ¼n Ã¶zellik olarak baÅŸarÄ±lÄ±, kullanÄ±mÄ± pratik ve kaliteli ama fiyatÄ± biraz pahalÄ± ve rengi farklÄ± geldi.\",\"[0,1,0,0,1,0,0,1,1,-1,0,0,-1,0]\"\n",
        "41,\"ÃœrÃ¼n genel olarak tatmin edici, kullanÄ±mÄ± kolay ve kaliteli ama kargo gecikti, kutu ezilmiÅŸti ve fiyatÄ± biraz pahalÄ±.\",\"[0,1,0,0,1,0,0,0,1,0,0,0,-1,-1]\"\n",
        "42,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, iade sÃ¼reci Ã§ok yordu fakat satÄ±cÄ± nazikti, Ã¼rÃ¼n Ã¶zellikleri fena deÄŸildi ve fiyatÄ± uygundu.\",\"[-1,0,0,0,0,-1,1,1,0,0,0,0,1,0]\"\n",
        "43,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§a eksikti ve kutu patlamÄ±ÅŸtÄ± ama kurulumu kolaydÄ± ve satÄ±cÄ± hÄ±zlÄ± dÃ¶nÃ¼ÅŸ yaptÄ±.\",\"[0,0,0,-1,1,0,1,0,0,0,0,-1,0,-1]\"\n",
        "44,\"Genel memnuniyetim yÃ¼ksek, Ã¼rÃ¼n kaliteli, rengi Ã§ok gÃ¼zel ve fiyat performans olarak baÅŸarÄ±lÄ±.\",\"[0,1,0,0,0,0,0,0,1,1,0,0,1,0]\"\n",
        "45,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, iade sÃ¼reci uzundu ve fiyatÄ±na kesinlikle deÄŸmez.\",\"[0,0,0,0,0,-1,0,0,0,0,-1,0,-1,0]\"\n",
        "46,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n hÄ±zlÄ± geldi, kurulumu kolaydÄ± ama kutu hasarlÄ± ve rengi farklÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,0,0,-1,0,0,0,-1]\"\n",
        "47,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu, satÄ±cÄ± hatayÄ± kabul etmedi ve iade almak Ã§ok zordu.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "48,\"ÃœrÃ¼n Ã¶zellikleri beklentimi karÅŸÄ±ladÄ±, kullanÄ±mÄ± rahat, kaliteli ve fiyatÄ±na gÃ¶re Ã§ok iyi.\",\"[0,1,0,0,1,0,0,1,1,0,0,0,1,0]\"\n",
        "49,\"ÃœrÃ¼n yanlÄ±ÅŸ beden geldi, iade ettim, kargo da geÃ§ teslim edildi ama satÄ±cÄ± yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,-1,0,0,-1,1,0,0,0,0,0,0,-1]\"\n",
        "50,\"ÃœrÃ¼n saÄŸlam geldi, kurulumu kolaydÄ±, genel kalite iyi fakat fiyat biraz yÃ¼ksek.\",\"[0,1,0,0,1,0,0,0,1,0,0,0,-1,0]\"\n",
        "51,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu daÄŸÄ±lmÄ±ÅŸtÄ± ve dayanÄ±klÄ±lÄ±ÄŸÄ± da kÃ¶tÃ¼ Ã§Ä±ktÄ±.\",\"[0,0,0,-1,0,0,0,1,0,0,-1,0,0,-1]\"\n",
        "52,\"Genel olarak memnun kaldÄ±m, Ã¼rÃ¼n kaliteli, gÃ¶rÃ¼nÃ¼mÃ¼ gÃ¼zel, kullanÄ±mÄ± rahat.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,0,0]\"\n",
        "53,\"Eksik parÃ§a vardÄ±, Ã¼rÃ¼n tam Ã§alÄ±ÅŸmadÄ±, iade sÃ¼reci sinir bozucuydu ve fiyatÄ±na deÄŸmez.\",\"[0,0,0,0,0,-1,0,0,0,0,0,-1,-1,0]\"\n",
        "54,\"ÃœrÃ¼n hÄ±zlÄ± geldi, satÄ±cÄ± nazikti, kurulumu kolay ve genel kalite baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "55,\"ÃœrÃ¼n kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ama Ã¶zellikleri ve kullanÄ±mÄ± fena deÄŸildi.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,0,0]\"\n",
        "56,\"YanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ± ama Ã¼rÃ¼n kaliteli ve fiyatÄ± uygundu.\",\"[-1,0,0,0,0,-1,0,0,1,-1,0,0,1,0]\"\n",
        "57,\"ÃœrÃ¼n hasarlÄ± geldi, kutu ezilmiÅŸti, kargo geÃ§ kaldÄ± ve satÄ±cÄ± ilgisizdi.\",\"[-1,0,0,-1,0,0,0,0,0,0,0,0,0,-1]\"\n",
        "58,\"ÃœrÃ¼n beklentimin Ã¼zerindeydi, kaliteli, rengi gÃ¼zel ve fiyat performansÄ± Ã§ok iyi.\",\"[0,1,0,0,0,0,0,0,1,1,0,0,1,0]\"\n",
        "59,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo rezaletti, iade sÃ¼reci Ã§ok uzundu.\",\"[0,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "60,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n sorunsuz geldi, kurulumu kolay ve genel memnuniyetim yÃ¼ksek.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "61,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± rahat ama yanlÄ±ÅŸ renk gÃ¶nderilmiÅŸti, kargo geÃ§ geldi ve kutu hasarlÄ±ydÄ±.\",\"[-1,1,0,0,1,0,0,0,1,-1,0,0,0,-1]\"\n",
        "62,\"Genel olarak memnun kaldÄ±m, Ã¼rÃ¼n Ã¶zellikleri baÅŸarÄ±lÄ±, fiyat performansÄ± iyi fakat dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f Ã§Ä±ktÄ±.\",\"[0,1,0,0,0,0,0,1,1,0,-1,0,1,0]\"\n",
        "63,\"ÃœrÃ¼n hasarlÄ± geldi, parÃ§asÄ± eksikti, iade sÃ¼reci uzadÄ± ama satÄ±cÄ± sonunda yardÄ±mcÄ± oldu.\",\"[0,0,0,-1,0,-1,1,0,0,0,0,-1,0,0]\"\n",
        "64,\"ÃœrÃ¼n Ã§ok ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli hissettiriyor ve kullanÄ±mÄ± kolay.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,0,0]\"\n",
        "65,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, iade etmek zorunda kaldÄ±m, kargo da geÃ§ teslim edildi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,0,0,-1]\"\n",
        "66,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±, Ã¶zellikleri gÃ¼zel, genel kalite iyi ama fiyat biraz pahalÄ±.\",\"[0,1,0,0,0,0,0,1,1,0,0,0,-1,0]\"\n",
        "67,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± Ã§ok kÃ¶tÃ¼ ve fiyatÄ±na kesinlikle deÄŸmez.\",\"[0,0,0,0,0,0,0,0,0,0,-1,0,-1,0]\"\n",
        "68,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n hÄ±zlÄ± geldi, kurulumu kolay ve genel kalite baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "69,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ± ve kargo Ã§ok geÃ§ geldi.\",\"[0,0,0,-1,0,0,0,1,0,0,0,0,0,-1]\"\n",
        "70,\"ÃœrÃ¼n fiyatÄ±na gÃ¶re Ã§ok iyi, kaliteli, kullanÄ±mÄ± rahat ve gÃ¶rÃ¼nÃ¼mÃ¼ gÃ¼zel.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,0]\"\n",
        "71,\"Eksik parÃ§a nedeniyle Ã¼rÃ¼n Ã§alÄ±ÅŸmadÄ±, iade sÃ¼reci sinir bozucuydu ve satÄ±cÄ± ilgisizdi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,0]\"\n",
        "72,\"ÃœrÃ¼n Ã¶zellikleri anlatÄ±ldÄ±ÄŸÄ± gibi, kurulumu kolay ve genel kalite beklentimin Ã¼zerinde.\",\"[0,1,0,0,1,0,0,1,1,0,0,0,0,0]\"\n",
        "73,\"YanlÄ±ÅŸ beden gÃ¶nderildi, iade sÃ¼reci uzadÄ± ama Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,-1,0,0,-1,0,0,1,0,0,0,0,0]\"\n",
        "74,\"ÃœrÃ¼n saÄŸlam geldi, kargo hÄ±zlÄ±ydÄ±, satÄ±cÄ± nazikti ve kullanÄ±mÄ± Ã§ok pratik.\",\"[0,1,0,0,1,0,1,0,0,0,0,0,0,0]\"\n",
        "75,\"ÃœrÃ¼n hasarlÄ±ydÄ±, kutu ezilmiÅŸti, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f Ã§Ä±ktÄ±.\",\"[0,0,0,-1,0,0,0,0,0,0,-1,0,0,-1]\"\n",
        "76,\"Genel memnuniyetim yÃ¼ksek, Ã¼rÃ¼n kaliteli, rengi Ã§ok hoÅŸ ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,0,0,0,0,1,1,0,0,1,0]\"\n",
        "77,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu, iade sÃ¼reci Ã§ok uzundu.\",\"[0,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "78,\"ÃœrÃ¼n Ã¶zellik olarak baÅŸarÄ±lÄ±, kullanÄ±mÄ± rahat, genel kalite iyi ama fiyatÄ± pahalÄ±.\",\"[0,1,0,0,1,0,0,1,1,0,0,0,-1,0]\"\n",
        "79,\"YanlÄ±ÅŸ renk ve hasarlÄ± Ã¼rÃ¼n geldi, kargo da geÃ§ teslim edildi.\",\"[-1,0,0,-1,0,0,0,0,0,-1,0,0,0,-1]\"\n",
        "80,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n sorunsuz geldi, kurulumu kolay ve genel olarak Ã§ok memnunum.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "81,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± rahat ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, iade sÃ¼reci uzadÄ± ve kargo geÃ§ geldi.\",\"[-1,1,0,0,1,-1,0,0,1,0,0,0,0,-1]\"\n",
        "82,\"Genel olarak memnun kaldÄ±m, Ã¼rÃ¼nÃ¼n Ã¶zellikleri baÅŸarÄ±lÄ±, rengi gÃ¼zel ve fiyat performansÄ± iyi.\",\"[0,1,0,0,0,0,0,1,1,1,0,0,1,0]\"\n",
        "83,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve iade sÃ¼reci Ã§ok yorucuydu.\",\"[0,0,0,-1,0,-1,0,0,0,0,-1,0,0,-1]\"\n",
        "84,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n hÄ±zlÄ± geldi, kurulumu kolay ve genel kalite beklentimin Ã¼zerindeydi.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "85,\"YanlÄ±ÅŸ beden gÃ¶nderildi, iade etmek zorunda kaldÄ±m ama Ã¼rÃ¼nÃ¼n kalitesi ve Ã¶zellikleri iyiydi.\",\"[-1,0,-1,0,0,-1,0,1,1,0,0,0,0,0]\"\n",
        "86,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± Ã§ok kÃ¶tÃ¼ ve fiyatÄ±na kesinlikle deÄŸmez.\",\"[0,0,0,0,0,0,0,0,0,0,-1,0,-1,0]\"\n",
        "87,\"ÃœrÃ¼n saÄŸlam geldi, kargo hÄ±zlÄ±ydÄ±, rengi gÃ¼zel ve kullanÄ±mÄ± Ã§ok pratik.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,0,0]\"\n",
        "88,\"Eksik parÃ§a vardÄ±, Ã¼rÃ¼n tam Ã§alÄ±ÅŸmadÄ±, satÄ±cÄ± ilgisizdi ve iade sÃ¼reci uzadÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,0]\"\n",
        "89,\"ÃœrÃ¼n Ã¶zellikleri beklentimi karÅŸÄ±ladÄ±, kaliteli hissettiriyor ama fiyatÄ± biraz pahalÄ±.\",\"[0,1,0,0,0,0,0,1,1,0,0,0,-1,0]\"\n",
        "90,\"Kargo Ã§ok geÃ§ geldi, kutu ezilmiÅŸti ve Ã¼rÃ¼n hasarlÄ±ydÄ±.\",\"[0,0,0,-1,0,0,0,0,0,0,0,0,0,-1]\"\n",
        "91,\"Genel memnuniyetim yÃ¼ksek, Ã¼rÃ¼n kaliteli, rengi ÅŸÄ±k ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,0,0,0,0,1,1,0,0,1,0]\"\n",
        "92,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderilmiÅŸti, iade sÃ¼reci zorlayÄ±cÄ±ydÄ± ama satÄ±cÄ± sonunda yardÄ±mcÄ± oldu.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,0,0,0]\"\n",
        "93,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ± ve kargo firmasÄ± Ã§ok geÃ§ teslim etti.\",\"[0,0,0,-1,0,0,0,1,0,0,0,0,0,-1]\"\n",
        "94,\"ÃœrÃ¼n beklentimin Ã¼zerinde Ã§Ä±ktÄ±, kullanÄ±mÄ± kolay, kaliteli ve fiyatÄ±na gÃ¶re Ã§ok iyi.\",\"[0,1,0,0,1,0,0,0,1,0,0,0,1,0]\"\n",
        "95,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu ve iade almak Ã§ok zor oldu.\",\"[0,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "96,\"SatÄ±cÄ± nazikti, Ã¼rÃ¼n hÄ±zlÄ± geldi, kurulumu kolay ama rengi gÃ¶rseldekinden farklÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,0,0,-1,0,0,0,0]\"\n",
        "97,\"ÃœrÃ¼n kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ama Ã¶zellikleri fena deÄŸildi.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,0,0]\"\n",
        "98,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat ve gÃ¶rÃ¼nÃ¼mÃ¼ gÃ¼zel ama fiyat biraz yÃ¼ksek.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,-1,0]\"\n",
        "99,\"Eksik parÃ§a nedeniyle Ã¼rÃ¼n Ã§alÄ±ÅŸmadÄ±, iade sÃ¼reci uzadÄ± ve satÄ±cÄ± ilgisizdi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,0]\"\n",
        "100,\"ÃœrÃ¼n sorunsuz geldi, kurulumu kolaydÄ± ve genel kalite beklentimi karÅŸÄ±ladÄ±.\",\"[0,1,0,0,1,0,0,0,1,0,0,0,0,0]\"\n",
        "101,\"YanlÄ±ÅŸ renk gÃ¶nderildi, kargo geÃ§ geldi ama Ã¼rÃ¼n kaliteli ve kullanÄ±mÄ± rahattÄ±.\",\"[-1,0,0,0,1,0,0,0,1,-1,0,0,0,-1]\"\n",
        "102,\"ÃœrÃ¼n fiyatÄ±na gÃ¶re baÅŸarÄ±lÄ±, Ã¶zellikleri iyi ve genel kalite yÃ¼ksek.\",\"[0,1,0,0,0,0,0,1,1,0,0,0,1,0]\"\n",
        "103,\"ÃœrÃ¼n hasarlÄ±ydÄ±, kutu ezilmiÅŸti ve dayanÄ±klÄ±lÄ±ÄŸÄ± Ã§ok zayÄ±f Ã§Ä±ktÄ±.\",\"[0,0,0,-1,0,0,0,0,0,0,-1,0,0,-1]\"\n",
        "104,\"SatÄ±cÄ± Ã§ok ilgiliydi, Ã¼rÃ¼n hÄ±zlÄ± geldi ve genel memnuniyetim yÃ¼ksek.\",\"[0,1,0,0,0,0,1,0,1,0,0,0,0,0]\"\n",
        "105,\"ÃœrÃ¼n yanlÄ±ÅŸ beden geldi, iade sÃ¼reci uzundu ama Ã¼rÃ¼nÃ¼n kalitesi iyiydi.\",\"[-1,0,-1,0,0,-1,0,0,1,0,0,0,0,0]\"\n",
        "106,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, fiyatÄ±na deÄŸmez ve dayanÄ±klÄ±lÄ±ÄŸÄ± Ã§ok kÃ¶tÃ¼.\",\"[0,0,0,0,0,0,0,0,0,0,-1,0,-1,0]\"\n",
        "107,\"ÃœrÃ¼n saÄŸlam geldi, rengi gÃ¼zel, kullanÄ±mÄ± pratik ve fiyat performansÄ± iyi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,0]\"\n",
        "108,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve iade sÃ¼reci Ã§ok yorucuydu.\",\"[0,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "109,\"ÃœrÃ¼n kaliteli hissettiriyor, Ã¶zellikleri baÅŸarÄ±lÄ± ama fiyat biraz pahalÄ±.\",\"[0,1,0,0,0,0,0,1,1,0,0,0,-1,0]\"\n",
        "110,\"SatÄ±cÄ± nazikti, Ã¼rÃ¼n sorunsuz geldi, kurulumu kolay ve genel olarak memnunum.\",\"[0,1,0,0,1,0,1,0,1,0,0,0,0,0]\"\n",
        "111,\"ÃœrÃ¼n genel olarak kaliteli ve kullanÄ±mÄ± kolay, rengi Ã§ok hoÅŸ ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderilmiÅŸti, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ± ve iade sÃ¼reci uzadÄ±.\",\"[-1,1,0,0,1,-1,0,0,1,1,0,0,0,-1]\"\n",
        "112,\"ÃœrÃ¼n Ã¶zellik olarak baÅŸarÄ±lÄ±, kaliteli hissettiriyor ve fiyat performansÄ± iyi fakat kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, kargo da geÃ§ teslim edildi.\",\"[0,1,0,0,0,0,0,1,1,0,-1,0,1,-1]\"\n",
        "113,\"YanlÄ±ÅŸ beden gÃ¶nderildi, iade sÃ¼reci yorucuydu, kargo gecikti ama satÄ±cÄ± nazikti, Ã¼rÃ¼n kaliteli ve gÃ¶rÃ¼nÃ¼mÃ¼ gÃ¼zeldi.\",\"[-1,0,-1,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "114,\"ÃœrÃ¼n hasarlÄ± geldi, parÃ§asÄ± eksikti, kutu patlamÄ±ÅŸtÄ± ve dayanÄ±klÄ±lÄ±ÄŸÄ± da zayÄ±f ama kurulumu kolaydÄ±.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "115,\"Genel olarak memnun kaldÄ±m, Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± rahat, rengi gÃ¼zel ve fiyatÄ±na gÃ¶re performansÄ± iyi ama kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,-1]\"\n",
        "116,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu, satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ve fiyatÄ±na deÄŸmez bir deneyimdi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "117,\"ÃœrÃ¼n Ã¶zellikleri beklentimi karÅŸÄ±ladÄ±, kurulumu kolay, genel kalite iyi ama yanlÄ±ÅŸ renk gÃ¶nderildi ve fiyat biraz pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,0,0,1,1,-1,0,0,-1,0]\"\n",
        "118,\"ÃœrÃ¼n saÄŸlam geldi, kullanÄ±mÄ± rahat, satÄ±cÄ± Ã§ok ilgiliydi, kargo hÄ±zlÄ±ydÄ± fakat kÄ±sa sÃ¼rede yÄ±prandÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,-1,0,0,0]\"\n",
        "119,\"Eksik parÃ§a nedeniyle Ã¼rÃ¼n tam Ã§alÄ±ÅŸmadÄ±, iade sÃ¼reci sinir bozucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "120,\"ÃœrÃ¼n kaliteli, gÃ¶rÃ¼nÃ¼mÃ¼ ÅŸÄ±k, kullanÄ±mÄ± kolay ve fiyat performansÄ± baÅŸarÄ±lÄ± ama kutu hasarlÄ± geldi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,-1]\"\n",
        "121,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolay, rengi gÃ¼zel ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ± ve iade sÃ¼reci uzadÄ±.\",\"[-1,1,0,0,1,-1,0,0,1,1,0,0,0,-1]\"\n",
        "122,\"Genel memnuniyetim var, Ã¼rÃ¼n Ã¶zellikleri baÅŸarÄ±lÄ± ve fiyat performansÄ± iyi fakat kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo da gecikti.\",\"[0,1,0,0,0,0,0,1,1,0,-1,0,1,-1]\"\n",
        "123,\"YanlÄ±ÅŸ beden gÃ¶nderildi, iade sÃ¼reci yorucuydu, kargo geÃ§ geldi ama satÄ±cÄ± nazikti ve Ã¼rÃ¼nÃ¼n kalitesi iyiydi.\",\"[-1,0,-1,0,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "124,\"ÃœrÃ¼n hasarlÄ± geldi, parÃ§asÄ± eksikti, kutu patlamÄ±ÅŸtÄ± ve dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±ftÄ± ama kurulumu oldukÃ§a kolaydÄ±.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "125,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, rengi Ã§ok hoÅŸ ve fiyat performansÄ± iyi fakat kargo geÃ§ teslim edildi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,-1]\"\n",
        "126,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu, satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ve fiyatÄ±na kesinlikle deÄŸmezdi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "127,\"ÃœrÃ¼n Ã¶zellikleri beklentimi karÅŸÄ±ladÄ±, genel kalite iyi ve kullanÄ±mÄ± kolay ama yanlÄ±ÅŸ renk gÃ¶nderildi ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,0,0,1,1,-1,0,0,-1,0]\"\n",
        "128,\"ÃœrÃ¼n saÄŸlam geldi, kullanÄ±mÄ± rahat, satÄ±cÄ± Ã§ok ilgiliydi, kargo hÄ±zlÄ±ydÄ± fakat kÄ±sa sÃ¼rede yÄ±prandÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,-1,0,0,0]\"\n",
        "129,\"Eksik parÃ§a yÃ¼zÃ¼nden Ã¼rÃ¼n Ã§alÄ±ÅŸmadÄ±, iade sÃ¼reci sinir bozucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "130,\"ÃœrÃ¼n kaliteli, gÃ¶rÃ¼nÃ¼mÃ¼ ÅŸÄ±k, kullanÄ±mÄ± kolay ve fiyat performansÄ± baÅŸarÄ±lÄ± ama kutu hasarlÄ± geldi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,-1]\"\n",
        "131,\"ÃœrÃ¼n genel olarak memnun edici, kullanÄ±mÄ± kolay ve kaliteli; Ã¶zellikleri ve rengi gÃ¼zel ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, iade sÃ¼reci uzadÄ± ve kargo geÃ§ geldi, fiyat performansÄ± yine de iyi.\",\"[-1,1,0,0,1,-1,0,1,1,1,0,0,1,-1]\"\n",
        "132,\"Beden bana uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade sÃ¼reci zorladÄ±; buna raÄŸmen kurulumu kolaydÄ± ve satÄ±cÄ± nazikti ama dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, fiyatÄ±na deÄŸmez ve kargo da gecikti.\",\"[0,0,-1,-1,1,-1,1,0,0,0,-1,0,-1,-1]\"\n",
        "133,\"ÃœrÃ¼nÃ¼ sevdim; Ã¶zellikleri baÅŸarÄ±lÄ±, kaliteli ve rengi ÅŸÄ±k fakat kÄ±sa sÃ¼rede yÄ±prandÄ±, bir parÃ§asÄ± eksikti ve kargo geÃ§ teslim edildi, satÄ±cÄ± yine de yardÄ±mcÄ± oldu.\",\"[0,1,0,0,0,0,1,1,1,1,-1,-1,0,-1]\"\n",
        "134,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, iade sÃ¼reci uzadÄ±; buna karÅŸÄ±n satÄ±cÄ± ilgiliydi, Ã¼rÃ¼nÃ¼n Ã¶zellikleri ve kalitesi iyi, fiyat performansÄ± da fena deÄŸil ama kargo gecikti.\",\"[-1,0,-1,0,0,-1,1,1,1,0,0,0,1,-1]\"\n",
        "135,\"ÃœrÃ¼n hasarlÄ± ve kutusu ezilmiÅŸ geldi, bir parÃ§asÄ± eksikti; buna raÄŸmen kurulumu kolay, rengi gÃ¼zel ve kalite hissi iyi fakat dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f.\",\"[0,0,0,-1,1,0,0,0,1,1,-1,-1,0,-1]\"\n",
        "136,\"Genel olarak memnunum; kullanÄ±mÄ± kolay, satÄ±cÄ± Ã§ok ilgili, Ã¶zellikleri ve genel kalitesi iyi ama fiyat biraz pahalÄ± ve kargo gecikti.\",\"[0,1,0,0,1,0,1,1,1,0,0,0,-1,-1]\"\n",
        "137,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve eksik parÃ§a vardÄ±; iade sÃ¼reci uzadÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ydÃ¼ ve kargo rezaletti ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,-1,0,-1,1,0,0,0,-1,-1,0,-1]\"\n",
        "138,\"ÃœrÃ¼n bana tam uydu, kurulumu kolay, Ã¶zellikleri baÅŸarÄ±lÄ±; kaliteli, rengi ÅŸÄ±k ve fiyat performansÄ± Ã§ok iyi.\",\"[0,1,1,0,1,0,0,1,1,1,0,0,1,0]\"\n",
        "139,\"ÃœrÃ¼n hasarlÄ± geldi, iade sÃ¼reci sinir bozucuydu; kurulumu kolaydÄ±, satÄ±cÄ± nazikti, Ã¶zellikleri gÃ¼zel ama dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo gecikti.\",\"[0,0,0,-1,1,-1,1,1,0,0,-1,0,0,-1]\"\n",
        "140,\"Genel memnuniyetim yÃ¼ksek; kullanÄ±mÄ± kolay, satÄ±cÄ± ilgili, Ã¶zellikleri ve genel kalitesi iyi, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "141,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi hoÅŸ ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, iade sÃ¼reci uzadÄ± ve kargo geÃ§ geldi.\",\"[-1,1,0,0,1,-1,0,1,1,1,0,0,0,-1]\"\n",
        "142,\"Genel memnuniyetim var; Ã¼rÃ¼n kaliteli, kurulumu kolay ve fiyat performansÄ± iyi fakat kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo gecikti.\",\"[0,1,0,0,1,0,0,0,1,0,-1,0,1,-1]\"\n",
        "143,\"YanlÄ±ÅŸ beden gÃ¶nderildi, iade sÃ¼reci yorucuydu; satÄ±cÄ± nazikti ama kargo geÃ§ geldi, Ã¼rÃ¼nÃ¼n kalitesi iyi ve gÃ¶rÃ¼nÃ¼mÃ¼ gÃ¼zeldi.\",\"[-1,0,-1,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "144,\"ÃœrÃ¼n hasarlÄ± geldi, parÃ§asÄ± eksikti ve kutu patlamÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo da geÃ§ teslim edildi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "145,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, rengi Ã§ok hoÅŸ ve fiyat performansÄ± baÅŸarÄ±lÄ± fakat kargo geÃ§ geldi ve kutu hasarlÄ±ydÄ±.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,-1]\"\n",
        "146,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; iade sÃ¼reci Ã§ok uzadÄ±, satÄ±cÄ± ilgisizdi ve fiyatÄ±na kesinlikle deÄŸmez.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "147,\"ÃœrÃ¼n Ã¶zellikleri beklentimi karÅŸÄ±ladÄ±, genel kalite iyi ve kullanÄ±mÄ± kolay ama yanlÄ±ÅŸ renk gÃ¶nderildi, fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,0,0,1,1,-1,0,0,-1,0]\"\n",
        "148,\"ÃœrÃ¼n saÄŸlam geldi, kullanÄ±mÄ± rahat; satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ± fakat kÄ±sa sÃ¼rede yÄ±prandÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,-1,0,0,0]\"\n",
        "149,\"Eksik parÃ§a yÃ¼zÃ¼nden Ã¼rÃ¼n Ã§alÄ±ÅŸmadÄ±; iade sÃ¼reci sinir bozucuydu, satÄ±cÄ± ilgisizdi ve kargo geÃ§ geldi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "150,\"ÃœrÃ¼n kaliteli, gÃ¶rÃ¼nÃ¼mÃ¼ ÅŸÄ±k, kullanÄ±mÄ± kolay ve fiyat performansÄ± iyi ama kutu hasarlÄ± geldi.\",\"[0,1,0,0,1,0,0,0,1,1,0,0,1,-1]\"\n",
        "151,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi; iade sÃ¼reci uzadÄ±, kargo geÃ§ geldi ama Ã¼rÃ¼nÃ¼n kalitesi ve Ã¶zellikleri iyiydi.\",\"[-1,0,0,0,0,-1,0,1,1,-1,0,0,0,-1]\"\n",
        "152,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay, satÄ±cÄ± ilgili fakat dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat biraz pahalÄ±.\",\"[0,1,0,0,1,0,1,0,1,0,-1,0,-1,0]\"\n",
        "153,\"ÃœrÃ¼n hasarlÄ± ve eksik parÃ§a ile geldi; iade sÃ¼reci uzadÄ±, kargo rezaletti ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "154,\"ÃœrÃ¼n Ã¶zellik olarak baÅŸarÄ±lÄ±, kaliteli hissettiriyor; kullanÄ±mÄ± rahat ve rengi gÃ¼zel ama fiyatÄ± yÃ¼ksek.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,0]\"\n",
        "155,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci zorlayÄ±cÄ± ve fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,0,-1,0,0,0,0,-1,0,-1,-1]\"\n",
        "156,\"ÃœrÃ¼n sorunsuz geldi; satÄ±cÄ± nazikti, kurulumu kolay ve genel kalite iyi ama yanlÄ±ÅŸ renk gÃ¶nderilmiÅŸti.\",\"[-1,1,0,0,1,0,1,0,1,-1,0,0,0,0]\"\n",
        "157,\"ÃœrÃ¼n hiÃ§ gelmedi; kargo kayboldu, satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ve tam bir hayal kÄ±rÄ±klÄ±ÄŸÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "158,\"ÃœrÃ¼n beklentimin Ã¼zerinde Ã§Ä±ktÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ±, kaliteli ve fiyat performansÄ± Ã§ok iyi.\",\"[0,1,0,0,1,0,0,1,1,0,0,0,1,0]\"\n",
        "159,\"ÃœrÃ¼n hasarlÄ± geldi, kutu ezilmiÅŸti; dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, iade sÃ¼reci sinir bozucuydu ve kargo geÃ§ geldi.\",\"[0,0,0,-1,0,-1,0,0,0,0,-1,0,0,-1]\"\n",
        "161,\"ÃœrÃ¼n genel olarak gÃ¼zel ve kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi hoÅŸ; satÄ±cÄ± nazikti ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderilmiÅŸ, kargo geÃ§ kalmÄ±ÅŸ ve kutu hasarlÄ± gelmiÅŸ, iade sÃ¼reci de uzadÄ±, fiyatÄ±na gÃ¶re yine fena deÄŸil.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,1,-1]\"\n",
        "162,\"Beklentimi karÅŸÄ±ladÄ±; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± rahat ve Ã¶zellikleri iyi fakat beden kÃ¼Ã§Ã¼k geldi, kÄ±sa sÃ¼rede yÄ±prandÄ±, kargo gecikti, kutu ezilmiÅŸti ve fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,-1,0,1,0,0,1,1,1,-1,0,-1,-1]\"\n",
        "163,\"ÃœrÃ¼n hasarlÄ± geldi, parÃ§asÄ± eksikti ve kutu patlamÄ±ÅŸtÄ±; kurulumu kolay olsa da Ã¼rÃ¼n Ã§abuk bozuldu, iade sÃ¼reci zorladÄ±, kargo gecikti ve satÄ±cÄ± pek yardÄ±mcÄ± olmadÄ±.\",\"[-1,0,0,-1,1,-1,0,0,0,0,-1,-1,0,-1]\"\n",
        "164,\"Genel memnuniyetim var; Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve Ã¶zellikleri baÅŸarÄ±lÄ±, kullanÄ±mÄ± da kolay ama yanlÄ±ÅŸ renk gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ± ve fiyat biraz yÃ¼ksek.\",\"[-1,1,0,0,1,0,0,1,1,-1,0,0,-1,-1]\"\n",
        "165,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ve fiyatÄ±na deÄŸmezdi, ayrÄ±ca sipariÅŸ zaten yanlÄ±ÅŸ iÅŸlenmiÅŸ gibiydi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "166,\"ÃœrÃ¼n Ã§ok ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor; rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat, Ã¶zellikleri de iyi fakat kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± bu yÃ¼zden dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "167,\"YanlÄ±ÅŸ beden ve yanlÄ±ÅŸ renk geldi; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸil ve Ã¶zellikleri de idare eder.\",\"[-1,0,-1,0,0,-1,1,1,1,-1,0,0,0,-1]\"\n",
        "168,\"ÃœrÃ¼n hasarlÄ± geldi, kutu ezilmiÅŸti; buna raÄŸmen kullanÄ±m kolay ama Ã¼rÃ¼n kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci zordu ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,-1,0,0,0,0,-1,0,0,-1]\"\n",
        "169,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve hÄ±zlÄ± kargoladÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli ve rengi gÃ¼zel, fiyat performansÄ± da iyi.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "170,\"ÃœrÃ¼n yanlÄ±ÅŸ Ã¼rÃ¼n olarak geldi ve bir parÃ§asÄ± eksikti; kargo gecikti, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "171,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi ve genel kalite yÃ¼ksek, rengi de gÃ¼zel ama fiyat pahalÄ± ve kargo gecikti.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "172,\"Beden bÃ¼yÃ¼k geldi, iade etmek zorunda kaldÄ±m; satÄ±cÄ± nazikti ve Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu ama kargo geÃ§ geldi ve kutu ezilmiÅŸti.\",\"[0,0,-1,0,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "173,\"ÃœrÃ¼n hasarlÄ± ve sÄ±zdÄ±rmÄ±ÅŸ ÅŸekilde geldi, kutu patlamÄ±ÅŸtÄ±; Ã¼rÃ¼nÃ¼n Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "174,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu ve dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, ayrÄ±ca kargo geÃ§ geldi.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "175,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo gecikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "176,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "177,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat ve Ã¶zellikleri baÅŸarÄ±lÄ±, rengi ÅŸÄ±k; satÄ±cÄ± ilgili ve hÄ±zlÄ±ydÄ± ama fiyat biraz pahalÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "178,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "179,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±k kÃ¶tÃ¼, iade sÃ¼reci sinir bozucu, kargo gecikmiÅŸti ama Ã¼rÃ¼nÃ¼n Ã¶zellikleri aslÄ±nda gÃ¼zel.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "180,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±, satÄ±cÄ± da nazik ve hÄ±zlÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "181,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± nazikti ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat biraz pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "182,\"ÃœrÃ¼n genel olarak iyi hissettiriyor, kurulumu kolay ve Ã¶zellikleri baÅŸarÄ±lÄ± fakat beden uymadÄ±, kÄ±sa sÃ¼rede yÄ±prandÄ±, kargo gecikti, kutu ezilmiÅŸti ve fiyatÄ±na deÄŸmez.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,-1,-1]\"\n",
        "183,\"ÃœrÃ¼n hasarlÄ± geldi, parÃ§asÄ± eksikti, kutu patlamÄ±ÅŸtÄ±; buna raÄŸmen kullanÄ±mÄ± kolay ama Ã¼rÃ¼n Ã§abuk bozuldu, iade sÃ¼reci zorladÄ± ve kargo geÃ§ teslim edildi.\",\"[0,0,0,-1,1,-1,0,0,0,0,-1,-1,0,-1]\"\n",
        "184,\"Genel memnuniyetim var; Ã¼rÃ¼n kaliteli, rengi gÃ¼zel, Ã¶zellikleri iyi ve kullanÄ±mÄ± rahat ama yanlÄ±ÅŸ renk gÃ¶nderildi, kargo gecikti, kutu hasarlÄ±ydÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,0,0,1,1,-1,0,0,-1,-1]\"\n",
        "185,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ sÃ¼reci baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "186,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kullanÄ±mÄ± rahat ve Ã¶zellikleri iyi fakat kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± hayal kÄ±rÄ±klÄ±ÄŸÄ± yarattÄ±.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "187,\"YanlÄ±ÅŸ beden ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo geÃ§ geldi ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n kalitesi ve Ã¶zellikleri yine de fena deÄŸildi.\",\"[-1,0,-1,0,0,-1,1,1,1,-1,0,0,0,-1]\"\n",
        "188,\"ÃœrÃ¼n hasarlÄ± geldi, kutu ezilmiÅŸti; kullanÄ±mÄ± kolay olsa da kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ydÃ¼, iade sÃ¼reci zordu ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,-1,0,0,0,0,-1,0,0,-1]\"\n",
        "189,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi, kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "190,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, parÃ§a eksikti, kargo gecikti, kutu hasarlÄ±ydÄ±; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "191,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "192,\"Beden bÃ¼yÃ¼k geldi, iade etmek zorunda kaldÄ±m; satÄ±cÄ± nazikti, Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu ama kargo gecikti ve kutu ezilmiÅŸti.\",\"[0,0,-1,0,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "193,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¼rÃ¼nÃ¼n Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "194,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; buna raÄŸmen kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez ve kargo geÃ§ geldi.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "195,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo gecikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "196,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "197,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "198,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "199,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "200,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "201,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ±, rengi gÃ¼zel ve genel kalite iyi; ancak yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat biraz pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,0,1,1,1,0,0,-1,-1]\"\n",
        "202,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay, Ã¶zellikleri iyi ve fiyat performansÄ± fena deÄŸil ama kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo gecikti.\",\"[0,1,0,0,1,0,0,1,1,0,-1,0,1,-1]\"\n",
        "203,\"YanlÄ±ÅŸ beden gÃ¶nderildi, iade sÃ¼reci yorucuydu; Ã¼rÃ¼n hasarlÄ±ydÄ±, bir parÃ§asÄ± eksikti, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "204,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolay olsa da kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci uzadÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "205,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ sÃ¼reci baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "206,\"ÃœrÃ¼n Ã§ok ÅŸÄ±k, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "207,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± nazikti ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "208,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "209,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi, kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "210,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "211,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "212,\"Beden uymadÄ±, iade etmek zorunda kaldÄ±m; Ã¼rÃ¼n hasarlÄ±ydÄ±, kargo gecikti ama satÄ±cÄ± nazikti ve Ã¼rÃ¼nÃ¼n kalitesi iyiydi.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "213,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "214,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "215,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "216,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "217,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "218,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "219,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "220,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "221,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± nazikti ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "222,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo gecikti.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,0,-1]\"\n",
        "223,\"YanlÄ±ÅŸ beden ve yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ±, parÃ§a eksikti; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "224,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "225,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "226,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "227,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± nazikti ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "228,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "229,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "230,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "231,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "232,\"Beden uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade etmek zorunda kaldÄ±m; kargo gecikti ama satÄ±cÄ± nazikti ve kalite fena deÄŸildi.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "233,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "234,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; buna raÄŸmen kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "235,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "236,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "237,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "238,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "239,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "240,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "241,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± ilgiliydi ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "242,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo gecikti.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,0,-1]\"\n",
        "243,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve parÃ§asÄ± eksikti; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "244,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "245,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ sÃ¼reci baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "246,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "247,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± ilgiliydi ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "248,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "249,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "250,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "251,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "252,\"Beden uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade etmek zorunda kaldÄ±m; kargo gecikti ama satÄ±cÄ± nazikti ve Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "253,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "254,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "255,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "256,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "257,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "258,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "259,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "260,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "261,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± ilgiliydi ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "262,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo gecikti.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,0,-1]\"\n",
        "263,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve parÃ§asÄ± eksikti; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "264,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "265,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ sÃ¼reci baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "266,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "267,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± ilgiliydi ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "268,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "269,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "270,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "271,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "272,\"Beden uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade etmek zorunda kaldÄ±m; kargo gecikti ama satÄ±cÄ± nazikti ve Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "273,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "274,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "275,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "276,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "277,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "278,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "279,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "280,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "281,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± nazikti ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo geÃ§ geldi, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "282,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, kargo gecikti ve fiyatÄ±na deÄŸmez.\",\"[0,1,-1,0,1,0,1,1,1,0,-1,0,-1,-1]\"\n",
        "283,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve parÃ§asÄ± eksikti; kurulumu kolaydÄ± ama iade sÃ¼reci uzadÄ±, kargo gecikti ve satÄ±cÄ± yine de yardÄ±mcÄ± oldu.\",\"[-1,0,-1,-1,1,-1,1,0,1,1,0,-1,0,-1]\"\n",
        "284,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ±, kargo rezaletti ve fiyat pahalÄ±ydÄ±.\",\"[0,1,0,-1,1,-1,0,1,1,1,-1,0,1,-1]\"\n",
        "285,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ama Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu, rengi gÃ¼zeldi ve fiyatÄ±na deÄŸmezdi.\",\"[-1,1,0,0,1,-1,1,0,1,1,0,-1,-1,-1]\"\n",
        "286,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor; rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat, Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼, kargo da gecikti.\",\"[0,1,0,0,1,0,1,1,1,1,-1,0,-1,-1]\"\n",
        "287,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti ve renk farklÄ± geldi; satÄ±cÄ± ilgiliydi ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu ve dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±ftÄ±.\",\"[-1,0,0,0,1,-1,1,1,1,-1,-1,0,0,-1]\"\n",
        "288,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ama kÄ±sa sÃ¼rede yÄ±prandÄ± ve kutu hasarlÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,-1,0,1,-1]\"\n",
        "289,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri ve rengi gÃ¼zeldi.\",\"[-1,0,0,-1,1,-1,0,1,1,1,-1,0,0,-1]\"\n",
        "290,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ± ama kargo gecikti ve kutu hasarlÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,-1,0,1,-1]\"\n",
        "291,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± ilgiliydi ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo gecikti, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "292,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo geÃ§ geldi.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,0,-1]\"\n",
        "293,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve parÃ§asÄ± eksikti; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "294,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "295,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "296,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "297,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± ilgiliydi ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "298,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "299,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "300,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "301,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "302,\"Beden uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade etmek zorunda kaldÄ±m; kargo gecikti ama satÄ±cÄ± nazikti ve Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "303,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "304,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "305,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "306,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "307,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "308,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "309,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "310,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "311,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± ilgiliydi ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo gecikti, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "312,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, kargo gecikti ve fiyatÄ±na deÄŸmez.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,-1,-1]\"\n",
        "313,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve parÃ§asÄ± eksikti; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "314,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "315,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "316,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "317,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± ilgiliydi ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "318,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "319,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "320,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "321,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "322,\"Beden uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade etmek zorunda kaldÄ±m; kargo gecikti ama satÄ±cÄ± nazikti ve Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "323,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "324,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "325,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "326,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "327,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "328,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "329,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "330,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "331,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolaydÄ±, Ã¶zellikleri ve rengi gÃ¼zeldi; ancak yanlÄ±ÅŸ Ã¼rÃ¼n geldi, kargo gecikti, kutu hasarlÄ±ydÄ± ve iade sÃ¼reci uzadÄ±.\",\"[-1,1,0,0,1,-1,0,1,1,1,0,0,0,-1]\"\n",
        "332,\"ÃœrÃ¼n Ã¶zellik olarak baÅŸarÄ±lÄ±ydÄ± ama beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, kargo gecikti ve fiyat pahalÄ±ydÄ±.\",\"[0,1,-1,0,0,0,0,1,1,0,-1,0,-1,-1]\"\n",
        "333,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, hasarlÄ± ve eksik parÃ§a vardÄ±; iade sÃ¼reci zorladÄ±, kargo gecikti ama satÄ±cÄ± ilgiliydi.\",\"[-1,0,0,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "334,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo rezaletti.\",\"[0,0,0,-1,1,0,0,1,0,0,-1,0,0,-1]\"\n",
        "335,\"ÃœrÃ¼n hiÃ§ gelmedi, sipariÅŸ hatalÄ±ydÄ±; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ve fiyatÄ±na deÄŸmezdi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "336,\"ÃœrÃ¼n kaliteli ve ÅŸÄ±k, rengi gÃ¼zel, kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ± ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "337,\"YanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; satÄ±cÄ± nazikti ama Ã¼rÃ¼n hasarlÄ±ydÄ±.\",\"[-1,0,0,-1,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "338,\"ÃœrÃ¼n eksik parÃ§a ile geldi, hasarlÄ±ydÄ±; kullanÄ±mÄ± kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "339,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± ilgiliydi, kargo hÄ±zlÄ±ydÄ±; Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± iyiydi.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "340,\"Genel olarak memnun kaldÄ±m; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi ama fiyat pahalÄ±ydÄ±, kargo gecikti ve kutu hasarlÄ±ydÄ±.\",\"[0,1,0,0,1,0,0,1,1,0,0,0,-1,-1]\"\n",
        "341,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± ilgiliydi ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo gecikti, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "342,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo geÃ§ geldi.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,0,-1]\"\n",
        "343,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve parÃ§asÄ± eksikti; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "344,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "345,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ sÃ¼reci baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "346,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "347,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± ilgiliydi ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "348,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "349,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "350,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "351,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "352,\"Beden uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade etmek zorunda kaldÄ±m; kargo gecikti ama satÄ±cÄ± nazikti ve Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "353,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "354,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "355,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "356,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "357,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "358,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "359,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "360,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "361,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolaydÄ±, Ã¶zellikleri ve rengi gÃ¼zeldi; ancak yanlÄ±ÅŸ Ã¼rÃ¼n geldi, kargo gecikti, kutu hasarlÄ±ydÄ± ve iade sÃ¼reci uzadÄ±.\",\"[-1,1,0,0,1,-1,0,1,1,1,0,0,0,-1]\"\n",
        "362,\"ÃœrÃ¼n Ã¶zellik olarak baÅŸarÄ±lÄ±ydÄ± ama beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, kargo gecikti ve fiyat pahalÄ±ydÄ±.\",\"[0,1,-1,0,0,0,0,1,1,0,-1,0,-1,-1]\"\n",
        "363,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, hasarlÄ± ve eksik parÃ§a vardÄ±; iade sÃ¼reci zorladÄ±, kargo gecikti ama satÄ±cÄ± ilgiliydi.\",\"[-1,0,0,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "364,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo rezaletti.\",\"[0,0,0,-1,1,0,0,1,0,0,-1,0,0,-1]\"\n",
        "365,\"ÃœrÃ¼n hiÃ§ gelmedi, sipariÅŸ hatalÄ±ydÄ±; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ve fiyatÄ±na deÄŸmezdi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "366,\"ÃœrÃ¼n kaliteli ve ÅŸÄ±k, rengi gÃ¼zel, kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ± ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "367,\"YanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; satÄ±cÄ± nazikti ama Ã¼rÃ¼n hasarlÄ±ydÄ±.\",\"[-1,0,0,-1,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "368,\"ÃœrÃ¼n eksik parÃ§a ile geldi, hasarlÄ±ydÄ±; kullanÄ±mÄ± kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "369,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± ilgiliydi, kargo hÄ±zlÄ±ydÄ±; Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± iyiydi.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "370,\"Genel olarak memnun kaldÄ±m; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi ama fiyat pahalÄ±ydÄ±, kargo gecikti ve kutu hasarlÄ±ydÄ±.\",\"[0,1,0,0,1,0,0,1,1,0,0,0,-1,-1]\"\n",
        "371,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi gÃ¼zel; satÄ±cÄ± ilgiliydi ama yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kargo gecikti, kutu hasarlÄ±ydÄ±, iade sÃ¼reci uzadÄ± ve fiyat pahalÄ±ydÄ±.\",\"[-1,1,0,0,1,-1,1,1,1,1,0,0,-1,-1]\"\n",
        "372,\"Genel olarak memnunum; Ã¼rÃ¼n kaliteli, kurulumu kolay ve Ã¶zellikleri iyi fakat beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo geÃ§ geldi.\",\"[0,1,-1,0,1,0,0,1,1,0,-1,0,0,-1]\"\n",
        "373,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ beden gÃ¶nderildi, Ã¼rÃ¼n hasarlÄ±ydÄ± ve parÃ§asÄ± eksikti; iade sÃ¼reci uzadÄ±, kargo gecikti ama satÄ±cÄ± nazikti.\",\"[-1,0,-1,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "374,\"ÃœrÃ¼n hasarlÄ± geldi, kutu patlamÄ±ÅŸtÄ± ve Ã¼rÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,1,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "375,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ±, fiyatÄ±na deÄŸmezdi ve sipariÅŸ sÃ¼reci baÅŸtan hatalÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "376,\"ÃœrÃ¼n ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor, rengi gÃ¼zel, kaliteli ve kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "377,\"YanlÄ±ÅŸ Ã¼rÃ¼n ve yanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; buna raÄŸmen satÄ±cÄ± ilgiliydi ve Ã¼rÃ¼nÃ¼n kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "378,\"ÃœrÃ¼n hasarlÄ± geldi, bir parÃ§asÄ± eksikti; kullanÄ±mÄ± kolaydÄ± ama Ã¼rÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼ ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "379,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± Ã§ok ilgiliydi ve kargo hÄ±zlÄ±ydÄ±; kurulumu kolay, Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± baÅŸarÄ±lÄ±.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "380,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ±, kargo gecikti ve parÃ§a eksikti; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± telafi etmeye Ã§alÄ±ÅŸtÄ±.\",\"[-1,0,0,0,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "381,\"ÃœrÃ¼n beklentimi karÅŸÄ±ladÄ±; kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, genel kalite yÃ¼ksek ve rengi gÃ¼zel ama fiyat pahalÄ± ve kargo geÃ§ geldi.\",\"[0,1,0,0,1,0,0,1,1,1,0,0,-1,-1]\"\n",
        "382,\"Beden uymadÄ±, Ã¼rÃ¼n hasarlÄ±ydÄ±, iade etmek zorunda kaldÄ±m; kargo gecikti ama satÄ±cÄ± nazikti ve Ã¼rÃ¼n kaliteli gÃ¶rÃ¼nÃ¼yordu.\",\"[0,0,-1,-1,0,-1,1,0,1,0,0,0,0,-1]\"\n",
        "383,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; Ã¶zellikleri iyiydi ama kalite hissi dÃ¼ÅŸtÃ¼, iade sÃ¼reci zorladÄ± ve kargo rezaletti.\",\"[0,0,0,-1,0,-1,0,1,-1,0,0,0,0,-1]\"\n",
        "384,\"ÃœrÃ¼n kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f; yine de kullanÄ±mÄ± kolay ve Ã¶zellikleri gÃ¼zel ama fiyatÄ±na deÄŸmez, kargo da gecikti.\",\"[0,0,0,0,1,0,0,1,0,0,-1,0,-1,-1]\"\n",
        "385,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, kutu hasarlÄ±ydÄ± ve kargo geÃ§ geldi; iade sÃ¼reci uzadÄ± ama satÄ±cÄ± nazikti, Ã¼rÃ¼nÃ¼n rengi gÃ¼zel ve kalitesi fena deÄŸildi.\",\"[-1,0,0,0,0,-1,1,0,1,1,0,0,0,-1]\"\n",
        "386,\"ÃœrÃ¼n eksik parÃ§a ile geldi, kurulumu bu yÃ¼zden zorlaÅŸtÄ±; iade sÃ¼reci yorucuydu, satÄ±cÄ± ilgisizdi ve kargo da geÃ§ geldi.\",\"[-1,0,0,0,-1,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "387,\"ÃœrÃ¼n kaliteli, kullanÄ±mÄ± rahat, Ã¶zellikleri baÅŸarÄ±lÄ± ve rengi ÅŸÄ±k; satÄ±cÄ± ilgiliydi ama fiyat biraz pahalÄ±ydÄ±.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,-1,0]\"\n",
        "388,\"ÃœrÃ¼n hiÃ§ gelmedi, kargo gecikti ve sonunda kayboldu; satÄ±cÄ± ilgisizdi, iade sÃ¼reci uzadÄ± ve tam bir kargo faciasÄ±ydÄ±.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,0,-1]\"\n",
        "389,\"ÃœrÃ¼n hasarlÄ± geldi ve kÄ±sa sÃ¼rede bozuldu; dayanÄ±klÄ±lÄ±ÄŸÄ± kÃ¶tÃ¼, iade sÃ¼reci sinir bozucuydu, kargo gecikmiÅŸti ama Ã¶zellikleri gÃ¼zeldi.\",\"[0,0,0,-1,0,-1,0,1,0,0,-1,0,0,-1]\"\n",
        "390,\"Genel olarak Ã§ok memnunum; Ã¼rÃ¼n kaliteli, kullanÄ±mÄ± kolay, Ã¶zellikleri iyi, rengi gÃ¼zel, fiyat performansÄ± baÅŸarÄ±lÄ± ve satÄ±cÄ± Ã§ok ilgiliydi.\",\"[0,1,0,0,1,0,1,1,1,1,0,0,1,0]\"\n",
        "391,\"ÃœrÃ¼n kaliteli ve kullanÄ±mÄ± kolaydÄ±, Ã¶zellikleri ve rengi gÃ¼zeldi; ancak yanlÄ±ÅŸ Ã¼rÃ¼n geldi, kargo gecikti, kutu hasarlÄ±ydÄ± ve iade sÃ¼reci uzadÄ±.\",\"[-1,1,0,0,1,-1,0,1,1,1,0,0,0,-1]\"\n",
        "392,\"ÃœrÃ¼n Ã¶zellik olarak baÅŸarÄ±lÄ±ydÄ± ama beden uymadÄ±, kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f, kargo gecikti ve fiyat pahalÄ±ydÄ±.\",\"[0,1,-1,0,0,0,0,1,1,0,-1,0,-1,-1]\"\n",
        "393,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, hasarlÄ± ve eksik parÃ§a vardÄ±; iade sÃ¼reci zorladÄ±, kargo gecikti ama satÄ±cÄ± ilgiliydi.\",\"[-1,0,0,-1,0,-1,1,0,0,0,0,-1,0,-1]\"\n",
        "394,\"ÃœrÃ¼n sÄ±zdÄ±rmÄ±ÅŸtÄ±, kutu patlamÄ±ÅŸtÄ±; kurulumu kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu, dayanÄ±klÄ±lÄ±ÄŸÄ± zayÄ±f ve kargo rezaletti.\",\"[0,0,0,-1,1,0,0,1,0,0,-1,0,0,-1]\"\n",
        "395,\"ÃœrÃ¼n hiÃ§ gelmedi, sipariÅŸ hatalÄ±ydÄ±; satÄ±cÄ± ilgisizdi, iade sÃ¼reci Ã§ok uzadÄ± ve fiyatÄ±na deÄŸmezdi.\",\"[-1,0,0,0,0,-1,0,0,0,0,0,-1,-1,-1]\"\n",
        "396,\"ÃœrÃ¼n kaliteli ve ÅŸÄ±k, rengi gÃ¼zel, kullanÄ±mÄ± rahat; Ã¶zellikleri iyi ama kÄ±sa sÃ¼rede yÄ±prandÄ± ve fiyat performansÄ± dÃ¼ÅŸtÃ¼.\",\"[0,1,0,0,1,0,0,1,1,1,-1,0,-1,0]\"\n",
        "397,\"YanlÄ±ÅŸ renk gÃ¶nderildi, iade sÃ¼reci uzadÄ±, kargo gecikti; satÄ±cÄ± nazikti ama Ã¼rÃ¼n hasarlÄ±ydÄ±.\",\"[-1,0,0,-1,0,-1,1,0,1,-1,0,0,0,-1]\"\n",
        "398,\"ÃœrÃ¼n eksik parÃ§a ile geldi, hasarlÄ±ydÄ±; kullanÄ±mÄ± kolaydÄ± ama kÄ±sa sÃ¼rede bozuldu ve kargo geÃ§ geldi.\",\"[0,0,0,-1,1,0,0,0,0,0,-1,-1,0,-1]\"\n",
        "399,\"ÃœrÃ¼n saÄŸlam geldi, satÄ±cÄ± ilgiliydi, kargo hÄ±zlÄ±ydÄ±; Ã¼rÃ¼n kaliteli, rengi gÃ¼zel ve fiyat performansÄ± iyiydi.\",\"[0,1,0,0,1,0,1,0,1,1,0,0,1,0]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "400,\"ÃœrÃ¼n fotoÄŸraftaki renkle birebir aynÄ±, Ã§ok ÅŸÄ±k. Kargo Ã§ok hÄ±zlÄ± geldi ve paketleme Ã§ok saÄŸlamdÄ±. Ancak kurulum kÄ±lavuzu yetersiz, montajda zorlandÄ±m. FiyatÄ± kalitesine gÃ¶re Ã§ok iyi ama ayaÄŸÄ±ma biraz dar geldi.\",\"[0, 1, -1, 0, -1, 0, 0, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "401,\"SatÄ±cÄ± yanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶ndermiÅŸ, gelen parÃ§alar da eksik Ã§Ä±ktÄ±. Ä°ade sÃ¼reci tam bir kabus, muhatap bulamÄ±yorum. Kargo paketi de yolda ezilmiÅŸ ve yÄ±rtÄ±lmÄ±ÅŸ. HiÃ§ memnun kalmadÄ±m, paranÄ±za yazÄ±k.\",\"[-1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1]\"\n",
        "402,\"Kokusu gerÃ§ekten bÃ¼yÃ¼leyici ve Ã§ok kalÄ±cÄ±. ÅžiÅŸe tasarÄ±mÄ± estetik duruyor ama kapaÄŸÄ± ilk kullanÄ±mda elimde kaldÄ±. SatÄ±cÄ±ya durumu ilettim, Ã§ok nazikÃ§e yeni kapak yolladÄ±lar. Boyutu da beklentimden bÃ¼yÃ¼k geldi.\",\"[0, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0]\"\n",
        "403,\"KulaklÄ±k Ã§ok kaliteli ses veriyor ve kullanÄ±mÄ± Ã§ok pratik. FiyatÄ± indirimdeyken almÄ±ÅŸtÄ±m, tam bir fÄ±rsat Ã¼rÃ¼nÃ¼. Kargo kutusu Ã§ok temizdi. Tek sorun rengi ilandakinden bir tÄ±k daha koyu ama benim iÃ§in dert deÄŸil.\",\"[0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "404,\"ÃœrÃ¼n elime ulaÅŸtÄ±ÄŸÄ±nda camÄ± Ã§atlaktÄ±, sanÄ±rÄ±m kargoda darbe almÄ±ÅŸ. SatÄ±cÄ±ya ulaÅŸmak imkansÄ±z, iade butonu da Ã§alÄ±ÅŸmÄ±yor. Boyutu fotoÄŸrafta gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nden Ã§ok daha kÃ¼Ã§Ã¼k. Malzeme kalitesi de Ã§ok ucuz duruyor.\",\"[0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1]\"\n",
        "405,\"Gelen elbisenin kumaÅŸÄ± Ã§ok kaliteli ve kalÄ±bÄ± tam oturdu. SatÄ±cÄ±nÄ±n iÃ§ine koyduÄŸu kÃ¼Ã§Ã¼k not ve hediye beni Ã§ok mutlu etti. Kargo ertesi gÃ¼n elimdeydi. FiyatÄ± biraz yÃ¼ksek olsa da kalitesine deÄŸer.\",\"[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1]\"\n",
        "406,\"CihazÄ±n ÅŸarjÄ± Ã§ok hÄ±zlÄ± bitiyor ve Ä±sÄ±nma yapÄ±yor. Kurulumu da Ã§ok karmaÅŸÄ±ktÄ±. SatÄ±cÄ± Ã§ok ilgili davrandÄ± ama Ã¼rÃ¼nÃ¼n dayanÄ±klÄ±lÄ±ÄŸÄ± konusunda ÅŸÃ¼phelerim var. GÃ¶rsel olarak gÃ¼zel olsa da performans zayÄ±f.\",\"[0, 0, 0, 0, -1, 0, 1, 0, -1, 1, -1, 0, -1, 0]\"\n",
        "407,\"AyakkabÄ± Ã§ok rahat ve hafif ama dikiÅŸ yerlerinden yapÄ±ÅŸkan izleri gÃ¶rÃ¼nÃ¼yor, iÅŸÃ§ilik kÃ¶tÃ¼. SatÄ±cÄ± hÄ±zlÄ± kargoladÄ±. Ä°ade etmek istedim ancak kargo Ã¼cretini bana Ã¶dettiler. Bu fiyata daha iyisi alÄ±nabilirdi.\",\"[0, 0, 1, 0, 0, -1, 1, 0, -1, -1, 0, 0, -1, 1]\"\n",
        "408,\"Mutfak robotu Ã§ok kullanÄ±ÅŸlÄ± ve tÃ¼m parÃ§alarÄ± tam geldi. Rengi mutfaÄŸÄ±ma tam uydu. Paketleme o kadar Ã¶zenliydi ki aÃ§makta zorlandÄ±m. FiyatÄ± da piyasaya gÃ¶re Ã§ok uygun, kesinlikle tavsiye ediyorum.\",\"[0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1]\"\n",
        "409,\"Gelen Ã¼rÃ¼n resmen kullanÄ±lmÄ±ÅŸ ve kirliydi, satÄ±cÄ± hatasÄ±nÄ± asla kabul etmiyor. Kargo kutusu Ä±slak geldi. Ä°ade sÃ¼reciyle uÄŸraÅŸmak Ã§ok yorucu. ÃœrÃ¼nÃ¼n kokusu da Ã§ok kÃ¶tÃ¼, hiÃ§ beklediÄŸim gibi deÄŸil.\",\"[-1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1]\"\n",
        "410,\"Laptop Ã§antasÄ± beklediÄŸimden bÃ¼yÃ¼k geldi ama kalitesi muazzam. SatÄ±cÄ± Ã§ok ilgiliydi, her soruma anÄ±nda yanÄ±t verdi. Paketlemesi Ã§ok titizdi, hiÃ§ zarar gÃ¶rmemiÅŸ. FermuarlarÄ± Ã§ok saÄŸlam duruyor, fiyatÄ± da kalitesine gÃ¶re Ã§ok makul.\",\"[0, 1, -1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\"\n",
        "411,\"ÃœrÃ¼n maalesef kÄ±rÄ±k geldi, plastik kÄ±sÄ±mlarÄ± Ã§atlamÄ±ÅŸ. SatÄ±cÄ±ya ulaÅŸmaya Ã§alÄ±ÅŸtÄ±m ama asla cevap vermiyorlar. Ä°ade sÃ¼reci Ã§ok yavaÅŸ ilerliyor. ÃœrÃ¼nÃ¼n rengi de ilandakinden Ã§ok daha soluk. VerdiÄŸim paraya gerÃ§ekten Ã¼zÃ¼ldÃ¼m.\",\"[0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1]\"\n",
        "412,\"Kahve makinesinin kullanÄ±mÄ± Ã§ok basit, sabahlarÄ± bÃ¼yÃ¼k kolaylÄ±k saÄŸlÄ±yor. TasarÄ±mÄ± mutfakta Ã§ok ÅŸÄ±k duruyor. Kargo Ã§ok hÄ±zlÄ±ydÄ±, ertesi gÃ¼n elimdeydi. Ancak haznesi biraz kÃ¼Ã§Ã¼k, sÄ±k sÄ±k su eklemek gerekiyor. FiyatÄ± ise Ã§ok uygun.\",\"[0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "413,\"SatÄ±cÄ± yanlÄ±ÅŸ renk gÃ¶ndermiÅŸ, ayrÄ±ca iÃ§inden Ã§Ä±kmasÄ± gereken kablo eksikti. Kargo poÅŸeti paramparÃ§aydÄ±, sanki Ã¼zerinde tepinmiÅŸler. Ä°ade ettim ama paramÄ± geri alana kadar Ã§ok uÄŸraÅŸtÄ±m. GÃ¶rÃ¼nÃ¼ÅŸÃ¼ gÃ¼zel olsa da hizmet sÄ±fÄ±r.\",\"[1, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0, -1, 0, -1]\"\n",
        "414,\"Krem Ã§ok gÃ¼zel kokuyor ve cildi yumuÅŸacÄ±k yapÄ±yor. AmbalajÄ± Ã§ok kaliteli ve lÃ¼ks hissettiriyor. SatÄ±cÄ± yanÄ±na bir sÃ¼rÃ¼ tester eklemiÅŸ, Ã§ok nazikler. Kargo kutusu Ã§ok temiz ve saÄŸlamdÄ±. Kesinlikle tekrar alÄ±rÄ±m.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "415,\"ÃœrÃ¼nÃ¼ kurmak tam bir iÅŸkenceydi, delikleri birbirini tutmuyor. AyrÄ±ca ahÅŸap kÄ±sÄ±mlarÄ± Ã§izik dolu geldi. SatÄ±cÄ± Ã§ok kaba bir Ã¼slupla cevap verdi. Kargo teslimatÄ± da Ã§ok gecikti. ÃœrÃ¼nÃ¼n tek artÄ±sÄ± fiyatÄ±nÄ±n ucuz olmasÄ±.\",\"[0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 1, -1]\"\n",
        "416,\"Vakumlu sÃ¼pÃ¼rge Ã§ok gÃ¼Ã§lÃ¼ Ã§ekiyor, temizliÄŸi Ã§ok kolaylaÅŸtÄ±rÄ±yor. Rengi ve tasarÄ±mÄ± Ã§ok modern. Paketleme Ã§ok Ã¶zenliydi, kutuda ezik bile yoktu. SatÄ±cÄ±ya hÄ±zlÄ± kargo iÃ§in teÅŸekkÃ¼rler. ParasÄ±na deÄŸer bir Ã¼rÃ¼n.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "417,\"Botlar ayaÄŸÄ±mÄ± vurdu, kalÄ±bÄ± Ã§ok sert ve dar. Ãœstelik bir haftada tabanÄ± aÃ§Ä±lmaya baÅŸladÄ±, dayanÄ±klÄ±lÄ±ÄŸÄ± Ã§ok kÃ¶tÃ¼. SatÄ±cÄ± iadeyi kabul etmedi. Kargo paketi de sÄ±rÄ±lsÄ±klam bir ÅŸekilde geldi. Tam bir hayal kÄ±rÄ±klÄ±ÄŸÄ±.\",\"[0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1]\"\n",
        "418,\"Tava seti Ã§ok hafif ve kullanÄ±mÄ± rahat, yapÄ±ÅŸma yapmÄ±yor. BoyutlarÄ± aile kullanÄ±mÄ± iÃ§in ideal. Kargo Ã§ok hÄ±zlÄ±ydÄ±. SatÄ±cÄ± Ã¼rÃ¼nÃ¼ balonlu naylona sarmÄ±ÅŸ, Ã§ok saÄŸlam geldi. FiyatÄ± piyasaya gÃ¶re oldukÃ§a cazip.\",\"[0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1]\"\n",
        "419,\"Telefon kÄ±lÄ±fÄ± Ã§ok adi bir plastikten yapÄ±lmÄ±ÅŸ, hemen rengi soldu. FotoÄŸraftakiyle alakasÄ± yok, yanlÄ±ÅŸ model gÃ¶nderilmiÅŸ. SatÄ±cÄ±ya iade iÃ§in yazdÄ±m ama ilgilenmediler. Kargo da Ã§ok geÃ§ geldi. HiÃ§ tavsiye etmiyorum.\",\"[1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1]\"\n",
        "420,\"GÃ¶mleÄŸin kumaÅŸÄ± Ã§ok ince ve kalitesiz, ilk yÄ±kamada hemen Ã§ekti. Rengi de gÃ¶rseldekinden Ã§ok farklÄ±, soluk bir pembe geldi. SatÄ±cÄ± paketlemeye hiÃ§ Ã¶zen gÃ¶stermemiÅŸ, poÅŸeti yÄ±rtÄ±lmÄ±ÅŸ. Kargo da 10 gÃ¼nde anca geldi. FiyatÄ± ucuz ama deÄŸmez.\",\"[0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1]\"\n",
        "421,\"Oyuncu faresi tam elime oturdu, ergonomisi harika. IÅŸÄ±klarÄ± ve tasarÄ±mÄ± Ã§ok havalÄ± duruyor. SatÄ±cÄ± Ã§ok hÄ±zlÄ± kargoladÄ±, paketleme Ã§ok gÃ¼venliydi. YazÄ±lÄ±m kurulumu biraz uÄŸraÅŸtÄ±rdÄ± ama ÅŸu an sorunsuz. Fiyat/performans olarak rakipsiz.\",\"[0, 1, 1, 0, -1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "422,\"ÃœrÃ¼nÃ¼n parÃ§alarÄ± eksik geldi, kurulumu tamamlayamadÄ±m. SatÄ±cÄ±yla iletiÅŸime geÃ§tim, eksik parÃ§ayÄ± hemen kargoladÄ±lar. Kargo kutusu Ã§ok ezilmiÅŸti, neyse ki Ã¼rÃ¼n zarar gÃ¶rmemiÅŸ. Malzeme kalitesi beklentimin Ã¼zerinde, Ã§ok saÄŸlam.\",\"[0, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, -1, 1, -1]\"\n",
        "423,\"ParfÃ¼mÃ¼n kokusu tam istediÄŸim gibi ama kalÄ±cÄ±lÄ±ÄŸÄ± Ã§ok az, yarÄ±m saate uÃ§uyor. ÅžiÅŸesi Ã§ok ÅŸÄ±k ve kargoda hiÃ§ sÄ±zdÄ±rmamÄ±ÅŸ. SatÄ±cÄ±nÄ±n gÃ¶nderdiÄŸi hediye iÃ§in teÅŸekkÃ¼rler. Ancak fiyatÄ± bu performans iÃ§in biraz pahalÄ±.\",\"[0, 1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, -1, 1]\"\n",
        "424,\"Gelen saat Ã§ok aÄŸÄ±r ve kordonu Ã§ok geniÅŸ, kolumda dÃ¶nÃ¼p duruyor. CamÄ±nda kÃ¼Ã§Ã¼k bir Ã§izik vardÄ±, sanÄ±rÄ±m Ã¼retim hatasÄ±. Ä°ade sÃ¼reciyle uÄŸraÅŸmak istemediÄŸim iÃ§in sakladÄ±m. Kargo hÄ±zÄ± iyiydi ama paketleme zayÄ±ftÄ±.\",\"[0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1]\"\n",
        "425,\"KitaplarÄ±n baskÄ± kalitesi mÃ¼kemmel, kapak tasarÄ±mlarÄ± Ã§ok estetik. SatÄ±cÄ± kitaplarÄ± balonlu naylona sarmÄ±ÅŸ, kÃ¶ÅŸeleri bile ezilmemiÅŸ. Kargo Ã§ok hÄ±zlÄ±ydÄ±. FiyatÄ± set olarak alÄ±ndÄ±ÄŸÄ±nda Ã§ok avantajlÄ±. Herkese tavsiye ederim.\",\"[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "426,\"SÃ¼pÃ¼rge baÅŸlÄ±ÄŸÄ± hatalÄ± geldi, yerine oturmuyor. SatÄ±cÄ± deÄŸiÅŸim yapamayacaÄŸÄ±nÄ± sÃ¶yledi, iade etmem gerekiyormuÅŸ. Kargo Ã§ok geÃ§ ulaÅŸtÄ± ve kutusu Ä±slaktÄ±. ÃœrÃ¼nÃ¼n Ã§ekim gÃ¼cÃ¼ iyi olsa da bu hizmetle bir daha almam.\",\"[-1, -1, 0, 0, -1, -1, -1, 0, 1, 0, 0, 0, 0, -1]\"\n",
        "427,\"Termosun yalÄ±tÄ±mÄ± harika, 12 saat sonra bile Ã§ayÄ±m sÄ±cacÄ±ktÄ±. Rengi tam istediÄŸim gibi canlÄ±. SatÄ±cÄ± Ã§ok ilgiliydi, faturayÄ± hemen e-posta ile gÃ¶nderdiler. Kargo kutusu sapasaÄŸlam geldi. FiyatÄ±nÄ± sonuna kadar hak ediyor.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "428,\"ÃœrÃ¼n Ã§ok kullanÄ±ÅŸlÄ± ama plastik kokusu Ã§ok baskÄ±n, balkonda havalandÄ±rmam gerekti. Kurulumu basit, 2 dakikada hallettim. SatÄ±cÄ± kargoyu ertesi gÃ¼n Ã§Ä±kardÄ±. Ancak Ã¼rÃ¼nÃ¼n ayaÄŸÄ± biraz sallanÄ±yor, Ã§ok dengeli deÄŸil.\",\"[0, 1, 0, 0, 1, 0, 1, -1, 1, 0, -1, 0, 0, 1]\"\n",
        "429,\"KulaklÄ±ÄŸÄ±n pedi ilk gÃ¼nden yÄ±rtÄ±ldÄ±, malzeme kalitesi Ã§ok kÃ¶tÃ¼. SatÄ±cÄ±ya yazdÄ±m, kullanÄ±cÄ± hatasÄ± dediler. Ä°ade kabul etmiyorlar. Ses kalitesi fena deÄŸil ama bu parayÄ± asla hak etmiyor. Kargo da Ã§ok yavaÅŸ geldi.\",\"[0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1]\"\n",
        "430,\"Masa Ã¶rtÃ¼sÃ¼nÃ¼n deseni harika, mutfaÄŸÄ±ma Ã§ok yakÄ±ÅŸtÄ±. Leke tutmayan kumaÅŸÄ± var, Ã§ok pratik. Kargo Ã§ok hÄ±zlÄ± geldi. SatÄ±cÄ± Ã¼rÃ¼nÃ¼ hediye paketi yapmÄ±ÅŸ, Ã§ok zarif. FiyatÄ± da kalitesine gÃ¶re gayet makul.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "431,\"BotlarÄ±n numarasÄ± dar geldi, bir numara bÃ¼yÃ¼k alÄ±nmalÄ±. Ãœst deri kÄ±smÄ± Ã§ok sert, ayaÄŸÄ±mÄ± acÄ±ttÄ±. SatÄ±cÄ± iade sÃ¼recinde Ã§ok zorluk Ã§Ä±kardÄ±. Kargo paketi de kirlenmiÅŸ ve yÄ±rtÄ±lmÄ±ÅŸtÄ±. ParasÄ±na deÄŸecek bir Ã¼rÃ¼n deÄŸil.\",\"[0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1]\"\n",
        "432,\"Tost makinesi Ã§ok hÄ±zlÄ± Ä±sÄ±nÄ±yor ve plakalarÄ± kolay Ã§Ä±kÄ±yor, temizliÄŸi rahat. Rengi Ã§ok modern duruyor. Paketleme Ã§ok Ã¶zenliydi, zarar gÃ¶rmeden geldi. SatÄ±cÄ± Ã§ok gÃ¼venilir. Tek eksisi kablosunun biraz kÄ±sa olmasÄ±.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "433,\"Gelen gÃ¼neÅŸ gÃ¶zlÃ¼ÄŸÃ¼ yamuk Ã§Ä±ktÄ±, Ã§erÃ§evesi dÃ¼zgÃ¼n durmuyor. Kargo kutusu ezilmiÅŸti, muhtemelen yolda oldu. SatÄ±cÄ± mesajlara dÃ¶nmÃ¼yor. ÃœrÃ¼n fotoÄŸraftaki gibi kaliteli durmuyor, plastik hissi veriyor. Ä°ade edeceÄŸim.\",\"[0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1]\"\n",
        "434,\"Yorgan beklediÄŸimden Ã§ok daha hafif ve sÄ±cak tutuyor. Dokusu yumuÅŸacÄ±k. SatÄ±cÄ± Ã§ok hÄ±zlÄ± gÃ¶nderdi, kargo paketlemesi Ã§ok hijyenikti. FiyatÄ± diÄŸer markalara gÃ¶re Ã§ok daha uygun. Genel olarak Ã§ok memnun kaldÄ±m.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1]\"\n",
        "435,\"ÃœrÃ¼nÃ¼n pilleri iÃ§inden Ã§Ä±kmadÄ±, ayrÄ±ca kumandasÄ± Ã§alÄ±ÅŸmÄ±yor. SatÄ±cÄ±ya durumu bildirdim, yeni kumanda gÃ¶ndereceklerini sÃ¶ylediler. Kargo hÄ±zlÄ±ydÄ± ama kutu Ã§ok hÄ±rpalanmÄ±ÅŸtÄ±. Malzeme kalitesi orta ÅŸekerli.\",\"[0, 0, 0, 0, -1, 1, 1, 0, 0, 0, -1, -1, 0, -1]\"\n",
        "436,\"Kahve fincanlarÄ± Ã§ok ÅŸÄ±k ama iki tanesi kÄ±rÄ±k geldi. Paketleme daha Ã¶zenli olabilirdi. SatÄ±cÄ± kÄ±rÄ±k olanlarÄ±n yerine yenisini yolladÄ±. Rengi fotoÄŸraftakinden bir ton koyu. FiyatÄ± indirimdeyken Ã§ok iyiydi.\",\"[0, 0, 0, -1, 0, 1, 1, 0, 0, 1, 0, 0, 1, -1]\"\n",
        "437,\"SÄ±rt Ã§antasÄ± Ã§ok fonksiyonel, Ã§ok fazla cebi var. KumaÅŸÄ± su geÃ§irmiyor, test ettim. Kargo ertesi gÃ¼n kapÄ±mdaydÄ±. SatÄ±cÄ± Ã§ok profesyonel Ã§alÄ±ÅŸÄ±yor. FiyatÄ± kalitesine gÃ¶re bedava diyebilirim.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1]\"\n",
        "438,\"Avize Ã§ok gÃ¼zel duruyor ama montajÄ± imkansÄ±z gibi, parÃ§alar uymuyor. SatÄ±cÄ± montaj videosu gÃ¶nderdi ama yine de zorlandÄ±k. Kargo Ã§ok yavaÅŸ geldi. ÃœrÃ¼nÃ¼n cam kÄ±sÄ±mlarÄ± Ã§ok ince, hemen kÄ±rÄ±labilir gibi duruyor.\",\"[0, 0, 0, 0, -1, 0, 1, 0, 0, 1, -1, 0, 0, -1]\"\n",
        "439,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderildi, ben siyah istedim beyaz geldi. Ãœstelik gelen Ã¼rÃ¼nÃ¼n yÃ¼zeyi hep Ã§izik dolu. SatÄ±cÄ± hatasÄ±nÄ± telafi etmedi. Kargo poÅŸeti de aÃ§Ä±ktÄ±. HiÃ§ kimseye Ã¶nermiyorum, rezalet bir alÄ±ÅŸveriÅŸti.\",\"[-1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1]\"\n",
        "440,\"VantilatÃ¶rÃ¼n serinletmesi Ã§ok iyi ama sesi uÃ§ak motoru gibi Ã§Ä±kÄ±yor. Kurulumu 1 saatimi aldÄ±, parÃ§alar zor geÃ§iyor. SatÄ±cÄ± kargoyu Ã§ok hÄ±zlÄ± Ã§Ä±kardÄ±. FiyatÄ± uygun ama gece Ã§alÄ±ÅŸtÄ±rmak imkansÄ±z. GÃ¶rÃ¼nÃ¼ÅŸÃ¼ ise Ã§ok kaba.\",\"[0, 0, 0, 0, -1, 0, 1, 0, 1, -1, -1, 0, 1, 1]\"\n",
        "441,\"Elbisenin rengi efsane, tam fotoÄŸraftaki gibi canlÄ±. KumaÅŸÄ± yumuÅŸacÄ±k ve kalÄ±bÄ± tam oturdu. SatÄ±cÄ± yanÄ±na Ã§ok tatlÄ± bir toka eklemiÅŸ. Kargo da Ã§ok hÄ±zlÄ± geldi. Bu fiyata bÃ¶yle bir kalite beklemiyordum, harika.\",\"[0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "442,\"ÃœrÃ¼n maalesef eksik parÃ§ayla geldi, kumandasÄ± kutudan Ã§Ä±kmadÄ±. SatÄ±cÄ± Ã§ok ilgisiz, iade edin diyorlar sadece. Kargo paketi de yÄ±rtÄ±k pÄ±rtÄ±ktÄ±. Malzeme kalitesi Ã§ok basit plastik gibi. HiÃ§ memnun kalmadÄ±m.\",\"[0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1]\"\n",
        "443,\"KulaklÄ±ÄŸÄ±n ses kalitesi Ã§ok temiz ancak Bluetooth baÄŸlantÄ±sÄ± sÃ¼rekli kopuyor. TasarÄ±mÄ± Ã§ok ergonomik, kulaÄŸÄ±mÄ± hiÃ§ acÄ±tmadÄ±. Kargo hÄ±zÄ± muazzam. FiyatÄ± piyasaya gÃ¶re ucuz ama baÄŸlantÄ± sorunu can sÄ±kÄ±cÄ±.\",\"[0, 0, 0, 0, -1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "444,\"Yemek takÄ±mÄ± Ã§ok ÅŸÄ±k ama 3 tabaÄŸÄ± kÄ±rÄ±k geldi. SatÄ±cÄ±ya yazdÄ±m hemen yenilerini kargoladÄ±lar. Paketleme aslÄ±nda iyiydi ama kargo firmasÄ± Ã§ok hor davranmÄ±ÅŸ. ÃœrÃ¼nÃ¼n aÄŸÄ±rlÄ±ÄŸÄ± ve kalitesi Ã§ok yerinde.\",\"[0, 1, 0, -1, 0, 1, 1, 0, 0, 1, 0, 0, 0, -1]\"\n",
        "445,\"Botlar Ã§ok aÄŸÄ±r, yÃ¼rÃ¼rken yoruyor ve ayaÄŸÄ±mÄ± arkadan vurdu. Ä°Ã§ astarÄ± hemen yÄ±rtÄ±ldÄ±, hiÃ§ dayanÄ±klÄ± deÄŸil. SatÄ±cÄ± Ã§ok nazik olsa da Ã¼rÃ¼n sÄ±nÄ±fta kaldÄ±. Kargo da beklediÄŸimden uzun sÃ¼rdÃ¼. VerdiÄŸim paraya yazÄ±k.\",\"[0, -1, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1]\"\n",
        "446,\"Organizer Ã§ok kullanÄ±ÅŸlÄ±, dolabÄ±mÄ± tertemiz yaptÄ±. BoyutlarÄ± tam belirtildiÄŸi gibi. Paketleme o kadar saÄŸlamdÄ± ki zor aÃ§tÄ±m. SatÄ±cÄ±ya hÄ±zlÄ± gÃ¶nderim iÃ§in teÅŸekkÃ¼rler. Malzemesi de oldukÃ§a sert ve saÄŸlam.\",\"[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1]\"\n",
        "447,\"YanlÄ±ÅŸ Ã¼rÃ¼n gÃ¶nderilmiÅŸ, ben Ã§ay makinesi istedim kettle geldi. Ãœstelik kireÃ§li gibi duruyordu, kullanÄ±lmÄ±ÅŸ olabilir. SatÄ±cÄ± deÄŸiÅŸim yapmadÄ±. Kargo kutusu da su iÃ§inde kalmÄ±ÅŸtÄ±. Tam bir fiyasko.\",\"[-1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1]\"\n",
        "448,\"ParfÃ¼mÃ¼n kokusu ÅŸahane ama kalÄ±cÄ±lÄ±ÄŸÄ± 1 saat bile deÄŸil. ÅžiÅŸe tasarÄ±mÄ± Ã§ok lÃ¼ks duruyor. SatÄ±cÄ± Ã§ok Ã¶zenli paketlemiÅŸ. FiyatÄ± indirimdeyken Ã§ok uygundu. Yine de beklentimin altÄ±nda bir performans sergiledi.\",\"[0, 0, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, 1, 1]\"\n",
        "449,\"Matkap seti Ã§ok eksiksiz, her uÃ§ var iÃ§inde. KullanÄ±mÄ± Ã§ok kolay, aÄŸÄ±rlÄ±ÄŸÄ± dengeli. Kargo hÄ±zÄ± inanÄ±lmazdÄ±. SatÄ±cÄ± Ã§ok dÃ¼rÃ¼st ve yardÄ±msever. FiyatÄ± bu set iÃ§eriÄŸine gÃ¶re Ã§ok ama Ã§ok avantajlÄ±.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1]\"\n",
        "450,\"Gelen gÃ¶mlek aÅŸÄ±rÄ± dar, XL aldÄ±m ama S gibi. KumaÅŸÄ± da naylon gibi, yakÄ±yor. SatÄ±cÄ±ya iade iÃ§in yazdÄ±m, kargo Ã¼cretini benim Ã¶dememi istediler. Paketleme Ã§ok Ã¶zensizdi. FiyatÄ± ucuz ama Ã§Ã¶p oldu.\",\"[0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0]\"\n",
        "451,\"Tost makinesi Ã§ok pratik, plakalarÄ± makinede yÄ±kanabiliyor. Rengi mutfaÄŸÄ±ma Ã§ok yakÄ±ÅŸtÄ±. Kargo ertesi gÃ¼n geldi. SatÄ±cÄ± Ã§ok profesyonel. Malzeme kalitesi de oldukÃ§a yÃ¼ksek hissettiriyor. Tavsiye ederim.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]\"\n",
        "452,\"ÃœrÃ¼nÃ¼n camÄ± tamamen tuz buz olmuÅŸ ÅŸekilde geldi. SatÄ±cÄ±ya ulaÅŸmak mÃ¼mkÃ¼n deÄŸil, telefonlarÄ± kapalÄ±. Ä°ade sÃ¼reci Ã§ok karmaÅŸÄ±k. GÃ¶rÃ¼nÃ¼ÅŸÃ¼ gÃ¼zeldir diye almÄ±ÅŸtÄ±m ama piÅŸman oldum. Kargo rezaletti.\",\"[0, 0, 0, -1, 0, -1, -1, 0, 0, 1, 0, 0, 0, -1]\"\n",
        "453,\"Oyuncu koltuÄŸu Ã§ok rahat, bel desteÄŸi Ã§ok iyi. Kurulumu biraz zorladÄ± ama kÄ±lavuz aÃ§Ä±klayÄ±cÄ±ydÄ±. Paketleme Ã§ok Ã¶zenli yapÄ±lmÄ±ÅŸtÄ±. SatÄ±cÄ±ya ilgisinden dolayÄ± teÅŸekkÃ¼rler. FiyatÄ± yÃ¼ksek ama konforuna deÄŸer.\",\"[0, 1, 1, 0, -1, 0, 1, 0, 1, 0, 0, 0, -1, 1]\"\n",
        "454,\"CÃ¼zdanÄ±n derisi Ã§ok kalitesiz, hemen soyulmaya baÅŸladÄ±. FermuarÄ± da takÄ±lÄ±yor, dÃ¼zgÃ¼n aÃ§Ä±lmÄ±yor. SatÄ±cÄ± hatalÄ± Ã¼rÃ¼n olduÄŸunu kabul etmedi. Kargo Ã§ok geÃ§ geldi. Bu fiyata pazardan alsam daha iyiydi.\",\"[0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1]\"\n",
        "455,\"Krem Ã§ok hÄ±zlÄ± emiliyor ve kokusu rahatsÄ±z etmiyor. Paketlemesi Ã§ok hijyenik ve ÅŸÄ±ktÄ±. SatÄ±cÄ± bir sÃ¼rÃ¼ numune gÃ¶ndermiÅŸ. Kargo hÄ±zÄ± beklentimin Ã¼zerindeydi. FiyatÄ± da kalitesine gÃ¶re gayet makul.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "456,\"Masa lambasÄ± Ã§ok ÅŸÄ±k ama kablosu Ã§ok kÄ±sa, her yere uzanmÄ±yor. Boyutu da gÃ¶rsele gÃ¶re kÃ¼Ã§Ã¼k. SatÄ±cÄ± kargoyu geÃ§ Ã§Ä±kardÄ±. Paketleme ise Ã§ok zayÄ±ftÄ±, kutusu ezilmiÅŸti. Yine de Ä±ÅŸÄ±ÄŸÄ± gÃ¼zel veriyor.\",\"[0, 0, -1, 0, -1, 0, -1, 1, 0, 1, 0, 0, 0, -1]\"\n",
        "457,\"Nevresim takÄ±mÄ± tam bir hayal kÄ±rÄ±klÄ±ÄŸÄ±, rengi yÄ±kamada aktÄ±. Dokusu Ã§ok sert, hiÃ§ konforlu deÄŸil. SatÄ±cÄ± iadeyi kabul etmedi. Kargo paketi de yÄ±rtÄ±lmÄ±ÅŸtÄ±. ParasÄ±na gÃ¶re Ã§ok dÃ¼ÅŸÃ¼k bir kalitede.\",\"[0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1]\"\n",
        "458,\"Porselen kupa seti harika, renkleri Ã§ok canlÄ±. Paketleme o kadar iyiydi ki kÄ±rÄ±lmasÄ± imkansÄ±zdÄ±. SatÄ±cÄ± Ã§ok nazik, iÃ§ine kÃ¼Ã§Ã¼k bir not bÄ±rakmÄ±ÅŸ. Kargo Ã§ok hÄ±zlÄ± ulaÅŸtÄ±. FiyatÄ± da oldukÃ§a uygun.\",\"[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "459,\"SaÃ§ kurutma makinesi aÅŸÄ±rÄ± gÃ¼rÃ¼ltÃ¼lÃ¼ ve plastik kokuyor. IsÄ±sÄ± da yeterli deÄŸil, Ã§ok geÃ§ kurutuyor. SatÄ±cÄ± Ã§ok kaba cevaplar verdi. Kargo kutusu Ä±slak geldi. HiÃ§ memnun kalmadÄ±m, iade edeceÄŸim.\",\"[0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1]\"\n",
        "460,\"Bluetooth hoparlÃ¶rÃ¼n sesi Ã§ok yÃ¼ksek ve kaliteli. ÅžarjÄ± da uzun sÃ¼re gidiyor. SatÄ±cÄ± Ã§ok hÄ±zlÄ± kargoladÄ±. Paketleme Ã§ok saÄŸlamdÄ±. FiyatÄ± performansÄ±na gÃ¶re Ã§ok iyi, kesinlikle tavsiye ederim.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1]\"\n",
        "461,\"ÃœrÃ¼nÃ¼n rengi fotoÄŸraftakinden Ã§ok farklÄ±, resmen baÅŸka renk yollamÄ±ÅŸlar. AyrÄ±ca parÃ§alarÄ± eksikti. SatÄ±cÄ± asla mesajlara dÃ¶nmÃ¼yor. Kargo hÄ±zÄ± Ã§ok yavaÅŸtÄ±. VerdiÄŸim para boÅŸa gitti, rezalet.\",\"[1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1]\"\n",
        "462,\"SÄ±rt Ã§antasÄ± Ã§ok dayanÄ±klÄ± gÃ¶rÃ¼nÃ¼yor, dikiÅŸleri Ã§ok saÄŸlam. Ä°Ã§ hacmi de oldukÃ§a geniÅŸ. SatÄ±cÄ± Ã§ok Ã¶zenli paketlemiÅŸti. Kargo ertesi gÃ¼n kapÄ±mdaydÄ±. FiyatÄ± kalitesiyle kÄ±yaslanÄ±nca Ã§ok ucuz kalÄ±yor.\",\"[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\"\n",
        "463,\"ÃœtÃ¼ Ã§ok geÃ§ iniyor ve su akÄ±tÄ±yor. Kargo paketi Ã§ok kÃ¶tÃ¼ydÃ¼, Ã¼rÃ¼nÃ¼n kutusu parÃ§alanmÄ±ÅŸtÄ±. SatÄ±cÄ± iade sÃ¼recinde Ã§ok zorluk Ã§Ä±kardÄ±. GÃ¶rÃ¼nÃ¼ÅŸÃ¼ gÃ¼zel olsa da performansÄ± Ã§ok kÃ¶tÃ¼. Parama yazÄ±k oldu.\",\"[0, -1, 0, -1, -1, -1, -1, 0, -1, 1, 0, 0, -1, -1]\"\n",
        "464,\"KitaplÄ±k Ã§ok saÄŸlam ve kurulumu Ã§ok basit. TasarÄ±mÄ± odama tam uydu. SatÄ±cÄ± kargoyu Ã§ok hÄ±zlÄ± gÃ¶nderdi. Paketleme kusursuzdu. FiyatÄ± da piyasa ortalamasÄ±nÄ±n altÄ±nda, Ã§ok memnun kaldÄ±m.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\"\n",
        "465,\"KulaklÄ±ÄŸÄ±n sÃ¼ngerleri Ã§ok sert, kulaÄŸÄ±mÄ± acÄ±ttÄ±. Ses kalitesi de boÄŸuk geliyor. SatÄ±cÄ± deÄŸiÅŸim yapmadÄ±. Kargo 1 haftada ancak geldi. Paketleme Ã§ok basitti, sadece bir poÅŸete sarÄ±p gÃ¶ndermiÅŸler.\",\"[0, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1]\"\n",
        "466,\"AkÄ±llÄ± saat Ã§ok fonksiyonel ve ÅŸarjÄ± 1 hafta gidiyor. Rengi ve duruÅŸu Ã§ok kaliteli. SatÄ±cÄ± Ã§ok ilgili, her soruma cevap verdi. Kargo kutusu tertemizdi. Fiyat/performans Ã¼rÃ¼nÃ¼ diyebilirim.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "467,\"ÃœrÃ¼nÃ¼n ayaÄŸÄ± sallanÄ±yor, Ã§ok dengesiz. AyrÄ±ca yÃ¼zeyinde derin Ã§izikler vardÄ±. SatÄ±cÄ± Ã¼rÃ¼nÃ¼n arkasÄ±nda durmuyor. Kargo Ã§ok geÃ§ geldi. Malzemesi Ã§ok kalitesiz bir suntadan yapÄ±lmÄ±ÅŸ. DeÄŸmez.\",\"[0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1]\"\n",
        "468,\"Bebek arabasÄ± Ã§ok hafif ve sÃ¼rÃ¼ÅŸÃ¼ Ã§ok rahat. KatlanmasÄ± da Ã§ok pratik. SatÄ±cÄ± hediye olarak yaÄŸmurluk gÃ¶ndermiÅŸ. Paketleme Ã§ok Ã¶zenliydi. FiyatÄ± pahalÄ± ama konforu iÃ§in alÄ±nÄ±r.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, -1, 1]\"\n",
        "469,\"AyakkabÄ±nÄ±n baÄŸcÄ±ÄŸÄ± ilk Ã§ekmede koptu, Ã§ok kalitesiz. Rengi fotoÄŸraftaki gibi deÄŸil, Ã§ok soluk. SatÄ±cÄ± ilgisiz, iadeyi kabul etmediler. Kargo paketi aÃ§Ä±k geldi. ParanÄ±zÄ± Ã§Ã¶pe atmayÄ±n.\",\"[0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1]\"\n",
        "470,\"YÃ¼z temizleme cihazÄ± Ã§ok pratik, cildimi yumuÅŸacÄ±k yaptÄ±. TasarÄ±mÄ± Ã§ok ergonomik ve rengi Ã§ok ÅŸÄ±k. SatÄ±cÄ± Ã§ok hÄ±zlÄ± kargoladÄ±, paketleme Ã§ok gÃ¼venliydi. FiyatÄ± biraz yÃ¼ksek ama kalitesine deÄŸer.\",\"[0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, -1, 1]\"\n",
        "471,\"ÃœrÃ¼n yanlÄ±ÅŸ model geldi, Ã¼stelik ekranÄ± Ã§izik iÃ§indeydi. SatÄ±cÄ± iade talebimi reddetti, Ã§ok kaba davrandÄ±lar. Kargo kutusu da paramparÃ§aydÄ±. Malzeme kalitesi tam bir fiyasko, kesinlikle uzak durun.\",\"[-1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1]\"\n",
        "472,\"Kahve fincanlarÄ± Ã§ok zarif ama beklediÄŸimden Ã§ok kÃ¼Ã§Ã¼k geldi. Paketleme o kadar iyiydi ki kÄ±rÄ±lmasÄ± imkansÄ±zdÄ±. SatÄ±cÄ± Ã§ok ilgili, hemen fatura gÃ¶nderdi. FiyatÄ± indirimdeyken Ã§ok makuldÃ¼. Kargo hÄ±zÄ± iyi.\",\"[0, 1, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "473,\"SÃ¼pÃ¼rgenin Ã§ekiÅŸ gÃ¼cÃ¼ baÅŸta iyiydi ama bir haftada azaldÄ±, Ã§ok Ã§abuk bozuldu. SatÄ±cÄ± servis numarasÄ± verip baÅŸÄ±ndan savdÄ±. Kargo poÅŸeti Ã§ok kirliydi. ParÃ§alarÄ± Ã§ok dayanÄ±ksÄ±z plastik. Parama yazÄ±k.\",\"[0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1]\"\n",
        "474,\"Ã‡adÄ±rÄ±n kurulumu Ã§ok kolay, 5 dakikada bitti. KumaÅŸÄ± su geÃ§irmiyor, test ettik. SatÄ±cÄ± kargoyu ertesi gÃ¼n teslim etti. Paketleme Ã§ok saÄŸlamdÄ±. FiyatÄ± performansÄ±na gÃ¶re bedava sayÄ±lÄ±r. Ã‡ok memnunum.\",\"[0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1]\"\n",
        "475,\"Gelen pantolonun boyu Ã§ok kÄ±sa, kalÄ±bÄ± da aÅŸÄ±rÄ± dar. KumaÅŸÄ± Ã§ok sert, rahatsÄ±z ediyor. SatÄ±cÄ± iade kargosunu bana Ã¶detti. Paketleme Ã§ok Ã¶zensizdi, Ã¼rÃ¼n kÄ±rÄ±ÅŸÄ±k geldi. Bu fiyata daha iyisi bulunur.\",\"[0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0]\"\n",
        "476,\"Mikser Ã§ok gÃ¼Ã§lÃ¼ Ã§alÄ±ÅŸÄ±yor ve tÃ¼m uÃ§larÄ± eksiksiz geldi. Rengi mutfaÄŸÄ±ma tam uydu. SatÄ±cÄ± Ã§ok Ã¶zenli paketlemiÅŸ, kat kat sarmÄ±ÅŸ. Kargo hÄ±zÄ± ÅŸaÅŸÄ±rtÄ±cÄ± derecede iyiydi. FiyatÄ± da Ã§ok cazip.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]\"\n",
        "477,\"Bluetooth hoparlÃ¶rÃ¼n ÅŸarj giriÅŸi bozuk Ã§Ä±ktÄ±, ÅŸarj olmuyor. SatÄ±cÄ± deÄŸiÅŸim yapmadÄ±, iadeyle uÄŸraÅŸtÄ±rÄ±yorlar. Kargo Ã§ok geÃ§ geldi. Malzemesi Ã§ok basit bir plastikten yapÄ±lmÄ±ÅŸ. Ses kalitesi de boÄŸuk.\",\"[0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1]\"\n",
        "478,\"Nevresimlerin deseni harika, odanÄ±n havasÄ±nÄ± deÄŸiÅŸtirdi. Dokusu Ã§ok yumuÅŸak ve kaliteli. SatÄ±cÄ± kÃ¼Ã§Ã¼k bir hediye eklemiÅŸ, Ã§ok nazikler. Kargo kutusu Ã§ok temizdi. FiyatÄ± kalitesine gÃ¶re Ã§ok uygun.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "479,\"ÃœrÃ¼nÃ¼n parÃ§alarÄ± birbirine uymuyor, montajÄ± imkansÄ±z. AyrÄ±ca metal kÄ±sÄ±mlarÄ± paslanmÄ±ÅŸ gibiydi. SatÄ±cÄ± telefonlarÄ± aÃ§mÄ±yor. Kargo paketi sÄ±rÄ±lsÄ±klamdÄ±. HiÃ§ memnun kalmadÄ±m, paranÄ±za yazÄ±k etmeyin.\",\"[0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1]\"\n",
        "480,\"Tava seti Ã§ok hafif ve saplarÄ± hiÃ§ Ä±sÄ±nmÄ±yor. BoyutlarÄ± ideal. SatÄ±cÄ± Ã§ok hÄ±zlÄ± gÃ¶nderdi. Paketleme kusursuzdu, hiÃ§ Ã§izik yoktu. FiyatÄ± piyasaya gÃ¶re Ã§ok ucuz. Herkese tavsiye ediyorum.\",\"[0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1]\"\n",
        "481,\"Gelen gÃ¶mlek resmen yÄ±rtÄ±k Ã§Ä±ktÄ±, dikiÅŸleri sÃ¶kÃ¼lmÃ¼ÅŸ. SatÄ±cÄ± hatasÄ±nÄ± telafi etmedi. Kargo Ã§ok yavaÅŸtÄ±, 2 haftada geldi. Paketleme Ã§ok kÃ¶tÃ¼ydÃ¼, sadece bir kaÄŸÄ±da sarmÄ±ÅŸlar. FiyatÄ± ucuz ama Ã§Ã¶p.\",\"[0, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1]\"\n",
        "482,\"Kitaplar Ã§ok temiz geldi, baskÄ± kalitesi ÅŸahane. SatÄ±cÄ± her birini ayrÄ± ayrÄ± paketlemiÅŸ, kÃ¶ÅŸeleri ezilmemiÅŸ. Kargo hÄ±zÄ± mÃ¼kemmeldi. FiyatÄ± set olarak Ã§ok ekonomik. Ã‡ok dÃ¼rÃ¼st bir satÄ±cÄ±.\",\"[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "483,\"AkÄ±llÄ± saatin ekranÄ± hemen karardÄ±, dokunmatiÄŸi de Ã§alÄ±ÅŸmÄ±yor. SatÄ±cÄ± servis fiÅŸi olmadan iade almam diyor. Kargo kutusu ezik geldi. Malzeme kalitesi Ã§ok ucuz duruyor. Beklentimin Ã§ok altÄ±nda.\",\"[0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1]\"\n",
        "484,\"Vakum makinesi Ã§ok iÅŸime yaradÄ±, kullanÄ±mÄ± Ã§ok basit. Rengi ve tasarÄ±mÄ± Ã§ok modern. SatÄ±cÄ± kargoyu aynÄ± gÃ¼n Ã§Ä±kardÄ±. Paketleme Ã§ok titizdi. FiyatÄ± pahalÄ± ama performansÄ±na deÄŸer.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, -1, 1]\"\n",
        "485,\"ÃœrÃ¼nÃ¼n rengi fotoÄŸraftakinden tamamen farklÄ±, Ã§ok Ã§irkin bir ton geldi. AyrÄ±ca kutusu aÃ§Ä±ktÄ±, sanki kullanÄ±lmÄ±ÅŸ. SatÄ±cÄ± mesajlara geÃ§ dÃ¶nÃ¼yor. Kargo paketi yÄ±rtÄ±ktÄ±. ParasÄ±na deÄŸmez.\",\"[-1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1]\"\n",
        "486,\"SÄ±rt Ã§antasÄ± Ã§ok dayanÄ±klÄ±, kumaÅŸÄ± Ã§ok kalÄ±n. Ä°Ã§ bÃ¶lmeleri Ã§ok kullanÄ±ÅŸlÄ±. SatÄ±cÄ± Ã§ok hÄ±zlÄ± gÃ¶nderdi, paketleme Ã§ok Ã¶zenliydi. FiyatÄ± bu kaliteye gÃ¶re Ã§ok dÃ¼ÅŸÃ¼k kalmÄ±ÅŸ. Ã‡ok memnun kaldÄ±m.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1]\"\n",
        "487,\"FÃ¶n makinesi aÅŸÄ±rÄ± gÃ¼rÃ¼ltÃ¼lÃ¼ ve kablosu Ã§ok kÄ±sa. AyrÄ±ca plastik kokusu tÃ¼m odayÄ± sardÄ±. SatÄ±cÄ± iadeyi kabul etmedi. Kargo Ã§ok geÃ§ geldi. Malzeme kalitesi Ã§ok zayÄ±f, gÃ¼ven vermiyor.\",\"[0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1]\"\n",
        "488,\"Nevresim takÄ±mÄ± yumuÅŸacÄ±k, yÄ±kadÄ±ktan sonra daha da gÃ¼zelleÅŸti. Renkleri Ã§ok canlÄ±. SatÄ±cÄ± Ã§ok ilgiliydi, her aÅŸamada bilgilendirdi. Paketleme harikaydÄ±. FiyatÄ± kesinlikle hak ediyor.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "489,\"ÃœrÃ¼nÃ¼n ayaklarÄ± eksik geldi, masayÄ± kuramÄ±yorum. SatÄ±cÄ± Ã§ok lakayÄ±t bir tavÄ±rla cevap verdi. Kargo kutusu su iÃ§indeydi. Malzemesi Ã§ok adi bir suntadan yapÄ±lmÄ±ÅŸ, hemen dÃ¶kÃ¼lÃ¼yor. Berbat.\",\"[0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1]\"\n",
        "490,\"Oyuncu faresi Ã§ok hÄ±zlÄ± ve tepkime sÃ¼resi mÃ¼kemmel. TasarÄ±mÄ± Ã§ok ergonomik. SatÄ±cÄ± kargoyu Ã§ok hÄ±zlÄ± ulaÅŸtÄ±rdÄ±. Paketleme Ã§ok gÃ¼venliydi. Fiyat/performans aÃ§Ä±sÄ±ndan piyasanÄ±n en iyisi.\",\"[0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "491,\"Gelen ayakkabÄ±nÄ±n teki farklÄ± numara, Ã¶teki farklÄ±. Resmen dalga geÃ§iyorlar. SatÄ±cÄ± hatasÄ±nÄ± kabul etmedi, iadeyle uÄŸraÅŸtÄ±rÄ±yorlar. Kargo paketi Ã§ok pisti. ÃœrÃ¼n kalitesi de Ã§ok kÃ¶tÃ¼, plastik kokuyor.\",\"[-1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0]\"\n",
        "492,\"Mutfak tartÄ±sÄ± Ã§ok hassas ve kullanÄ±mÄ± Ã§ok kolay. Rengi mutfaÄŸÄ±ma Ã§ok yakÄ±ÅŸtÄ±. SatÄ±cÄ± Ã¼rÃ¼nÃ¼ Ã§ok gÃ¼zel paketlemiÅŸ, pilleri de iÃ§indeydi. Kargo ertesi gÃ¼n geldi. FiyatÄ± da gayet uygun.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]\"\n",
        "493,\"Termos sÄ±zdÄ±rÄ±yor, her yer Ã§ay oldu. SatÄ±cÄ±ya durumu anlattÄ±m, kullanÄ±cÄ± hatasÄ± dediler. Kargo kutusu paramparÃ§a geldi. Malzeme kalitesi Ã§ok kalitesiz plastik. Kesinlikle almayÄ±n, paranÄ±zla rezil olmayÄ±n.\",\"[0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1]\"\n",
        "494,\"Ã‡antanÄ±n deri kalitesi Ã§ok yÃ¼ksek, dikiÅŸleri Ã§ok muntazam. Rengi tam fotoÄŸraftaki gibi. SatÄ±cÄ± yanÄ±na bir de anahtarlÄ±k eklemiÅŸ. Kargo hÄ±zÄ± ÅŸahaneydi. FiyatÄ± biraz tuzlu ama deÄŸer.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, -1, 1]\"\n",
        "495,\"ÃœrÃ¼nÃ¼n parÃ§alarÄ± Ã§izik iÃ§inde geldi, boyasÄ± dÃ¶kÃ¼lmÃ¼ÅŸtÃ¼. SatÄ±cÄ± deÄŸiÅŸim yapamayÄ±z diyor. Kargo Ã§ok geÃ§ ulaÅŸtÄ± ve kutu hÄ±rpalanmÄ±ÅŸtÄ±. Malzemesi Ã§ok dayanÄ±ksÄ±z gÃ¶rÃ¼nÃ¼yor. HiÃ§ beÄŸenmedim.\",\"[0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1]\"\n",
        "496,\"KulaklÄ±k pedleri Ã§ok yumuÅŸak, uzun sÃ¼reli kullanÄ±mda rahatsÄ±z etmiyor. Ses kalitesi harika. SatÄ±cÄ± kargoyu Ã§ok hÄ±zlÄ± Ã§Ä±kardÄ±. Paketleme Ã§ok Ã¶zenliydi. FiyatÄ± performansÄ±na gÃ¶re Ã§ok iyi.\",\"[0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1]\"\n",
        "497,\"Gelen elbise lekeliydi, sanki daha Ã¶nce giyilmiÅŸ gibi. SatÄ±cÄ± Ã§ok ilgisiz davrandÄ±. Kargo kutusu aÃ§Ä±ktÄ±. ÃœrÃ¼nÃ¼n rengi de fotoÄŸraftakinden Ã§ok daha soluk. Parama yazÄ±k oldu, iade ediyorum.\",\"[-1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1]\"\n",
        "498,\"Robot sÃ¼pÃ¼rge Ã§ok baÅŸarÄ±lÄ±, her yeri pÄ±rÄ±l pÄ±rÄ±l yaptÄ±. Uygulama kurulumu Ã§ok basitti. SatÄ±cÄ± Ã§ok ilgiliydi, sorularÄ±ma hemen yanÄ±t verdi. Kargo Ã§ok hÄ±zlÄ±ydÄ±. FiyatÄ± kesinlikle hak ediyor.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "499,\"YanlÄ±ÅŸ renk gÃ¶nderildi, iade etmek istediÄŸimde ise satÄ±cÄ± Ã§ok kaba bir Ã¼slupla reddetti. Kargo poÅŸeti paramparÃ§aydÄ±. ÃœrÃ¼n kalitesi Ã§ok dÃ¼ÅŸÃ¼k, dikiÅŸleri hemen sÃ¶kÃ¼ldÃ¼. Tam bir piÅŸmanlÄ±k.\",\"[1, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1]\"\n",
        "500,\"ÃœtÃ¼ masasÄ± Ã§ok dengesiz, ayaÄŸÄ± kÄ±sa kalÄ±yor. AyrÄ±ca kumaÅŸ kÄ±lÄ±fÄ± lekeli geldi. SatÄ±cÄ±ya yazdÄ±m ama Ã§ok kaba bir cevap aldÄ±m. Kargo da iki haftada ancak ulaÅŸtÄ±. FiyatÄ± ucuz ama kesinlikle kaliteli deÄŸil.\",\"[0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 1, -1]\"\n",
        "501,\"Porselen yemek takÄ±mÄ± tek kelimeyle kusursuz. Paketleme o kadar profesyoneldi ki hiÃ§bir parÃ§a zarar gÃ¶rmemiÅŸ. SatÄ±cÄ±nÄ±n notu Ã§ok nazikti. Kargo hÄ±zÄ± muazzam. FiyatÄ± biraz yÃ¼ksek ama duruÅŸu Ã§ok lÃ¼ks.\",\"[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, -1, 1]\"\n",
        "502,\"Gelen kulaklÄ±ÄŸÄ±n saÄŸ tarafÄ± Ã§alÄ±ÅŸmÄ±yor, ses Ã§ok cÄ±zÄ±rtÄ±lÄ±. SatÄ±cÄ± iade kabul etmiyor, yetkili servise gidin diyorlar. Kargo paketi de ezilmiÅŸti. Malzemesi Ã§ok basit bir plastik. ParanÄ±za yazÄ±k.\",\"[0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1]\"\n",
        "503,\"Termos Ã§antamÄ±n boyutu tam istediÄŸim gibi, iÃ§ hacmi Ã§ok geniÅŸ. KumaÅŸÄ± su geÃ§irmiyor. SatÄ±cÄ± ertesi gÃ¼n kargoya verdi. Paketleme Ã§ok hijyenikti. FiyatÄ± piyasadaki diÄŸer markalara gÃ¶re Ã§ok daha makul.\",\"[0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1]\"\n",
        "504,\"Gelen Ã¼rÃ¼nÃ¼n rengi ilandakinden tamamen farklÄ±, Ã§ok kalitesiz duruyor. ParÃ§alarÄ± eksik olduÄŸu iÃ§in kuramadÄ±m bile. SatÄ±cÄ± mesajlara dÃ¶nmÃ¼yor. Kargo hÄ±zÄ± berbat. Tam bir hayal kÄ±rÄ±klÄ±ÄŸÄ± oldu benim iÃ§in.\",\"[1, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1]\"\n",
        "505,\"Botlar Ã§ok ÅŸÄ±k ve su geÃ§irmiyor, tam kÄ±ÅŸlÄ±k. KalÄ±bÄ± tam oldu. SatÄ±cÄ± Ã§ok Ã¶zenli paketlemiÅŸ, kutusu bile tertemizdi. Kargo hÄ±zÄ± ÅŸaÅŸÄ±rtÄ±cÄ±. FiyatÄ± kalitesine gÃ¶re gerÃ§ekten Ã§ok iyi, tavsiye ederim.\",\"[0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "506,\"ÃœrÃ¼n maalesef kÄ±rÄ±k ulaÅŸtÄ±, her yeri Ã§atlak iÃ§indeydi. SatÄ±cÄ± iade sÃ¼recinde Ã§ok zorluk Ã§Ä±kardÄ±, suÃ§ kargonun diyorlar. Kargo paketi de yÄ±rtÄ±ktÄ±. Malzeme kalitesi hiÃ§ gÃ¼ven vermiyor, Ã§ok adi.\",\"[0, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1]\"\n",
        "507,\"Blender seti Ã§ok gÃ¼Ã§lÃ¼, buzlarÄ± bile saniyeler iÃ§inde kÄ±rÄ±yor. Rengi mutfaÄŸÄ±ma Ã§ok yakÄ±ÅŸtÄ±. SatÄ±cÄ± kargoyu Ã§ok hÄ±zlÄ± Ã§Ä±kardÄ±. Paketleme mÃ¼kemmeldi. FiyatÄ± performansÄ±na gÃ¶re Ã§ok baÅŸarÄ±lÄ± bir Ã¼rÃ¼n.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "508,\"Gelen gÃ¶mleÄŸin dikiÅŸleri ilk giymede sÃ¶kÃ¼ldÃ¼, dÃ¼ÄŸmeleri de dÃ¼ÅŸÃ¼yor. KumaÅŸÄ± fotoÄŸraftakinden Ã§ok daha ince. SatÄ±cÄ± iadeyi reddetti. Kargo poÅŸeti Ä±slanmÄ±ÅŸtÄ±. Bu fiyata kesinlikle deÄŸmeyecek bir kalite.\",\"[0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1]\"\n",
        "509,\"Nevresim takÄ±mÄ± yumuÅŸacÄ±k, renkleri Ã§ok canlÄ±. SatÄ±cÄ± Ã§ok hÄ±zlÄ± gÃ¶nderdi ve paketleme Ã§ok saÄŸlamdÄ±. FaturayÄ± da iÃ§ine eklemiÅŸler. FiyatÄ± kalitesiyle kÄ±yaslanÄ±nca Ã§ok cazip. Ã‡ok memnun kaldÄ±m.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "510,\"ÃœrÃ¼nÃ¼n parÃ§alarÄ± eksik Ã§Ä±ktÄ±, kuramadÄ±m. AyrÄ±ca her yerinde Ã§izikler vardÄ±. SatÄ±cÄ± deÄŸiÅŸim yapmadÄ±, iadeyle uÄŸraÅŸtÄ±rÄ±yorlar. Kargo teslimatÄ± da bir haftayÄ± geÃ§ti. Malzemesi Ã§ok dandik plastik.\",\"[0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1]\"\n",
        "511,\"AkÄ±llÄ± saat beklediÄŸimden Ã§ok daha fonksiyonel, ÅŸarjÄ± da 5 gÃ¼n gidiyor. TasarÄ±mÄ± Ã§ok modern. SatÄ±cÄ± Ã§ok ilgiliydi. Kargo paketi Ã§ok Ã¶zenliydi. Fiyat/performans aÃ§Ä±sÄ±ndan kesinlikle alÄ±nmalÄ±.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "512,\"AyakkabÄ± numaramÄ± aldÄ±m ama Ã§ok kÃ¼Ã§Ã¼k geldi, kalÄ±bÄ± hatalÄ±. Ãœstelik Ã§ok aÄŸÄ±r kokuyor, havalandÄ±rmama raÄŸmen geÃ§medi. SatÄ±cÄ± iade kargo Ã¼cretini bana yÄ±ktÄ±. Paketleme rezaletti. Parama yazÄ±k.\",\"[0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0]\"\n",
        "513,\"Tava seti Ã§ok hafif, kullanÄ±mÄ± Ã§ok rahat. YÃ¼zeyi yapÄ±ÅŸma yapmÄ±yor. SatÄ±cÄ± kargoyu ertesi gÃ¼n ulaÅŸtÄ±rdÄ±. Paketleme kusursuzdu. FiyatÄ± piyasa deÄŸerinin altÄ±nda. Herkese Ã¶neririm, harika bir set.\",\"[0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1]\"\n",
        "514,\"Gelen Ã¼rÃ¼n resmen kullanÄ±lmÄ±ÅŸ, Ã¼zerinde parmak izleri vardÄ±. AyrÄ±ca kablosu eksikti. SatÄ±cÄ± hatasÄ±nÄ± telafi etmedi. Kargo kutusu Ã§ok hÄ±rpalanmÄ±ÅŸtÄ±. Malzeme kalitesi Ã§ok ucuz ve basit duruyor.\",\"[-1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1]\"\n",
        "515,\"Krem cildimi anÄ±nda nemlendirdi ve kokusu Ã§ok hafif. AmbalajÄ± Ã§ok lÃ¼ks duruyor. SatÄ±cÄ± yanÄ±na bir sÃ¼rÃ¼ tester eklemiÅŸ. Kargo Ã§ok hÄ±zlÄ±ydÄ±. FiyatÄ± kalitesine gÃ¶re Ã§ok uygun. Tekrar sipariÅŸ vereceÄŸim.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "516,\"Matkap seti Ã§ok kalitesiz, motoru ilk kullanÄ±mda yandÄ±. AyrÄ±ca Ã§antasÄ± kÄ±rÄ±ktÄ±. SatÄ±cÄ± servis sÃ¼reciyle uÄŸraÅŸtÄ±rÄ±yor. Kargo da Ã§ok geÃ§ geldi. Bu paraya daha kaliteli bir marka alÄ±nabilirdi.\",\"[0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1]\"\n",
        "517,\"KitaplÄ±k Ã§ok saÄŸlam ve kurulum ÅŸemasÄ± Ã§ok aÃ§Ä±klayÄ±cÄ±. TasarÄ±mÄ± Ã§ok estetik. SatÄ±cÄ± kargoyu aynÄ± gÃ¼n Ã§Ä±kardÄ±. Paketleme o kadar iyiydi ki aÃ§arken zorlandÄ±m. FiyatÄ± kalitesine deÄŸer.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, -1, 1]\"\n",
        "518,\"VantilatÃ¶r Ã§ok gÃ¼rÃ¼ltÃ¼lÃ¼ Ã§alÄ±ÅŸÄ±yor, gece uyutmaz. Boyutu da gÃ¶rsele gÃ¶re Ã§ok kÃ¼Ã§Ã¼k. SatÄ±cÄ± kargo iÃ§in Ã§ok bekletti. Paketleme Ã§ok zayÄ±ftÄ±, kutu daÄŸÄ±lmÄ±ÅŸ gelmiÅŸ. PerformansÄ± kesinlikle yetersiz.\",\"[0, -1, -1, 0, -1, 0, -1, 1, -1, 0, 0, 0, 0, -1]\"\n",
        "519,\"SÄ±rt Ã§antasÄ± Ã§ok dayanÄ±klÄ± ve Ã§ok fazla bÃ¶lmesi var. KumaÅŸÄ± Ã§ok kaliteli. SatÄ±cÄ± Ã§ok nazik, hediye iÃ§in teÅŸekkÃ¼rler. Kargo hÄ±zÄ± muazzam. FiyatÄ± bu kalite iÃ§in gerÃ§ekten Ã§ok makul.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1]\"\n",
        "520,\"CihazÄ±n ekranÄ± gelmiyor, anakartÄ± bozuk olabilir. SatÄ±cÄ± deÄŸiÅŸim yapmayacaÄŸÄ±nÄ± belirtti. Kargo kutusu Ä±slak bir halde kapÄ±ma bÄ±rakÄ±ldÄ±. Malzeme kalitesi Ã§ok dayanÄ±ksÄ±z plastik. Berbat bir deneyim.\",\"[0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1]\"\n",
        "521,\"Kahve makinesi Ã§ok pratik, sabahlarÄ± vaktim bana kalÄ±yor. Rengi mutfaÄŸÄ±ma harika uydu. SatÄ±cÄ± paketlemeye Ã§ok Ã¶zen gÃ¶stermiÅŸ. Kargo hÄ±zÄ± iyiydi. FiyatÄ± indirimdeyken Ã§ok uyguna geldi.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\"\n",
        "522,\"Pantolonun boyu Ã§ok uzun, tadilat gerektiriyor. KumaÅŸÄ± da Ã§ok sert, kaÅŸÄ±ndÄ±rÄ±yor. SatÄ±cÄ± mesajlara hiÃ§ dÃ¶nmÃ¼yor. Kargo hÄ±zÄ± Ã§ok yavaÅŸtÄ±. Paketleme Ã§ok Ã¶zensizdi, Ã¼rÃ¼n buruÅŸuk geldi. DeÄŸmez.\",\"[0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0]\"\n",
        "523,\"DambÄ±l seti Ã§ok kaliteli, tutuÅŸu hiÃ§ kaydÄ±rmÄ±yor. ParÃ§alarÄ± eksiksiz geldi. SatÄ±cÄ± Ã§ok dÃ¼rÃ¼st ve hÄ±zlÄ±. Paketleme Ã§ok gÃ¼venliydi. FiyatÄ± piyasaya gÃ¶re oldukÃ§a uygun. Kesinlikle tavsiye ederim.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1]\"\n",
        "524,\"ÃœrÃ¼nÃ¼n camÄ± tamamen buz olmuÅŸ ÅŸekilde geldi. SatÄ±cÄ±ya ulaÅŸmak mÃ¼mkÃ¼n deÄŸil. Ä°ade sÃ¼reci Ã§ok karmaÅŸÄ±k ve yorucu. GÃ¶rÃ¼nÃ¼ÅŸÃ¼ gÃ¼zeldir diye almÄ±ÅŸtÄ±m ama hayal kÄ±rÄ±klÄ±ÄŸÄ±. Kargo firmasÄ± rezalet.\",\"[0, 0, 0, -1, 0, -1, -1, 0, 0, 1, 0, 0, 0, -1]\"\n",
        "525,\"Nevresimlerin dokusu ÅŸahane, Ã§ok rahat uyutuyor. Renkleri yÄ±kama sonrasÄ± solmadÄ±. SatÄ±cÄ± yanÄ±na Ã§ok ÅŸÄ±k bir kese eklemiÅŸ. Kargo ertesi gÃ¼n geldi. FiyatÄ± bu kalite iÃ§in Ã§ok Ã§ok iyi.\",\"[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\"\n",
        "526,\"Gelen robot sÃ¼pÃ¼rgenin ÅŸarjÄ± 10 dakikada bitiyor. SatÄ±cÄ± iade talebimi kullanÄ±cÄ± hatasÄ± diye reddetti. Kargo poÅŸeti paramparÃ§aydÄ±. Malzemesi Ã§ok dandik duruyor. ParanÄ±za yazÄ±k, almayÄ±n.\",\"[0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1]\"\n",
        "527,\"Mutfak robotu Ã§ok gÃ¼Ã§lÃ¼ ve sessiz Ã§alÄ±ÅŸÄ±yor. TÃ¼m aksesuarlarÄ± tam geldi. SatÄ±cÄ± Ã§ok Ã¶zenli paketlemiÅŸ, kat kat sarmÄ±ÅŸ. Kargo hÄ±zÄ± harikaydÄ±. FiyatÄ± da oldukÃ§a cazip bir seviyede.\",\"[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]\"\n",
        "528,\"KÄ±lÄ±fÄ±n rengi fotoÄŸraftakinden Ã§ok daha koyu, siyah gibi. Ãœstelik Ã§ok kalitesiz bir silikon, hemen koptu. SatÄ±cÄ± ilgilenmedi. Kargo da Ã§ok geÃ§ ulaÅŸtÄ±. FiyatÄ± ucuz ama kalite sÄ±fÄ±r.\",\"[1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1]\"\n",
        "529,\"ParfÃ¼mÃ¼n kokusu tam istediÄŸim gibi ama hiÃ§ kalÄ±cÄ± deÄŸil, hemen uÃ§uyor. ÅžiÅŸesi Ã§ok ÅŸÄ±k. SatÄ±cÄ± Ã§ok nazik, paketleme Ã§ok gÃ¼venliydi. FiyatÄ± bu marka iÃ§in biraz pahalÄ± kalmÄ±ÅŸ.\",\"[0, 1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, -1, 1]\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Ai3HnNGIQCPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¥ Cihaz: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_new = pd.read_csv(io.StringIO(new_data_csv))\n",
        "df_new.rename(columns={'comment_text': 'review_text', 'sentiments': 'scores'}, inplace=True)\n",
        "print(f\"ðŸ†• Yeni eklenen veri sayÄ±sÄ±: {len(df_new)}\")\n",
        "\n",
        "\n",
        "LABELS_PATH = \"/content/labeled_reviews_parallel3.csv\"\n",
        "REVIEWS_PATH = \"/content/reviews.csv\"\n",
        "\n",
        "try:\n",
        "    df_labels = pd.read_csv(LABELS_PATH)\n",
        "    df_reviews = pd.read_csv(REVIEWS_PATH)\n",
        "    df_main = pd.merge(df_labels, df_reviews[['id', 'review_text']], on='id', how='inner')\n",
        "    df_main = df_main[['review_text', 'scores']]\n",
        "    print(f\"ðŸ“¦ Ana veri sayÄ±sÄ±: {len(df_main)}\")\n",
        "\n",
        "    df_merged = pd.concat([df_main, df_new[['review_text', 'scores']]], ignore_index=True)\n",
        "    print(f\"âœ… Toplam BirleÅŸmiÅŸ Veri: {len(df_merged)}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ HATA: Dosya bulunamadÄ±! Yolunu kontrol et.\\n{e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "df_merged.dropna(subset=['review_text', 'scores'], inplace=True)\n",
        "\n",
        "def parse_scores(score_str):\n",
        "    try:\n",
        "        if isinstance(score_str, list): return score_str\n",
        "        return ast.literal_eval(score_str)\n",
        "    except: return None\n",
        "\n",
        "df_merged['parsed_scores'] = df_merged['scores'].apply(parse_scores)\n",
        "df_merged.dropna(subset=['parsed_scores'], inplace=True)\n",
        "\n",
        "initial_len = len(df_merged)\n",
        "df_merged = df_merged[~df_merged['parsed_scores'].apply(lambda x: all(s == 0 for s in x))]\n",
        "final_len = len(df_merged)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"ðŸ—‘ï¸ TEMÄ°ZLÄ°K SONRASI DURUM:\")\n",
        "print(f\"   Silinen (TÃ¼mÃ¼ 0) : {initial_len - final_len}\")\n",
        "print(f\"   Kalan Net Veri   : {final_len}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "ASPECTS_DICT = {\n",
        "    0: \"Seller Errors\", 1: \"General Satisfaction\", 2: \"Size & Fit\", 3: \"Damaged/Broken Item\",\n",
        "    4: \"Installation & Usage\", 5: \"Return Process\", 6: \"Seller Courtesy\", 7: \"Product Features\",\n",
        "    8: \"General Quality\", 9: \"Color & Appearance\", 10: \"Durability Issues\", 11: \"Missing/Defective Item\",\n",
        "    12: \"Price/Performance\", 13: \"Shipping Disasters\"\n",
        "}\n",
        "ASPECT_NAMES = [ASPECTS_DICT[i] for i in range(len(ASPECTS_DICT))]\n",
        "NUM_ASPECTS = len(ASPECT_NAMES)\n",
        "\n",
        "map_to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "\n",
        "def encode_labels_list(scores_list):\n",
        "    encoded = []\n",
        "    for s in scores_list:\n",
        "        val = int(s)\n",
        "        if val not in map_to_idx: encoded.append(map_to_idx[0])\n",
        "        else: encoded.append(map_to_idx[val])\n",
        "    return encoded\n",
        "\n",
        "all_labels = np.stack(df_merged['parsed_scores'].apply(encode_labels_list))\n",
        "\n",
        "flat_labels = all_labels.flatten()\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.array([0, 1, 2]),\n",
        "    y=flat_labels\n",
        ")\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "print(\"\\nðŸ“Š GÃœNCEL AÄžIRLIKLAR:\")\n",
        "print(f\"   Negatif (0): {class_weights[0]:.4f}\")\n",
        "print(f\"   NÃ¶tr    (1): {class_weights[1]:.4f}\")\n",
        "print(f\"   Pozitif (2): {class_weights[2]:.4f}\")\n",
        "\n",
        "train_texts, temp_texts, train_y, temp_y = train_test_split(\n",
        "    df_merged['review_text'].tolist(), all_labels, test_size=0.2, random_state=SEED\n",
        ")\n",
        "val_texts, test_texts, val_y, test_y = train_test_split(\n",
        "    temp_texts, temp_y, test_size=0.5, random_state=SEED\n",
        ")\n",
        "\n",
        "\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"text\": str(self.texts[idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [b[\"text\"] for b in batch]\n",
        "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
        "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    enc[\"labels\"] = labels\n",
        "    return enc\n",
        "\n",
        "train_dataset = ABSADataset(train_texts, train_y)\n",
        "val_dataset   = ABSADataset(val_texts, val_y)\n",
        "test_dataset  = ABSADataset(test_texts, test_y)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "=\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, num_classes=3, loss_weights=None):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "        for param in self.bert.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        for i in range(4):\n",
        "            for param in self.bert.encoder.layer[i].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        print(\"â„ï¸ BERT'in sadece ilk 4 katmanÄ± donduruldu. Son 8 katman eÄŸitilecek.\")\n",
        "\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(out.last_hidden_state[:, 0, :])\n",
        "        logits_list = [head(pooled) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            ce = nn.CrossEntropyLoss(weight=self.loss_weights)\n",
        "            losses = [ce(logits[:, i, :], labels[:, i]) for i in range(logits.shape[1])]\n",
        "            loss = torch.mean(torch.stack(losses))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=NUM_ASPECTS, loss_weights=weights_tensor).to(device)\n",
        "\n",
        "\n",
        "EPOCHS = 15\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            y_true.append(batch[\"labels\"].cpu().numpy())\n",
        "            y_pred.append(preds)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    scores = {}\n",
        "    for i, aspect_name in enumerate(ASPECT_NAMES):\n",
        "        f1 = f1_score(y_true[:, i], y_pred[:, i], average=\"macro\")\n",
        "        scores[aspect_name] = f1\n",
        "    scores[\"GENEL_F1\"] = np.mean(list(scores.values()))\n",
        "    scores[\"ACCURACY\"] = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
        "    return scores\n",
        "\n",
        "train_losses, val_f1_scores, val_accuracies = [], [], []\n",
        "best_f1 = 0\n",
        "\n",
        "print(\"\\nðŸš€ EÄŸitim BaÅŸlÄ±yor (Light Freezing)...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    val_scores = evaluate(model, val_loader)\n",
        "    curr_f1 = val_scores['GENEL_F1']\n",
        "    curr_acc = val_scores['ACCURACY']\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_f1_scores.append(curr_f1)\n",
        "    val_accuracies.append(curr_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Val F1: {curr_f1:.4f} | Val Acc: {curr_acc:.4f}\")\n",
        "\n",
        "    if curr_f1 > best_f1:\n",
        "        best_f1 = curr_f1\n",
        "        torch.save(model.state_dict(), \"absa_model_light_frozen.pt\")\n",
        "        print(f\"âœ… Model Kaydedildi! En iyi F1: {best_f1:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 3, 1); plt.plot(train_losses, 'r-o'); plt.title('Loss')\n",
        "plt.subplot(1, 3, 2); plt.plot(val_f1_scores, 'b-s'); plt.title('F1 Score')\n",
        "plt.subplot(1, 3, 3); plt.plot(val_accuracies, 'g-^'); plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rXjJwJ4MVL7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install transformers==4.44.2 accelerate datasets evaluate scikit-learn torch --upgrade\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DynHhYiQek0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¥ Cihaz: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "df_new = pd.read_csv(io.StringIO(new_data_csv))\n",
        "df_new.rename(columns={'comment_text': 'review_text', 'sentiments': 'scores'}, inplace=True)\n",
        "\n",
        "\n",
        "LABELS_PATH = \"/content/labeled_reviews_parallel3.csv\"\n",
        "REVIEWS_PATH = \"/content/reviews.csv\"\n",
        "\n",
        "try:\n",
        "    df_labels = pd.read_csv(LABELS_PATH)\n",
        "    df_reviews = pd.read_csv(REVIEWS_PATH)\n",
        "    df_main = pd.merge(df_labels, df_reviews[['id', 'review_text']], on='id', how='inner')\n",
        "    df_main = df_main[['review_text', 'scores']]\n",
        "\n",
        "    df_merged = pd.concat([df_main, df_new[['review_text', 'scores']]], ignore_index=True)\n",
        "    print(f\"âœ… Toplam Veri: {len(df_merged)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"HATA: Dosya yok.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "def parse_scores(score_str):\n",
        "    try:\n",
        "        if isinstance(score_str, list): return score_str\n",
        "        return ast.literal_eval(score_str)\n",
        "    except: return None\n",
        "\n",
        "df_merged['parsed_scores'] = df_merged['scores'].apply(parse_scores)\n",
        "df_merged.dropna(subset=['review_text', 'parsed_scores'], inplace=True)\n",
        "\n",
        "initial_len = len(df_merged)\n",
        "df_merged = df_merged[~df_merged['parsed_scores'].apply(lambda x: all(s == 0 for s in x))]\n",
        "print(f\"ðŸ—‘ï¸ Temizlik SonrasÄ± Veri: {len(df_merged)} (Silinen BoÅŸ SatÄ±rlar: {initial_len - len(df_merged)})\")\n",
        "\n",
        "\n",
        "ASPECTS_DICT = {\n",
        "    0: \"Seller Errors\", 1: \"General Satisfaction\", 2: \"Size & Fit\", 3: \"Damaged/Broken Item\",\n",
        "    4: \"Installation & Usage\", 5: \"Return Process\", 6: \"Seller Courtesy\", 7: \"Product Features\",\n",
        "    8: \"General Quality\", 9: \"Color & Appearance\", 10: \"Durability Issues\", 11: \"Missing/Defective Item\",\n",
        "    12: \"Price/Performance\", 13: \"Shipping Disasters\"\n",
        "}\n",
        "ASPECT_NAMES = [ASPECTS_DICT[i] for i in range(len(ASPECTS_DICT))]\n",
        "NUM_ASPECTS = len(ASPECT_NAMES)\n",
        "\n",
        "map_to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "\n",
        "def encode_labels_list(scores_list):\n",
        "    encoded = []\n",
        "    for s in scores_list:\n",
        "        val = int(s)\n",
        "        if val in map_to_idx:\n",
        "            encoded.append(map_to_idx[val])\n",
        "        else:\n",
        "            encoded.append(map_to_idx[0])\n",
        "    return encoded\n",
        "\n",
        "all_labels = np.stack(df_merged['parsed_scores'].apply(encode_labels_list))\n",
        "\n",
        "train_texts, temp_texts, train_y, temp_y = train_test_split(df_merged['review_text'].tolist(), all_labels, test_size=0.2, random_state=SEED)\n",
        "val_texts, test_texts, val_y, test_y = train_test_split(temp_texts, temp_y, test_size=0.5, random_state=SEED)\n",
        "\n",
        "\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"text\": str(self.texts[idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [b[\"text\"] for b in batch]\n",
        "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
        "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    enc[\"labels\"] = labels\n",
        "    return enc\n",
        "\n",
        "train_loader = DataLoader(ABSADataset(train_texts, train_y), batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ABSADataset(val_texts, val_y), batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(ABSADataset(test_texts, test_y), batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(out.last_hidden_state[:, 0, :])\n",
        "        logits_list = [head(pooled) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            ce = nn.CrossEntropyLoss()\n",
        "            losses = [ce(logits[:, i, :], labels[:, i]) for i in range(logits.shape[1])]\n",
        "            loss = torch.mean(torch.stack(losses))\n",
        "        return logits, loss\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=NUM_ASPECTS).to(device)\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            y_true.append(batch[\"labels\"].cpu().numpy())\n",
        "            y_pred.append(preds)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    scores[\"GENEL_F1\"] = f1_score(y_true.flatten(), y_pred.flatten(), average=\"macro\")\n",
        "    scores[\"ACCURACY\"] = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
        "    return scores\n",
        "\n",
        "train_losses, val_f1_scores = [], []\n",
        "best_f1 = 0\n",
        "\n",
        "print(\"\\nðŸš€ EÄŸitim BaÅŸlÄ±yor (KLASÄ°K MOD - HATA DÃœZELTÄ°LDÄ°)...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    val_scores = evaluate(model, val_loader)\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_f1_scores.append(val_scores['GENEL_F1'])\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Val F1: {val_scores['GENEL_F1']:.4f} | Val Acc: {val_scores['ACCURACY']:.4f}\")\n",
        "\n",
        "    if val_scores['GENEL_F1'] > best_f1:\n",
        "        best_f1 = val_scores['GENEL_F1']\n",
        "        torch.save(model.state_dict(), \"absa_model_classic_fixed.pt\")\n",
        "        print(f\"âœ… En iyi model: {best_f1:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1); plt.plot(train_losses, 'r-o'); plt.title('Loss')\n",
        "plt.subplot(1, 2, 2); plt.plot(val_f1_scores, 'b-s'); plt.title('F1 Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MAX2u8-AX3NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Live test**"
      ],
      "metadata": {
        "id": "0hC-40Rv5sZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ASPECTS_DICT = {\n",
        "    0: \"Seller Errors\", 1: \"General Satisfaction\", 2: \"Size & Fit\", 3: \"Damaged/Broken Item\",\n",
        "    4: \"Installation & Usage\", 5: \"Return Process\", 6: \"Seller Courtesy\", 7: \"Product Features\",\n",
        "    8: \"General Quality\", 9: \"Color & Appearance\", 10: \"Durability Issues\", 11: \"Missing/Defective Item\",\n",
        "    12: \"Price/Performance\", 13: \"Shipping Disasters\"\n",
        "}\n",
        "ASPECT_NAMES = [ASPECTS_DICT[i] for i in range(len(ASPECTS_DICT))]\n",
        "\n",
        "idx_to_sent = {0: \"Negatif ðŸ˜¡\", 1: \"NÃ¶tr/Yok ðŸ˜\", 2: \"Pozitif ðŸ˜ƒ\"}\n",
        "\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(out.last_hidden_state[:, 0, :])\n",
        "        logits_list = [head(pooled) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "        return logits, None\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=len(ASPECT_NAMES)).to(device)\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"absa_model_classic_fixed.pt\", map_location=device))\n",
        "    print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi ve teste hazÄ±r!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ HATA: Model dosyasÄ± bulunamadÄ±. Ã–nce eÄŸitimi tamamla!\")\n",
        "\n",
        "def predict_new(text):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "        preds = torch.argmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    print(f\"\\nðŸ“ Yorum: {text}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    found_any = False\n",
        "    for i, aspect in enumerate(ASPECT_NAMES):\n",
        "        if preds[i] != 1:\n",
        "            sentiment = idx_to_sent[preds[i]]\n",
        "            print(f\"ðŸ”¹ {aspect:<25}: {sentiment}\")\n",
        "            found_any = True\n",
        "\n",
        "    if not found_any:\n",
        "        print(\"ðŸ”¸ (Bu yorumda belirgin bir duygu tespit edilemedi / Hepsi NÃ¶tr)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "adpI2udzkrxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_new(\"canavar gibi Ã§alÄ±sÄ±yor :)\")\n"
      ],
      "metadata": {
        "id": "Nd-Y8anOktwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_new(\"kargo hÄ±zlÄ± geldi ama beden uymadÄ±\")\n"
      ],
      "metadata": {
        "id": "zVLp7akPlPLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of results**"
      ],
      "metadata": {
        "id": "niLwnot6511I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "import random\n",
        "\n",
        "\n",
        "ASPECT_NAMES_ENG = [\n",
        "    \"Seller Errors\", \"General Satisfaction\", \"Size & Fit\", \"Damaged/Broken Item\",\n",
        "    \"Installation & Usage\", \"Return Process\", \"Seller Courtesy\", \"Product Features\",\n",
        "    \"General Quality\", \"Color & Appearance\", \"Durability Issues\", \"Missing/Defective Item\",\n",
        "    \"Price/Performance\", \"Shipping Disasters\"\n",
        "]\n",
        "\n",
        "\n",
        "SENTIMENT_NAMES = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "model.load_state_dict(torch.load(\"absa_model_classic_fixed.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "print(\"ðŸ“Š Generating predictions on Test Set...\")\n",
        "\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "input_texts = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        y_true_all.append(labels)\n",
        "        y_pred_all.append(preds)\n",
        "\n",
        "\n",
        "        decoded = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "        input_texts.extend(decoded)\n",
        "\n",
        "y_true_all = np.concatenate(y_true_all)\n",
        "y_pred_all = np.concatenate(y_pred_all)\n",
        "\n",
        "\n",
        "print(\"ðŸŽ¨ Plotting Confusion Matrices...\")\n",
        "\n",
        "n_cols = 4\n",
        "n_rows = (len(ASPECT_NAMES_ENG) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(24, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, aspect in enumerate(ASPECT_NAMES_ENG):\n",
        "    cm = confusion_matrix(y_true_all[:, i], y_pred_all[:, i])\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                xticklabels=SENTIMENT_NAMES,\n",
        "                yticklabels=SENTIMENT_NAMES, cbar=False)\n",
        "\n",
        "    axes[i].set_title(f\"{aspect}\", fontsize=14, fontweight='bold', color='darkblue')\n",
        "    axes[i].set_xlabel(\"Predicted\")\n",
        "    axes[i].set_ylabel(\"True (Actual)\")\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"ðŸŽ¨ Plotting Accuracy & F1 Scores...\")\n",
        "\n",
        "f1_scores = []\n",
        "accuracies = []\n",
        "\n",
        "for i in range(len(ASPECT_NAMES_ENG)):\n",
        "    f1 = f1_score(y_true_all[:, i], y_pred_all[:, i], average='macro')\n",
        "    acc = accuracy_score(y_true_all[:, i], y_pred_all[:, i])\n",
        "    f1_scores.append(f1)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "import pandas as pd\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Aspect': ASPECT_NAMES_ENG,\n",
        "    'F1 Score (Macro)': f1_scores,\n",
        "    'Accuracy': accuracies\n",
        "})\n",
        "\n",
        "metrics_melted = metrics_df.melt(id_vars=\"Aspect\", var_name=\"Metric\", value_name=\"Score\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.barplot(data=metrics_melted, x=\"Aspect\", y=\"Score\", hue=\"Metric\", palette=\"viridis\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Model Performance by Aspect (English)\", fontsize=16)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "iHt1itQPpo1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"{'ðŸ”Ž RANDOMLY SELECTED 5 TEST EXAMPLES':^60}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "indices = random.sample(range(len(input_texts)), 5)\n",
        "\n",
        "for idx in indices:\n",
        "    text = input_texts[idx]\n",
        "    true_lbls = y_true_all[idx]\n",
        "    pred_lbls = y_pred_all[idx]\n",
        "\n",
        "    print(f\"\\nðŸ“ Review: \\\"{text}\\\"\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Aspect':<25} | {'True Label':<15} | {'Predicted':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    found_any = False\n",
        "    for i, aspect in enumerate(ASPECT_NAMES_ENG):\n",
        "        if true_lbls[i] != 1 or pred_lbls[i] != 1:\n",
        "            t_sent = SENTIMENT_NAMES[true_lbls[i]]\n",
        "            p_sent = SENTIMENT_NAMES[pred_lbls[i]]\n",
        "\n",
        "            match_icon = \"âœ…\" if true_lbls[i] == pred_lbls[i] else \"âŒ\"\n",
        "\n",
        "            print(f\"{aspect:<25} | {t_sent:<15} | {p_sent:<15} {match_icon}\")\n",
        "            found_any = True\n",
        "\n",
        "    if not found_any:\n",
        "        print(\"(No specific sentiment found in both True and Predicted - All Neutral)\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "AualzscdqJ4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJeRK2hxxhvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class weights are determined by class distribution.**"
      ],
      "metadata": {
        "id": "pelRcs1Z7LRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import io\n",
        "import random\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¥ Cihaz: {device}\")\n",
        "\n",
        "\n",
        "ASPECTS_DICT = {\n",
        "    0: \"Seller Errors\", 1: \"General Satisfaction\", 2: \"Size & Fit\", 3: \"Damaged/Broken Item\",\n",
        "    4: \"Installation & Usage\", 5: \"Return Process\", 6: \"Seller Courtesy\", 7: \"Product Features\",\n",
        "    8: \"General Quality\", 9: \"Color & Appearance\", 10: \"Durability Issues\", 11: \"Missing/Defective Item\",\n",
        "    12: \"Price/Performance\", 13: \"Shipping Disasters\"\n",
        "}\n",
        "ASPECT_NAMES = list(ASPECTS_DICT.values())\n",
        "NUM_ASPECTS = len(ASPECT_NAMES)\n",
        "\n",
        "\n",
        "def encode_labels_list(scores_list):\n",
        "    if isinstance(scores_list, str):\n",
        "        scores_list = ast.literal_eval(scores_list)\n",
        "\n",
        "    map_to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "    return [map_to_idx.get(int(s), 1) for s in scores_list]\n",
        "\n",
        "all_labels = np.stack(df_merged['scores'].apply(encode_labels_list))\n",
        "\n",
        "train_texts, temp_texts, train_y, temp_y = train_test_split(\n",
        "    df_merged['review_text'].tolist(), all_labels, test_size=0.2, random_state=SEED\n",
        ")\n",
        "val_texts, test_texts, val_y, test_y = train_test_split(\n",
        "    temp_texts, temp_y, test_size=0.5, random_state=SEED\n",
        ")\n",
        "\n",
        "flat_train_labels = train_y.flatten()\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1, 2]), y=flat_train_labels)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
        "print(f\"âš–ï¸ SÄ±nÄ±f AÄŸÄ±rlÄ±klarÄ± (Neg:0, Neu:1, Poz:2): {class_weights.tolist()}\")\n",
        "\n",
        "\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"text\": str(self.texts[idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [b[\"text\"] for b in batch]\n",
        "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
        "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    enc[\"labels\"] = labels\n",
        "    return enc\n",
        "\n",
        "train_loader = DataLoader(ABSADataset(train_texts, train_y), batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ABSADataset(val_texts, val_y), batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(ABSADataset(test_texts, test_y), batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "class MultiHeadBert(nn.Module):\n",
        "    def __init__(self, model_name, num_aspects, class_weights, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.num_aspects = num_aspects\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(hidden, num_classes) for _ in range(num_aspects)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "        logits_list = [head(pooled_output) for head in self.classifiers]\n",
        "        logits = torch.stack(logits_list, dim=1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "            aspect_losses = []\n",
        "            for i in range(self.num_aspects):\n",
        "                aspect_losses.append(criterion(logits[:, i, :], labels[:, i]))\n",
        "            loss = torch.mean(torch.stack(aspect_losses))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "model = MultiHeadBert(MODEL_NAME, num_aspects=NUM_ASPECTS, class_weights=class_weights).to(device)\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items() if k != \"text\"}\n",
        "            logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            y_true.append(batch[\"labels\"].cpu().numpy())\n",
        "            y_pred.append(preds)\n",
        "\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1_score(y_true.flatten(), y_pred.flatten(), average=\"macro\"),\n",
        "        \"acc\": accuracy_score(y_true.flatten(), y_pred.flatten())\n",
        "    }\n",
        "\n",
        "\n",
        "train_losses, val_f1_scores = [], []\n",
        "best_f1 = 0\n",
        "\n",
        "print(\"\\nðŸš€ EÄŸitim BaÅŸlÄ±yor (Class Weights Aktif)...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k != \"text\"}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    metrics = evaluate(model, val_loader)\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_f1_scores.append(metrics[\"f1\"])\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Val F1: {metrics['f1']:.4f} | Val Acc: {metrics['acc']:.4f}\")\n",
        "\n",
        "    if metrics[\"f1\"] > best_f1:\n",
        "        best_f1 = metrics[\"f1\"]\n",
        "        torch.save(model.state_dict(), \"best_absa_model.pt\")\n",
        "        print(f\"â­ En iyi model kaydedildi! F1: {best_f1:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1); plt.plot(train_losses, 'r-o'); plt.title('Training Loss')\n",
        "plt.subplot(1, 2, 2); plt.plot(val_f1_scores, 'b-s'); plt.title('Validation F1 Score')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Ä°ÅŸlem TamamlandÄ±.\")"
      ],
      "metadata": {
        "id": "gI6OWWuzxhtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "import random\n",
        "\n",
        "\n",
        "ASPECT_NAMES_ENG = [\n",
        "    \"Seller Errors\", \"General Satisfaction\", \"Size & Fit\", \"Damaged/Broken Item\",\n",
        "    \"Installation & Usage\", \"Return Process\", \"Seller Courtesy\", \"Product Features\",\n",
        "    \"General Quality\", \"Color & Appearance\", \"Durability Issues\", \"Missing/Defective Item\",\n",
        "    \"Price/Performance\", \"Shipping Disasters\"\n",
        "]\n",
        "\n",
        "\n",
        "SENTIMENT_NAMES = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_absa_model.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "print(\"ðŸ“Š Generating predictions on Test Set...\")\n",
        "\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "input_texts = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        y_true_all.append(labels)\n",
        "        y_pred_all.append(preds)\n",
        "\n",
        "\n",
        "        decoded = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "        input_texts.extend(decoded)\n",
        "\n",
        "y_true_all = np.concatenate(y_true_all)\n",
        "y_pred_all = np.concatenate(y_pred_all)\n",
        "\n",
        "\n",
        "print(\"ðŸŽ¨ Plotting Confusion Matrices...\")\n",
        "\n",
        "n_cols = 4\n",
        "n_rows = (len(ASPECT_NAMES_ENG) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(24, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, aspect in enumerate(ASPECT_NAMES_ENG):\n",
        "    cm = confusion_matrix(y_true_all[:, i], y_pred_all[:, i])\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                xticklabels=SENTIMENT_NAMES,\n",
        "                yticklabels=SENTIMENT_NAMES, cbar=False)\n",
        "\n",
        "    axes[i].set_title(f\"{aspect}\", fontsize=14, fontweight='bold', color='darkblue')\n",
        "    axes[i].set_xlabel(\"Predicted\")\n",
        "    axes[i].set_ylabel(\"True (Actual)\")\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"ðŸŽ¨ Plotting Accuracy & F1 Scores...\")\n",
        "\n",
        "f1_scores = []\n",
        "accuracies = []\n",
        "\n",
        "for i in range(len(ASPECT_NAMES_ENG)):\n",
        "    f1 = f1_score(y_true_all[:, i], y_pred_all[:, i], average='macro')\n",
        "    acc = accuracy_score(y_true_all[:, i], y_pred_all[:, i])\n",
        "    f1_scores.append(f1)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "import pandas as pd\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Aspect': ASPECT_NAMES_ENG,\n",
        "    'F1 Score (Macro)': f1_scores,\n",
        "    'Accuracy': accuracies\n",
        "})\n",
        "\n",
        "metrics_melted = metrics_df.melt(id_vars=\"Aspect\", var_name=\"Metric\", value_name=\"Score\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.barplot(data=metrics_melted, x=\"Aspect\", y=\"Score\", hue=\"Metric\", palette=\"viridis\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Model Performance by Aspect (English)\", fontsize=16)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NJhqbDH70J2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion**"
      ],
      "metadata": {
        "id": "thMQwX2o7z-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike traditional Aspect-Based Sentiment Analysis (ABSA) studies that often rely on arbitrarily defined or manually selected categories, this research adopted a data-driven scientific methodology. By utilizing unsupervised clustering techniques on the embedding space, we mathematically determined the most representative aspect categories inherent in the dataset.\n",
        "\n",
        "Initially, the study focused on the 7 most inclusive clusters. The evaluation metrics for this configuration, presented in the Results section, demonstrated a robust performance with high F1 scores. This indicates that the model successfully captures sentiment when the semantic boundaries between aspects are broad and distinct.\n",
        "\n",
        "Subsequently, the granularity of the analysis was increased by expanding the scope to 14 clusters to capture more specific nuances of user feedback. A critical observation emerged from this comparative analysis: there is an inverse correlation between the number of clusters and the model's performance. As the number of aspects increased from 7 to 14, the F1 score exhibited a noticeable decline. This phenomenon can be attributed to the increased semantic overlap between finer-grained categories and the resulting class imbalance, which poses a greater challenge for the model in distinguishing between closely related aspects.\n",
        "\n",
        "Despite the reduction in performance scores, high-granularity analysis remains essential for detailed customer insights. Consequently, this observation lays the groundwork for future research. In upcoming thesis work or supplementary projects, we aim to extend this experimental framework to test up to 50 distinct aspects. To mitigate the performance drop observed in this study, future iterations will likely incorporate advanced techniques such as hierarchical classification or data augmentation to stabilize F1 scores at higher granularities."
      ],
      "metadata": {
        "id": "6kPIHRHE74FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/best_absa_model.pt /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "SPGZFabe_O1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/absa_model_classic_fixed.pt /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "ChuHvouG_uSK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}